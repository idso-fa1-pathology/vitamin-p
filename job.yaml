apiVersion: batch/v1
kind: Job
metadata:
  name: vitamin-p-flex-large-fold2  # Change this for each job
  namespace: yn-gpu-workload
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 600
  template:
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
        nvidia.com/gpu.product: "NVIDIA-H100-80GB-HBM3"
      securityContext:
        runAsUser: 297724
        runAsGroup: 1944303352
        fsGroup: 1944303352
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: '1000Gi'
        - name: program
          persistentVolumeClaim:
            claimName: yshokrollahi-gpu-rsrch9-home-plm
      containers:
        - name: training-container
          image: hpcharbor.mdanderson.edu/yshokrollahi/vitamin-p:latest
          # NOW YOU JUST CHANGE THE ARGUMENTS! ðŸŽ‰
          command: 
            - "python"
            - "scripts/train.py"
            - "--model"
            - "flex"           # Change: flex, dual, he, mif
            - "--size"
            - "large"          # Change: small, base, large, giant
            - "--fold"
            - "1"              # Change: 1, 2, 3
            - "--epochs"
            - "250"
          workingDir: /rsrch9/home/plm/idso_fa1_pathology/codes/yshokrollahi/vitamin-p
          env:
            - name: WANDB_API_KEY
              value: "979fc21fe6b28c4e4a4503f7e421bfc270a4300c"
            - name: WANDB_PROJECT
              value: "vitamin-p"
            - name: WANDB_MODE
              value: "online"
            - name: HOME
              value: /tmp
            - name: PYTHONPATH
              value: /rsrch9/home/plm/idso_fa1_pathology/codes/yshokrollahi/vitamin-p
          volumeMounts:
            - name: shm
              mountPath: "/dev/shm"
            - name: program
              mountPath: "/rsrch9/home/plm/idso_fa1_pathology"
          resources:
            limits:
              nvidia.com/gpu: "4"
              cpu: "60"
              memory: "700Gi"
      restartPolicy: Never