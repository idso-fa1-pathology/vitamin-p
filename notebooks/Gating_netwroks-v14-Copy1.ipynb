{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea1e70-a975-4c88-a0f2-dbfc719a1053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install natsort\n",
    "# !pip install albumentations\n",
    "# !pip install wandb\n",
    "# !pip install torchinfo\n",
    "# !pip install schema\n",
    "# !pip install torchmetrics\n",
    "# !pip install einops\n",
    "# !pip install timm\n",
    "# !pip install natsort\n",
    "# !pip install torchsummary\n",
    "# !pip install natsort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76097a81-9039-4e0d-bd0d-ac49911f07e2",
   "metadata": {},
   "source": [
    "## Cell 1 - Imports and Base Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b964577-967d-4c73-9561-76c7297c176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Add CellViT to python path first\n",
    "cellvit_path = \"/rsrch5/home/plm/yshokrollahi/CellViT\"\n",
    "if cellvit_path not in sys.path:\n",
    "    sys.path.append(cellvit_path)\n",
    "    print(f\"Added {cellvit_path} to Python path\")\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import albumentations as A\n",
    "\n",
    "try:\n",
    "    from cell_segmentation.datasets.pannuke import PanNukeDataset\n",
    "    from cell_segmentation.datasets.tissuenet import TissueNetDataset\n",
    "    print(\"Successfully imported CellViT modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652ee04-2a6b-4917-88e7-e85b1673aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814154d-4bdc-4bb0-ba24-33fc25f3c24a",
   "metadata": {},
   "source": [
    "## Cell 2 - Base Dataset Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5249b23-37a9-4320-8a23-e0e62dc059c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModalityDataset(Dataset, ABC):\n",
    "    \"\"\"Base class for all modality datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path: Path, split: str, transforms=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.modality_type = self._get_modality_type()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def _get_modality_type(self) -> str:\n",
    "        \"\"\"Return the modality type identifier\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def standardize_output(self, image, targets, metadata) -> Dict:\n",
    "        \"\"\"Standardize the output format across all modalities\"\"\"\n",
    "        return {\n",
    "            'image': image,\n",
    "            'metadata': {\n",
    "                'modality_type': self.modality_type,\n",
    "                'available_targets': self._get_available_targets(),\n",
    "                **metadata\n",
    "            },\n",
    "            'targets': targets\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_available_targets(self) -> List[str]:\n",
    "        \"\"\"Return list of available target types for this modality\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3644cc7-2111-452d-9fc9-56e1ac4de72f",
   "metadata": {},
   "source": [
    "## Cell 3 - TissueNet Standardized Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeecd02-4719-4fe6-bbd6-4619b1be6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.ndimage import center_of_mass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class TissueNetStandardized(BaseModalityDataset):\n",
    "    def __init__(self, dataset_path: Path, split: str, transforms=None, **kwargs):\n",
    "        super().__init__(dataset_path, split, transforms)\n",
    "        self.dataset = TissueNetDataset(\n",
    "            dataset_path=dataset_path,\n",
    "            split=split,\n",
    "            transforms=transforms,\n",
    "            stardist=False,\n",
    "            regression=False,\n",
    "            cache_dataset=False,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _get_modality_type(self) -> str:\n",
    "        return 'tissuenet'\n",
    "    \n",
    "    def _get_available_targets(self) -> List[str]:\n",
    "        return ['nuclei', 'membrane', 'hv_maps']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        \n",
    "        if isinstance(data, (tuple, list)):\n",
    "            image = data[0]\n",
    "            original_targets = data[1]\n",
    "        else:\n",
    "            image = data['image']\n",
    "            original_targets = data.get('targets', {})\n",
    "        \n",
    "        standardized_targets = {\n",
    "            'masks': {\n",
    "                'nuclei': original_targets.get('nuclei_mask', None),\n",
    "                'membrane': original_targets.get('cell_mask', None)\n",
    "            },\n",
    "            'semantic': {\n",
    "                'has_membrane': True\n",
    "            },\n",
    "            'hv_maps': {\n",
    "                'nuclei': original_targets.get('nuclei_hv_map', None),\n",
    "                'membrane': original_targets.get('cell_hv_map', None)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata = {\n",
    "            'original_size': image.shape[-2:],\n",
    "        }\n",
    "        \n",
    "        return self.standardize_output(image, standardized_targets, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187e403-c7b9-4827-bb34-116b3b572e93",
   "metadata": {},
   "source": [
    "## Cell 4 - PanNuke Standardized Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82115407-ecf8-4904-a899-57f3cb0a3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.ndimage import center_of_mass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class PanNukeStandardized(BaseModalityDataset):\n",
    "    def __init__(self, dataset_path: Path, split: str, transforms=None, folds=None, val_ratio=0.1):\n",
    "        super().__init__(dataset_path, split, transforms)\n",
    "        \n",
    "        # Keep existing initialization\n",
    "        if split == 'test':\n",
    "            self.dataset = PanNukeDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                folds=[2],\n",
    "                transforms=transforms,\n",
    "                stardist=False,\n",
    "                regression=False,\n",
    "                cache_dataset=False\n",
    "            )\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "        else:\n",
    "            self.dataset = PanNukeDataset(\n",
    "                dataset_path=dataset_path,\n",
    "                folds=[0, 1],\n",
    "                transforms=transforms,\n",
    "                stardist=False,\n",
    "                regression=False,\n",
    "                cache_dataset=False\n",
    "            )\n",
    "            total_len = len(self.dataset)\n",
    "            val_size = int(total_len * val_ratio)\n",
    "            train_size = total_len - val_size\n",
    "            all_indices = list(range(total_len))\n",
    "            \n",
    "            if split == 'train':\n",
    "                self.indices = all_indices[:train_size]\n",
    "            else:  # val\n",
    "                self.indices = all_indices[train_size:]\n",
    "        \n",
    "        # Cell type mappings\n",
    "        self.cell_types = {\n",
    "            0: 'Background',\n",
    "            1: 'Neoplastic',\n",
    "            2: 'Inflammatory',\n",
    "            3: 'Connective',\n",
    "            4: 'Dead',\n",
    "            5: 'Epithelial'\n",
    "        }\n",
    "        \n",
    "        # Tissue types remain the same\n",
    "        self.tissue_types = {\n",
    "            'Breast': 0, 'Colon': 1, 'Liver': 2, 'Kidney': 3,\n",
    "            'Prostate': 4, 'Bladder': 5, 'Ovarian': 6, 'Uterus': 7,\n",
    "            'Thyroid': 8, 'Testis': 9, 'Lung': 10, 'Esophagus': 11,\n",
    "            'Skin': 12, 'Head&Neck': 13, 'Bile-duct': 14, 'Stomach': 15,\n",
    "            'Pancreatic': 16, 'Adrenal': 17\n",
    "        }\n",
    "        \n",
    "        # Initialize sampling related attributes\n",
    "        self.cell_counts = None\n",
    "        self.tissue_counts = None\n",
    "        \n",
    "    def load_cell_count(self):\n",
    "        \"\"\"Calculate number of images containing each cell type\"\"\"\n",
    "        if self.cell_counts is not None:\n",
    "            return\n",
    "\n",
    "        # Initialize counts for presence of each cell type in images\n",
    "        self.cell_counts = {i: 0 for i in range(1, len(self.cell_types))}  # Skip background (0)\n",
    "\n",
    "        # Count images containing each cell type\n",
    "        for idx in self.indices:\n",
    "            data = self.dataset[idx]\n",
    "            if isinstance(data, (tuple, list)):\n",
    "                targets = data[1]\n",
    "            else:\n",
    "                targets = data.get('targets', {})\n",
    "\n",
    "            cell_types = targets.get('nuclei_type_map', None)\n",
    "            if cell_types is not None:\n",
    "                unique_types = np.unique(cell_types)\n",
    "                # Count presence of each cell type (excluding background)\n",
    "                for cell_type in unique_types:\n",
    "                    if cell_type > 0 and cell_type in self.cell_counts:\n",
    "                        self.cell_counts[cell_type] += 1\n",
    "\n",
    "        print(\"Images containing each cell type:\")\n",
    "        for cell_type, count in self.cell_counts.items():\n",
    "            print(f\"{self.cell_types[cell_type]}: {count} images\")\n",
    "\n",
    "    \n",
    "    def get_sampling_weights_cell(self, gamma=1.5, min_weight=0.1):\n",
    "        \"\"\"\n",
    "        Calculate sampling weights optimized for PanNuke class distribution\n",
    "\n",
    "        Args:\n",
    "            gamma (float): Power factor for rare class boosting (1.5 gives moderate boost)\n",
    "            min_weight (float): Minimum weight for any sample\n",
    "        \"\"\"\n",
    "        if self.cell_counts is None:\n",
    "            self.load_cell_count()\n",
    "\n",
    "        weights = np.ones(len(self), dtype=np.float32)\n",
    "\n",
    "        # Calculate class frequencies\n",
    "        total_samples = sum(self.cell_counts.values())\n",
    "        class_weights = {}\n",
    "\n",
    "        # Calculate base weights with moderate balancing\n",
    "        for cell_type, count in self.cell_counts.items():\n",
    "            if count > 0:\n",
    "                freq = count / total_samples\n",
    "                if freq < 0.1:  # Rare classes (Dead and Epithelial)\n",
    "                    # More moderate boosting for rare classes\n",
    "                    class_weight = (1.0 / freq) ** gamma\n",
    "                    # Add upper limit to prevent excessive oversampling\n",
    "                    class_weight = min(class_weight, 5.0)\n",
    "                else:\n",
    "                    # Slight boost for less common classes\n",
    "                    class_weight = (1.0 / freq) ** 0.75\n",
    "                class_weights[cell_type] = class_weight\n",
    "\n",
    "        # Normalize class weights\n",
    "        max_weight = max(class_weights.values())\n",
    "        class_weights = {k: max(v/max_weight, min_weight) for k, v in class_weights.items()}\n",
    "\n",
    "        # Assign sample weights\n",
    "        for i, idx in enumerate(self.indices):\n",
    "            data = self.dataset[idx]\n",
    "            if isinstance(data, (tuple, list)):\n",
    "                targets = data[1]\n",
    "            else:\n",
    "                targets = data.get('targets', {})\n",
    "\n",
    "            cell_types = targets.get('nuclei_type_map', None)\n",
    "\n",
    "            if cell_types is not None:\n",
    "                unique_types = np.unique(cell_types)\n",
    "                sample_weights = []\n",
    "\n",
    "                for t in unique_types:\n",
    "                    if t > 0:  # Skip background\n",
    "                        weight = class_weights.get(t, min_weight)\n",
    "                        if t in [4, 5]:  # Additional small boost for Dead and Epithelial\n",
    "                            weight *= 1.2\n",
    "                        sample_weights.append(weight)\n",
    "\n",
    "                weights[i] = max(sample_weights) if sample_weights else min_weight\n",
    "            else:\n",
    "                weights[i] = min_weight\n",
    "\n",
    "        # Add minimum weight to ensure all classes are represented\n",
    "        weights = np.maximum(weights, min_weight)\n",
    "\n",
    "        # Final normalization\n",
    "        weights = weights / weights.sum()\n",
    "        return weights\n",
    "\n",
    "    # Keep all existing methods\n",
    "    def _get_modality_type(self) -> str:\n",
    "        return 'pannuke'\n",
    "    \n",
    "    def _get_available_targets(self) -> List[str]:\n",
    "        return ['cell', 'cell_types', 'hv_maps']\n",
    "    \n",
    "    def gen_instance_hv_map(self, inst_map: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Generate horizontal and vertical maps\"\"\"\n",
    "        if len(inst_map.shape) != 2:\n",
    "            raise ValueError(f\"Instance map should be 2D, got shape {inst_map.shape}\")\n",
    "\n",
    "        x_map = np.zeros(inst_map.shape[:2], dtype=np.float32)\n",
    "        y_map = np.zeros(inst_map.shape[:2], dtype=np.float32)\n",
    "\n",
    "        inst_list = list(np.unique(inst_map))\n",
    "        if 0 in inst_list:\n",
    "            inst_list.remove(0)\n",
    "\n",
    "        for inst_id in inst_list:\n",
    "            inst = (inst_map == inst_id).astype(np.uint8)\n",
    "            \n",
    "            rows = np.any(inst, axis=1)\n",
    "            cols = np.any(inst, axis=0)\n",
    "            if not np.any(rows) or not np.any(cols):\n",
    "                continue\n",
    "                \n",
    "            y1, y2 = np.where(rows)[0][[0, -1]]\n",
    "            x1, x2 = np.where(cols)[0][[0, -1]]\n",
    "            \n",
    "            inst_crop = inst[y1:y2+1, x1:x2+1]\n",
    "            \n",
    "            if inst_crop.shape[0] < 2 or inst_crop.shape[1] < 2:\n",
    "                continue\n",
    "\n",
    "            cy, cx = center_of_mass(inst_crop)\n",
    "            \n",
    "            y_coords, x_coords = np.mgrid[0:inst_crop.shape[0], 0:inst_crop.shape[1]]\n",
    "            \n",
    "            x_coords = (x_coords - cx) / (inst_crop.shape[1] - 1) * 2\n",
    "            y_coords = (y_coords - cy) / (inst_crop.shape[0] - 1) * 2\n",
    "            \n",
    "            x_coords[inst_crop == 0] = 0\n",
    "            y_coords[inst_crop == 0] = 0\n",
    "            \n",
    "            x_map[y1:y2+1, x1:x2+1][inst_crop > 0] = x_coords[inst_crop > 0]\n",
    "            y_map[y1:y2+1, x1:x2+1][inst_crop > 0] = y_coords[inst_crop > 0]\n",
    "\n",
    "        return np.stack([x_map, y_map])\n",
    "    \n",
    "    def get_cell_type_name(self, type_idx: int) -> str:\n",
    "        \"\"\"Convert cell type index to name\"\"\"\n",
    "        return self.cell_types.get(type_idx, f'Unknown Type {type_idx}')\n",
    "    \n",
    "    def get_tissue_type_id(self, tissue_name: str) -> int:\n",
    "        \"\"\"Convert tissue name to ID\"\"\"\n",
    "        return self.tissue_types.get(tissue_name, -1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[self.indices[idx]]\n",
    "\n",
    "        if isinstance(data, (tuple, list)):\n",
    "            image = data[0]\n",
    "            original_targets = data[1]\n",
    "            tissue_type = data[2]\n",
    "            img_name = data[3]\n",
    "        else:\n",
    "            image = data['image']\n",
    "            original_targets = data.get('targets', {})\n",
    "            tissue_type = None\n",
    "            img_name = None\n",
    "\n",
    "        # Update cell type handling to account for background class\n",
    "        cell_mask = original_targets.get('instance_map', None)\n",
    "        cell_types = original_targets.get('nuclei_type_map', None)\n",
    "        \n",
    "        # Add 1 to cell types to shift classes (0->1, 1->2, etc.) to match new mapping\n",
    "        if cell_types is not None:\n",
    "            if cell_mask is not None:\n",
    "                cell_types[cell_mask == 0] = 0  # Just ensure background is 0\n",
    "\n",
    "        cell_hv_map = self.gen_instance_hv_map(cell_mask.numpy()) if cell_mask is not None else None\n",
    "        tissue_type_id = self.get_tissue_type_id(tissue_type) if tissue_type else None\n",
    "\n",
    "        standardized_targets = {\n",
    "            'masks': {\n",
    "                'cell': cell_mask,\n",
    "            },\n",
    "            'semantic': {\n",
    "                'cell_types': cell_types,\n",
    "                'tissue_type': tissue_type_id,\n",
    "                'has_membrane': False\n",
    "            },\n",
    "            'hv_maps': {\n",
    "                'cell': torch.tensor(cell_hv_map) if cell_hv_map is not None else None\n",
    "            }\n",
    "        }\n",
    "\n",
    "        metadata = {\n",
    "            'original_size': image.shape[-2:],\n",
    "            'fold': getattr(self.dataset, 'current_fold', None),\n",
    "            'tissue_type': tissue_type,\n",
    "            'tissue_type_id': tissue_type_id,\n",
    "            'img_name': img_name,\n",
    "            'cell_type_mapping': self.cell_types,\n",
    "            'tissue_type_mapping': self.tissue_types\n",
    "        }\n",
    "\n",
    "        return self.standardize_output(image, standardized_targets, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d08bd1-e563-45f4-b44c-515891e8c26c",
   "metadata": {},
   "source": [
    "## Cell 5 - Enhanced Dataset Manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ded321-596b-428b-b73d-27bb9ee63cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModularDatasetManager:\n",
    "    def __init__(self):\n",
    "        # Dataset paths\n",
    "        self.tissuenet_path = Path(\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/tissuenet\")\n",
    "        self.pannuke_path = Path(\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/reassemble\")\n",
    "        \n",
    "        # Config paths\n",
    "        self.tissuenet_config_path = Path(\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/examples/cell_segmentation/vitaminp-tissuenet.yaml\")\n",
    "        self.pannuke_config_path = Path(\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/examples/cell_segmentation/pannuke-vitaminp.yaml\")\n",
    "        \n",
    "        self._load_configs()\n",
    "        self.datasets = {}\n",
    "\n",
    "    def print_dataset_info(self, split: str):\n",
    "        \"\"\"Print information about datasets for a given split\"\"\"\n",
    "        print(f\"\\nDataset information for split '{split}':\")\n",
    "        for modality, splits in self.datasets.items():\n",
    "            if split in splits:\n",
    "                print(f\"{modality}: {len(splits[split])} samples\")\n",
    "    \n",
    "    def _load_configs(self):\n",
    "        try:\n",
    "            with open(self.tissuenet_config_path, 'r') as file:\n",
    "                self.tissuenet_config = yaml.safe_load(file)\n",
    "            with open(self.pannuke_config_path, 'r') as file:\n",
    "                self.pannuke_config = yaml.safe_load(file)\n",
    "            print(\"Successfully loaded configuration files\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading configs: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_transforms(self, transform_settings, input_shape=256):\n",
    "        transforms = []\n",
    "        if input_shape != 256:\n",
    "            transforms.append(A.Resize(input_shape, input_shape))\n",
    "        \n",
    "        transforms.extend([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Downscale(scale_min=0.5, scale_max=1.0, p=0.15),\n",
    "            A.Blur(blur_limit=9, p=0.2),\n",
    "            A.GaussNoise(var_limit=50, p=0.25),\n",
    "            A.ElasticTransform(p=0.2),\n",
    "        ])\n",
    "        \n",
    "        if 'normalize' in transform_settings:\n",
    "            transforms.append(A.Normalize(**transform_settings['normalize']))\n",
    "        \n",
    "        return A.Compose(transforms)\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_collate_fn(batch):\n",
    "       images = []\n",
    "       masks = {\n",
    "           'nuclei': [],\n",
    "           'membrane': [],\n",
    "           'cell': []\n",
    "       }\n",
    "       semantic = {\n",
    "           'has_membrane': [],\n",
    "           'cell_types': [],\n",
    "           'tissue_type': []\n",
    "       }\n",
    "       metadata = {\n",
    "           'original_channels': [],\n",
    "           'sample_metadata': []\n",
    "       }\n",
    "       hv_maps = {\n",
    "           'nuclei': [],\n",
    "           'membrane': [],\n",
    "           'cell': []\n",
    "       }\n",
    "\n",
    "       max_channels = max(item['image'].shape[0] for item in batch)\n",
    "\n",
    "       for item in batch:\n",
    "           # Handle image\n",
    "           image = item['image']\n",
    "           metadata['original_channels'].append(image.shape[0])\n",
    "           if image.shape[0] < max_channels:\n",
    "               padding = torch.zeros((max_channels - image.shape[0], *image.shape[1:]), \n",
    "                                  dtype=image.dtype, device=image.device)\n",
    "               image = torch.cat([image, padding], dim=0)\n",
    "           images.append(image)\n",
    "\n",
    "           # Handle masks\n",
    "           for mask_type in masks:\n",
    "               if mask_type in item['targets']['masks'] and item['targets']['masks'][mask_type] is not None:\n",
    "                   mask = item['targets']['masks'][mask_type]\n",
    "                   if mask.dim() == 2:\n",
    "                       mask = mask.unsqueeze(0)\n",
    "                   masks[mask_type].append(mask)\n",
    "               else:\n",
    "                   zero_mask = torch.zeros((1, *image.shape[-2:]), \n",
    "                                        dtype=image.dtype, \n",
    "                                        device=image.device)\n",
    "                   masks[mask_type].append(zero_mask)\n",
    "\n",
    "           # Handle HV maps\n",
    "           if 'hv_maps' in item['targets']:\n",
    "               for hv_type in hv_maps:\n",
    "                   if hv_type in item['targets']['hv_maps']:\n",
    "                       hv_maps[hv_type].append(item['targets']['hv_maps'][hv_type])\n",
    "                   else:\n",
    "                       zero_map = torch.zeros((2, *image.shape[-2:]), \n",
    "                                           dtype=image.dtype, \n",
    "                                           device=image.device)\n",
    "                       hv_maps[hv_type].append(zero_map)\n",
    "           else:\n",
    "               for hv_type in hv_maps:\n",
    "                   zero_map = torch.zeros((2, *image.shape[-2:]), \n",
    "                                       dtype=image.dtype, \n",
    "                                       device=image.device)\n",
    "                   hv_maps[hv_type].append(zero_map)\n",
    "\n",
    "           # Handle cell_types\n",
    "           if 'cell_types' in item['targets']['semantic']:\n",
    "               cell_types = item['targets']['semantic']['cell_types']\n",
    "               if cell_types.dim() == 2:\n",
    "                   semantic['cell_types'].append(cell_types)\n",
    "               else:\n",
    "                   zeros = torch.zeros((*image.shape[-2:],), dtype=torch.long, device=image.device)\n",
    "                   semantic['cell_types'].append(zeros)\n",
    "           else:\n",
    "               zeros = torch.zeros((*image.shape[-2:],), dtype=torch.long, device=image.device)\n",
    "               semantic['cell_types'].append(zeros)\n",
    "    \n",
    "            \n",
    "           # Boolean values\n",
    "           has_membrane = item['targets']['semantic'].get('has_membrane', False)\n",
    "           semantic['has_membrane'].append(torch.tensor(has_membrane, dtype=torch.bool))\n",
    "\n",
    "           # Tissue type\n",
    "           tissue_type = item['targets']['semantic'].get('tissue_type', 0)\n",
    "           semantic['tissue_type'].append(torch.tensor(tissue_type, dtype=torch.long))\n",
    "\n",
    "           metadata['sample_metadata'].append(item['metadata'])\n",
    "\n",
    "       try:\n",
    "           batch_dict = {\n",
    "               'image': torch.stack(images),\n",
    "               'targets': {\n",
    "                   'masks': {},\n",
    "                   'semantic': {},\n",
    "                   'hv_maps': {}\n",
    "               },\n",
    "               'metadata': {\n",
    "                   'original_channels': metadata['original_channels'],\n",
    "                   'padded_channels': max_channels,\n",
    "                   'sample_metadata': metadata['sample_metadata']\n",
    "               }\n",
    "           }\n",
    "\n",
    "           # Stack masks\n",
    "           for mask_type, mask_list in masks.items():\n",
    "               if any(x is not None for x in mask_list):\n",
    "                   batch_dict['targets']['masks'][mask_type] = torch.stack(mask_list)\n",
    "\n",
    "           # Stack HV maps\n",
    "           for hv_type, maps in hv_maps.items():\n",
    "               if any(x is not None for x in maps):\n",
    "                   batch_dict['targets']['hv_maps'][hv_type] = torch.stack(maps)\n",
    "\n",
    "           # Stack semantic data\n",
    "           batch_dict['targets']['semantic']['cell_types'] = torch.stack(semantic['cell_types'])\n",
    "           batch_dict['targets']['semantic']['has_membrane'] = torch.stack(semantic['has_membrane'])\n",
    "           batch_dict['targets']['semantic']['tissue_type'] = torch.stack(semantic['tissue_type'])\n",
    "\n",
    "           return batch_dict\n",
    "\n",
    "       except Exception as e:\n",
    "           print(\"Error in collate function:\", str(e))\n",
    "           print(\"Image shapes:\", [img.shape for img in images])\n",
    "           print(\"Mask shapes:\", {k: [m.shape for m in v] for k, v in masks.items()})\n",
    "           print(\"HV map shapes:\", {k: [m.shape for m in v] for k, v in hv_maps.items()})\n",
    "           print(\"Semantic values:\", semantic)\n",
    "           raise e\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        print(\"\\nSetting up datasets...\")\n",
    "\n",
    "        # Setup TissueNet\n",
    "        train_transforms = self.get_transforms(self.tissuenet_config['transformations'])\n",
    "        val_transforms = A.Compose([\n",
    "            A.Normalize(**self.tissuenet_config['transformations']['normalize'])\n",
    "        ])\n",
    "\n",
    "        self.datasets['tissuenet'] = {\n",
    "            'train': TissueNetStandardized(\n",
    "                dataset_path=self.tissuenet_path,\n",
    "                split='train',\n",
    "                transforms=train_transforms\n",
    "            ),\n",
    "            'val': TissueNetStandardized(\n",
    "                dataset_path=self.tissuenet_path,\n",
    "                split='val',\n",
    "                transforms=val_transforms\n",
    "            ),\n",
    "            'test': TissueNetStandardized(\n",
    "                dataset_path=self.tissuenet_path,\n",
    "                split='test',\n",
    "                transforms=val_transforms\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Setup PanNuke with explicit test set\n",
    "        self.datasets['pannuke'] = {\n",
    "            'train': PanNukeStandardized(\n",
    "                dataset_path=self.pannuke_path,\n",
    "                split='train',\n",
    "                transforms=train_transforms,\n",
    "                val_ratio=0.1  # 10% for validation\n",
    "            ),\n",
    "            'val': PanNukeStandardized(\n",
    "                dataset_path=self.pannuke_path,\n",
    "                split='val',\n",
    "                transforms=val_transforms,\n",
    "                val_ratio=0.1\n",
    "            ),\n",
    "            'test': PanNukeStandardized(  # Add test split using fold [2]\n",
    "                dataset_path=self.pannuke_path,\n",
    "                split='test',\n",
    "                transforms=val_transforms\n",
    "            )\n",
    "        }\n",
    "\n",
    "        print(\"Dataset setup completed!\")\n",
    "        return self.datasets\n",
    "\n",
    "    def get_dataloader(self, modality: str, split: str, batch_size: int = 32) -> DataLoader:\n",
    "        dataset = self.datasets[modality][split]\n",
    "\n",
    "        # Add weighted sampling for PanNuke training\n",
    "        if modality == 'pannuke' and split == 'train':\n",
    "            # Calculate weights for cell type balancing\n",
    "            dataset.load_cell_count()  # This will now count actual instances\n",
    "            weights = dataset.get_sampling_weights_cell(gamma=0.80)\n",
    "            sampler = WeightedRandomSampler(\n",
    "                weights=weights,\n",
    "                num_samples=len(weights),\n",
    "                replacement=True\n",
    "            )\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                sampler=sampler,  # Use weighted sampler\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                collate_fn=self.custom_collate_fn\n",
    "            )\n",
    "        else:\n",
    "            # Keep original loader for other cases\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=(split == 'train'),\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                collate_fn=self.custom_collate_fn\n",
    "            )\n",
    "\n",
    "    def get_combined_dataloader(self, split: str, batch_size: int = 32) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create a balanced combined dataloader that ensures samples from both datasets\n",
    "        \"\"\"\n",
    "        from torch.utils.data import Dataset\n",
    "        from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "        class BalancedConcatDataset(Dataset):\n",
    "            def __init__(self, datasets, batch_size):\n",
    "                self.datasets = datasets\n",
    "                self.batch_size = batch_size\n",
    "                self.lengths = [len(d) for d in datasets]\n",
    "                self.total_length = max(self.lengths) * len(datasets)\n",
    "\n",
    "                # Calculate weights for PanNuke dataset if in training\n",
    "                self.weights = []\n",
    "                if split == 'train':\n",
    "                    for dataset in datasets:\n",
    "                        if hasattr(dataset, 'load_cell_count'):  # PanNuke dataset\n",
    "                            dataset.load_cell_count()\n",
    "                            self.weights.extend(dataset.get_sampling_weights_cell(gamma=1.0))\n",
    "                        else:  # TissueNet dataset\n",
    "                            self.weights.extend([1.0] * len(dataset))\n",
    "\n",
    "            def __len__(self):\n",
    "                return self.total_length\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                # Determine which dataset to sample from\n",
    "                dataset_idx = idx % len(self.datasets)\n",
    "                # Get item from the corresponding dataset\n",
    "                item_idx = idx // len(self.datasets) % self.lengths[dataset_idx]\n",
    "                return self.datasets[dataset_idx][item_idx]\n",
    "\n",
    "        # Create list of datasets for the given split\n",
    "        datasets = [\n",
    "            self.datasets[modality][split]\n",
    "            for modality in self.datasets\n",
    "            if split in self.datasets[modality]\n",
    "        ]\n",
    "\n",
    "        combined_dataset = BalancedConcatDataset(datasets, batch_size)\n",
    "\n",
    "        if split == 'train':\n",
    "            sampler = WeightedRandomSampler(\n",
    "                weights=combined_dataset.weights,\n",
    "                num_samples=len(combined_dataset.weights),\n",
    "                replacement=True\n",
    "            )\n",
    "\n",
    "            return DataLoader(\n",
    "                combined_dataset,\n",
    "                batch_size=batch_size,\n",
    "                sampler=sampler,  # Use weighted sampler for training\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                collate_fn=self.custom_collate_fn\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                combined_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                collate_fn=self.custom_collate_fn\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25cfda-a9a9-469b-8c65-97a904bbb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# First cell - Run this only once to set up datasets\n",
    "def setup_datasets_and_loaders(batch_size=16):\n",
    "    manager = ModularDatasetManager()\n",
    "    datasets = manager.setup_datasets()\n",
    "    \n",
    "    # Create all dataloaders\n",
    "    loaders = {\n",
    "        'mif': {\n",
    "            'train': manager.get_dataloader('tissuenet', 'train', batch_size),\n",
    "            'val': manager.get_dataloader('tissuenet', 'val', batch_size),\n",
    "            'test': manager.get_dataloader('tissuenet', 'test', batch_size)\n",
    "        },\n",
    "        'he': {\n",
    "            'train': manager.get_dataloader('pannuke', 'train', batch_size),\n",
    "            'val': manager.get_dataloader('pannuke', 'val', batch_size),\n",
    "            'test': manager.get_dataloader('pannuke', 'test', batch_size)\n",
    "        },\n",
    "        'combined': {\n",
    "            'train': manager.get_combined_dataloader('train', batch_size),\n",
    "            'val': manager.get_combined_dataloader('val', batch_size)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return manager, datasets, loaders\n",
    "\n",
    "# Run this once\n",
    "manager, datasets, loaders = setup_datasets_and_loaders(batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bafe0-b118-44b0-b7ff-39a2f0f0a365",
   "metadata": {},
   "source": [
    "## Class imbalence Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67e0f9-0c88-4edd-b351-33c4540ef9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_batch_distribution(loader, num_batches=50):\n",
    "    class_counts = defaultdict(int)\n",
    "    batch_distributions = []\n",
    "    \n",
    "    print(\"Analyzing class distribution across batches...\")\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "            \n",
    "        # Get cell types from the batch\n",
    "        cell_types = batch['targets']['semantic']['cell_types']\n",
    "        \n",
    "        # Count unique cell types in this batch\n",
    "        batch_dist = defaultdict(int)\n",
    "        for img in cell_types:\n",
    "            unique_types = torch.unique(img)\n",
    "            for cell_type in unique_types:\n",
    "                if cell_type != 0:  # Ignore background\n",
    "                    type_idx = cell_type.item()\n",
    "                    class_counts[type_idx] += 1\n",
    "                    batch_dist[type_idx] += 1\n",
    "        \n",
    "        batch_distributions.append(dict(batch_dist))\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Overall distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    classes = list(class_counts.keys())\n",
    "    counts = [class_counts[c] for c in classes]\n",
    "    plt.bar(classes, counts)\n",
    "    plt.title('Overall Class Distribution')\n",
    "    plt.xlabel('Cell Type')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Box plot of per-batch distributions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    data = [[dist.get(c, 0) for dist in batch_distributions] for c in classes]\n",
    "    plt.boxplot(data, labels=classes)\n",
    "    plt.title('Per-batch Distribution')\n",
    "    plt.xlabel('Cell Type')\n",
    "    plt.ylabel('Count per Batch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nClass distribution statistics:\")\n",
    "    total = sum(counts)\n",
    "    for c in classes:\n",
    "        print(f\"Class {c}: {class_counts[c]} ({class_counts[c]/total*100:.2f}%)\")\n",
    "\n",
    "# Test with both original and weighted loaders\n",
    "print(\"Analyzing HE (PanNuke) training loader:\")\n",
    "analyze_batch_distribution(loaders['he']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238285d4-cfbb-4538-ba6e-c03b71be7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# # Initialize the dataset manager\n",
    "# manager = ModularDatasetManager()\n",
    "# manager.setup_datasets()\n",
    "\n",
    "# # Get the PanNuke training dataset\n",
    "# pannuke_dataset = manager.datasets['pannuke']['train']\n",
    "\n",
    "# # Load and print cell counts (now shows images containing each cell type)\n",
    "# pannuke_dataset.load_cell_count()\n",
    "\n",
    "# # Calculate sampling weights\n",
    "# weights = pannuke_dataset.get_sampling_weights_cell(gamma=1.0)\n",
    "\n",
    "# # Create sampler\n",
    "# sampler = WeightedRandomSampler(\n",
    "#     weights=weights,\n",
    "#     num_samples=len(weights),\n",
    "#     replacement=True\n",
    "# )\n",
    "\n",
    "# # Create test dataloader with the sampler\n",
    "# test_loader = DataLoader(\n",
    "#     pannuke_dataset,\n",
    "#     batch_size=8,\n",
    "#     sampler=sampler,\n",
    "#     num_workers=4,\n",
    "#     pin_memory=True,\n",
    "#     collate_fn=manager.custom_collate_fn\n",
    "# )\n",
    "\n",
    "# # Test a few batches and count class presence\n",
    "# class_presence = {i: 0 for i in range(1, 6)}  # Only counting cell types, not background\n",
    "# num_test_batches = 10\n",
    "\n",
    "# print(\"\\nChecking cell type presence in sampled batches:\")\n",
    "# for i, batch in enumerate(test_loader):\n",
    "#     if i >= num_test_batches:\n",
    "#         break\n",
    "        \n",
    "#     cell_types = batch['targets']['semantic']['cell_types']\n",
    "#     for type_vec in cell_types:\n",
    "#         for class_idx, is_present in enumerate(type_vec[1:], 1):  # Skip background\n",
    "#             if is_present:\n",
    "#                 class_presence[class_idx] += 1\n",
    "\n",
    "# print(\"\\nNumber of images containing each cell type in sampled batches:\")\n",
    "# for class_idx, count in class_presence.items():\n",
    "#     class_name = pannuke_dataset.cell_types[class_idx]\n",
    "#     print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd979c89-424d-4f37-9e2f-d51befe31c51",
   "metadata": {},
   "source": [
    "## Cell 6 - Usage Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b7057-1e45-4fd0-b19d-83be39d8d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nTesting combined loader...\")\n",
    "# # Create manager and setup datasets if not already done\n",
    "# manager = ModularDatasetManager()\n",
    "# manager.setup_datasets()\n",
    "\n",
    "# # Print dataset sizes\n",
    "# manager.print_dataset_info('train')\n",
    "\n",
    "# # Test the loader\n",
    "# combined_train_loader = manager.get_combined_dataloader('train', batch_size=4)\n",
    "# batch = next(iter(combined_train_loader))\n",
    "\n",
    "# print(\"\\nBatch structure:\")\n",
    "# print(\"Image shape:\", batch['image'].shape)\n",
    "# print(\"Available masks:\", list(batch['targets']['masks'].keys()))\n",
    "# print(\"Available semantic:\", list(batch['targets']['semantic'].keys()))\n",
    "# print(\"Sample semantic values:\", {k: v.shape if isinstance(v, torch.Tensor) else v \n",
    "#                                 for k, v in batch['targets']['semantic'].items()})\n",
    "\n",
    "# print(\"\\nMask shapes:\")\n",
    "# for mask_type, mask_tensor in batch['targets']['masks'].items():\n",
    "#     print(f\"{mask_type}: {mask_tensor.shape}\")\n",
    "\n",
    "# print(\"\\nOriginal channels per sample:\")\n",
    "# original_channels = batch['metadata']['original_channels']\n",
    "# for idx, channels in enumerate(original_channels):\n",
    "#     dataset_type = \"PanNuke\" if channels == 3 else \"TissueNet\"\n",
    "#     print(f\"Sample {idx}: {channels} channels ({dataset_type})\")\n",
    "\n",
    "# print(\"\\nSemantic values per sample:\")\n",
    "# for idx in range(batch['image'].shape[0]):\n",
    "#     print(f\"\\nSample {idx}:\")\n",
    "#     for key in batch['targets']['semantic']:\n",
    "#         if batch['targets']['semantic'][key] is not None:\n",
    "#             value = batch['targets']['semantic'][key][idx]\n",
    "#             print(f\"  {key}: {value}\")\n",
    "\n",
    "# print(\"\\nMask statistics:\")\n",
    "# for mask_type, mask_tensor in batch['targets']['masks'].items():\n",
    "#     non_zero = torch.count_nonzero(mask_tensor).item()\n",
    "#     total = mask_tensor.numel()\n",
    "#     print(f\"{mask_type}:\")\n",
    "#     print(f\"  - Non-zero elements: {non_zero}\")\n",
    "#     print(f\"  - Coverage: {(non_zero/total)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c335b09-6723-44b3-aae7-8e18b4796799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Check all splits\n",
    "# print(\"\\nChecking all splits:\")\n",
    "# for split in ['train', 'val', 'test']:\n",
    "#     manager.print_dataset_info(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbfb8d-a432-43cb-b9e1-0aba91ce2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check raw data from the base dataset\n",
    "# pannuke_train = manager.datasets['pannuke']['train']\n",
    "# raw_data = pannuke_train.dataset[0]  # Get first sample from base dataset\n",
    "# print(\"\\nRaw data structure:\", type(raw_data))\n",
    "# if isinstance(raw_data, tuple):\n",
    "#     print(\"Tuple contents:\", [type(x) for x in raw_data])\n",
    "#    # Print the actual cell type information\n",
    "#     masks = raw_data[1]\n",
    "#     print(\"\\nAvailable mask keys:\", masks.keys())\n",
    "#     print(\"\\nNuclei type map shape:\", masks['nuclei_type_map'].shape)\n",
    "#     print(\"Unique cell types in nuclei_type_map:\", torch.unique(masks['nuclei_type_map']))\n",
    "\n",
    "# elif isinstance(raw_data, dict):\n",
    "#     print(\"Dict keys:\", raw_data.keys())\n",
    "\n",
    "# # Check cell type mapping\n",
    "# print(\"\\nCell type mapping:\", pannuke_train.cell_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762b1d5-a1c5-40ae-a3be-01ba88e75299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def create_random_colormap(n_instances):\n",
    "    \"\"\"Create a colormap with random distinct colors\"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    colors = np.random.rand(n_instances + 1, 3)  # +1 for background\n",
    "    colors[0] = [0, 0, 0]  # Background black\n",
    "    return ListedColormap(colors)\n",
    "\n",
    "def get_cell_type_name(type_idx):\n",
    "    \"\"\"Convert cell type index to name for PanNuke dataset\"\"\"\n",
    "    class_names = {\n",
    "        0: \"Background\",\n",
    "        1: \"Neoplastic\",\n",
    "        2: \"Inflammatory\",\n",
    "        3: \"Connective\",\n",
    "        4: \"Dead\",\n",
    "        5: \"Epithelial\"\n",
    "    }\n",
    "    return class_names.get(type_idx, f'Unknown Type {type_idx}')\n",
    "\n",
    "def get_cell_type_color(type_idx):\n",
    "    \"\"\"Get color for cell type\"\"\"\n",
    "    type_colors = {\n",
    "        0: 'black',      # Background\n",
    "        1: '#00FF00',    # Neoplastic\n",
    "        2: 'blue',       # Inflammatory\n",
    "        3: 'yellow',     # Connective\n",
    "        4: 'magenta',    # Dead\n",
    "        5: 'cyan'        # Epithelial\n",
    "    }\n",
    "    return type_colors.get(type_idx, 'gray')\n",
    "\n",
    "def visualize_sample(sample, title):\n",
    "    \"\"\"Visualize a single sample with its masks, HV maps and magnitude\"\"\"\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    image = sample['image']\n",
    "    \n",
    "    if image.shape[0] == 2:  # TissueNet\n",
    "        # Channel 1\n",
    "        plt.subplot(251)\n",
    "        img_ch1 = image[0].numpy()\n",
    "        if img_ch1.max() > 1:\n",
    "            img_ch1 = img_ch1 / img_ch1.max()\n",
    "        plt.imshow(img_ch1, cmap='gray')\n",
    "        plt.title('Channel 1')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Channel 2\n",
    "        plt.subplot(252)\n",
    "        img_ch2 = image[1].numpy()\n",
    "        if img_ch2.max() > 1:\n",
    "            img_ch2 = img_ch2 / img_ch2.max()\n",
    "        plt.imshow(img_ch2, cmap='gray')\n",
    "        plt.title('Channel 2')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Cell/Membrane mask\n",
    "        plt.subplot(253)\n",
    "        membrane_mask = sample['targets']['masks']['membrane']\n",
    "        cell_types = sample['targets']['semantic'].get('cell_types', None)\n",
    "        if membrane_mask is not None:\n",
    "            n_instances = len(np.unique(membrane_mask.numpy()))\n",
    "            random_cmap = create_random_colormap(n_instances)\n",
    "            plt.imshow(membrane_mask, cmap=random_cmap)\n",
    "            title_str = f'Cell Instances (n={n_instances-1})'\n",
    "            if cell_types is not None:\n",
    "                type_idx = cell_types.item() if isinstance(cell_types, torch.Tensor) else cell_types\n",
    "                title_str += f'\\n{get_cell_type_name(type_idx)}'\n",
    "                plt.imshow(membrane_mask, cmap=ListedColormap([get_cell_type_color(0), get_cell_type_color(type_idx)]))\n",
    "            plt.title(title_str)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Cell HV horizontal\n",
    "        if 'membrane' in sample['targets'].get('hv_maps', {}):\n",
    "            plt.subplot(254)\n",
    "            plt.imshow(sample['targets']['hv_maps']['membrane'][0], cmap='coolwarm')\n",
    "            plt.title('Cell HV (H)')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Cell HV vertical\n",
    "            plt.subplot(255)\n",
    "            plt.imshow(sample['targets']['hv_maps']['membrane'][1], cmap='coolwarm')\n",
    "            plt.title('Cell HV (V)')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Combined view\n",
    "        plt.subplot(256)\n",
    "        img_ch1 = np.clip((img_ch1 - img_ch1.min()) / (img_ch1.max() - img_ch1.min() + 1e-8), 0, 1)\n",
    "        img_ch2 = np.clip((img_ch2 - img_ch2.min()) / (img_ch2.max() - img_ch2.min() + 1e-8), 0, 1)\n",
    "        \n",
    "        combined_img = np.dstack([\n",
    "            np.zeros_like(img_ch1),\n",
    "            img_ch2 * 0.8,\n",
    "            img_ch1\n",
    "        ])\n",
    "        plt.imshow(combined_img)\n",
    "        plt.title('Combined')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Nuclei mask\n",
    "        plt.subplot(257)\n",
    "        nuclei_mask = sample['targets']['masks']['nuclei']\n",
    "        if nuclei_mask is not None:\n",
    "            n_instances = len(np.unique(nuclei_mask.numpy()))\n",
    "            random_cmap = create_random_colormap(n_instances)\n",
    "            plt.imshow(nuclei_mask, cmap=random_cmap)\n",
    "            plt.title(f'Nuclei Instances (n={n_instances-1})')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Nuclei HV maps\n",
    "        if 'nuclei' in sample['targets'].get('hv_maps', {}):\n",
    "            plt.subplot(258)\n",
    "            plt.imshow(sample['targets']['hv_maps']['nuclei'][0], cmap='coolwarm')\n",
    "            plt.title('Nuclei HV (H)')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(259)\n",
    "            plt.imshow(sample['targets']['hv_maps']['nuclei'][1], cmap='coolwarm')\n",
    "            plt.title('Nuclei HV (V)')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(2,5,10)\n",
    "            nuclei_hv = sample['targets']['hv_maps']['nuclei']\n",
    "            nuclei_hv_magnitude = np.sqrt(nuclei_hv[0]**2 + nuclei_hv[1]**2)\n",
    "            plt.imshow(nuclei_hv_magnitude, cmap='viridis')\n",
    "            plt.title('Nuclei HV Magnitude')\n",
    "            plt.axis('off')\n",
    "            \n",
    "    else:  # PanNuke\n",
    "        # Original image\n",
    "        plt.subplot(251)\n",
    "        plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Instance mask\n",
    "        plt.subplot(252)\n",
    "        cell_mask = sample['targets']['masks']['cell']\n",
    "        if cell_mask is not None:\n",
    "            n_instances = len(np.unique(cell_mask.numpy()))\n",
    "            random_cmap = create_random_colormap(n_instances)\n",
    "            plt.imshow(cell_mask, cmap=random_cmap)\n",
    "            plt.title(f'Cell Instances (n={n_instances-1})')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Cell types visualization\n",
    "        plt.subplot(253)\n",
    "        cell_types = sample['targets']['semantic']['cell_types']\n",
    "        if isinstance(cell_types, torch.Tensor):\n",
    "            if cell_types.dim() == 1:  # One-hot encoded vector\n",
    "                # Create a color-coded mask based on the one-hot vector\n",
    "                type_map = np.zeros_like(cell_mask.numpy())\n",
    "                for type_idx, is_present in enumerate(cell_types, 1):\n",
    "                    if is_present:\n",
    "                        type_map[cell_mask.numpy() > 0] = type_idx\n",
    "                \n",
    "                unique_types = np.unique(type_map)\n",
    "                colors = [get_cell_type_color(t) for t in range(max(unique_types) + 1)]\n",
    "                plt.imshow(type_map, cmap=ListedColormap(colors))\n",
    "                plt.title('Cell Types\\n' + '\\n'.join([get_cell_type_name(t) \n",
    "                                                     for t in unique_types if t != 0]))\n",
    "            else:  # 2D tensor (original format)\n",
    "                type_map = cell_types.numpy()\n",
    "                unique_types = np.unique(type_map)\n",
    "                colors = [get_cell_type_color(t) for t in range(max(unique_types) + 1)]\n",
    "                plt.imshow(type_map, cmap=ListedColormap(colors))\n",
    "                plt.title('Cell Types\\n' + '\\n'.join([get_cell_type_name(t) \n",
    "                                                     for t in unique_types if t != 0]))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Type distribution pie chart\n",
    "        plt.subplot(254)\n",
    "        if isinstance(cell_types, torch.Tensor):\n",
    "            if cell_types.dim() == 1:  # One-hot encoded vector\n",
    "                type_counts = {get_cell_type_name(i+1): count.item() \n",
    "                             for i, count in enumerate(cell_types) if count > 0}\n",
    "            else:  # 2D tensor\n",
    "                unique_types = np.unique(type_map)\n",
    "                type_counts = {get_cell_type_name(t): (type_map == t).sum() \n",
    "                             for t in unique_types if t != 0}\n",
    "            \n",
    "            if type_counts:\n",
    "                colors = [get_cell_type_color(t) for t in range(1, len(type_counts) + 1)]\n",
    "                plt.pie(type_counts.values(), labels=type_counts.keys(), \n",
    "                       autopct='%1.1f%%', colors=colors)\n",
    "                plt.title('Cell Type Distribution')\n",
    "        \n",
    "        # HV maps\n",
    "        if 'cell' in sample['targets'].get('hv_maps', {}):\n",
    "            plt.subplot(255)\n",
    "            cell_hv = sample['targets']['hv_maps']['cell']\n",
    "            cell_hv_magnitude = np.sqrt(cell_hv[0]**2 + cell_hv[1]**2)\n",
    "            plt.imshow(cell_hv_magnitude, cmap='viridis')\n",
    "            plt.title('Cell HV Magnitude')\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test visualizations\n",
    "\n",
    "# Now visualize samples using the existing datasets\n",
    "print(\"\\nTissueNet Samples:\")\n",
    "for i in range(2):\n",
    "    idx = np.random.randint(0, len(datasets['tissuenet']['train']))\n",
    "    sample = datasets['tissuenet']['train'][idx]\n",
    "    visualize_sample(sample, f'TissueNet Train Sample (idx={idx})')\n",
    "\n",
    "print(\"\\nPanNuke Samples:\")\n",
    "for i in range(2):\n",
    "    idx = np.random.randint(0, len(datasets['pannuke']['train']))\n",
    "    sample = datasets['pannuke']['train'][idx]\n",
    "    visualize_sample(sample, f'PanNuke Train Sample (idx={idx})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cada86-37ff-4032-baf2-4f8a0db5ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_datasets(dataset_manager):\n",
    "#     for modality in ['tissuenet', 'pannuke']:\n",
    "#         print(f\"\\n{modality.upper()} Dataset Analysis:\")\n",
    "#         for split in ['train', 'val', 'test']:\n",
    "#             sample = dataset_manager.datasets[modality][split][0]\n",
    "#             print(f\"\\n{split} split:\")\n",
    "#             print(f\"Number of samples: {len(dataset_manager.datasets[modality][split])}\")\n",
    "#             print(f\"Image channels: {sample['image'].shape[0]}\")\n",
    "#             print(\"Available masks:\", list(sample['targets']['masks'].keys()))\n",
    "#             print(\"Available semantic info:\", list(sample['targets']['semantic'].keys()))\n",
    "            \n",
    "#             # Print shapes\n",
    "#             print(\"\\nShapes:\")\n",
    "#             print(f\"Image: {sample['image'].shape}\")\n",
    "#             for mask_type, mask in sample['targets']['masks'].items():\n",
    "#                 if mask is not None:\n",
    "#                     print(f\"{mask_type} mask: {mask.shape}\")\n",
    "\n",
    "# # Usage\n",
    "# dataset_manager = ModularDatasetManager()\n",
    "# dataset_manager.setup_datasets()\n",
    "# analyze_datasets(dataset_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7401e8-9801-488c-8f6f-9bf471b35f58",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73366745-cfbf-4dc6-a3fd-6e31bedb1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseExpert(nn.Module):\n",
    "    \"\"\"Abstract base class for all experts\"\"\"\n",
    "    @property\n",
    "    def input_channels(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bc15d-4541-4a30-9c96-99ad688bcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "from typing import List, Tuple, Literal, OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class Conv2DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv2DBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x))) + 1e-5 * torch.sum(torch.pow(self.conv.weight, 2))\n",
    "\n",
    "class Deconv2DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2):\n",
    "        super(Deconv2DBlock, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.deconv(x)))\n",
    "\n",
    "\n",
    "class SwinEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "        else:\n",
    "            weights = None\n",
    "        self.swin = swin_v2_b(weights=weights)\n",
    "        self.swin.head = nn.Identity()  \n",
    "\n",
    "        # Corrected channel dimensions for extra processing layers\n",
    "        self.extra_processing = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(256, 256),  # Changed from 1024 to match input channels\n",
    "                SEBlock(256),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(512, 512),\n",
    "                SEBlock(512),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(1024, 1024),\n",
    "                SEBlock(1024),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.swin.features):\n",
    "            x = layer(x)\n",
    "            if i in [2, 4, 6, 7]:  \n",
    "                curr_x = x.permute(0, 3, 1, 2)  # Change from [B, H, W, C] to [B, C, H, W]\n",
    "                if len(features) < len(self.extra_processing):\n",
    "                    curr_x = self.extra_processing[len(features)](curr_x)\n",
    "                features.append(curr_x)\n",
    "        return features\n",
    "\n",
    "class FeaturePyramidNetwork(nn.Module):\n",
    "    def __init__(self, in_channels_list, out_channels):\n",
    "        super(FeaturePyramidNetwork, self).__init__()\n",
    "        self.inner_blocks = nn.ModuleList()\n",
    "        self.layer_blocks = nn.ModuleList()\n",
    "        self.extra_blocks = nn.ModuleList()  # New extra processing blocks\n",
    "\n",
    "        for in_channels in in_channels_list:\n",
    "            inner_block_module = nn.Conv2d(in_channels, out_channels, 1)\n",
    "            layer_block_module = nn.Sequential(\n",
    "                Conv2DBlock(out_channels, out_channels),\n",
    "                Conv2DBlock(out_channels, out_channels),\n",
    "                SEBlock(out_channels)\n",
    "            )\n",
    "            extra_block = nn.Sequential(\n",
    "                Conv2DBlock(out_channels, out_channels),\n",
    "                SEBlock(out_channels)\n",
    "            )\n",
    "            \n",
    "            self.inner_blocks.append(inner_block_module)\n",
    "            self.layer_blocks.append(layer_block_module)\n",
    "            self.extra_blocks.append(extra_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        \n",
    "        last_inner = self.inner_blocks[-1](x[-1])\n",
    "        last_inner = self.extra_blocks[-1](last_inner)  # Extra processing\n",
    "        results.append(self.layer_blocks[-1](last_inner))\n",
    "\n",
    "        for feature, inner_block, layer_block, extra_block in zip(\n",
    "            x[:-1][::-1], \n",
    "            self.inner_blocks[:-1][::-1], \n",
    "            self.layer_blocks[:-1][::-1],\n",
    "            self.extra_blocks[:-1][::-1]\n",
    "        ):\n",
    "            if last_inner.shape[-2:] != feature.shape[-2:]:\n",
    "                inner_top_down = F.interpolate(last_inner, size=feature.shape[-2:], mode=\"nearest\")\n",
    "            else:\n",
    "                inner_top_down = last_inner\n",
    "                \n",
    "            inner_lateral = inner_block(feature)\n",
    "            last_inner = inner_lateral + inner_top_down\n",
    "            last_inner = extra_block(last_inner)  # Extra processing\n",
    "            results.insert(0, layer_block(last_inner))\n",
    "\n",
    "        return results\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = Conv2DBlock(F_g, F_int, kernel_size=1, padding=0)\n",
    "        self.W_x = Conv2DBlock(F_l, F_int, kernel_size=1, padding=0)\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.residual = Conv2DBlock(F_g, F_l, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        \n",
    "        if g1.shape[2:] != x1.shape[2:]:\n",
    "            g1 = F.interpolate(g1, size=x1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = x * psi\n",
    "        out = self.residual(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6, bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12, bias=False)\n",
    "        self.conv4 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18, bias=False)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv5 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        self.conv_out = nn.Conv2d(5 * out_channels, out_channels, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat1 = self.conv1(x)\n",
    "        feat2 = self.conv2(x)\n",
    "        feat3 = self.conv3(x)\n",
    "        feat4 = self.conv4(x)\n",
    "        feat5 = self.conv5(self.pool(x))\n",
    "        feat5 = F.interpolate(feat5, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        out = torch.cat((feat1, feat2, feat3, feat4, feat5), dim=1)\n",
    "        out = self.conv_out(out)\n",
    "        out = self.bn(out)\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class GlobalContextBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, pooling_type='att'):\n",
    "        super(GlobalContextBlock, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.pooling_type = pooling_type\n",
    "\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.conv_in = nn.Conv2d(inplanes, planes, kernel_size=1)\n",
    "        self.conv_out = nn.Conv2d(planes, inplanes, kernel_size=1)\n",
    "        \n",
    "        # Replace BatchNorm with LayerNorm\n",
    "        self.ln_in = nn.LayerNorm([planes, 1, 1])\n",
    "        self.ln_out = nn.LayerNorm([inplanes, 1, 1])\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            context_mask = self.conv_mask(x)\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = self.spatial_pool(x)\n",
    "        out = self.conv_in(context)\n",
    "        out = self.ln_in(out)  # Use LayerNorm instead of BatchNorm\n",
    "        out = self.relu(out)\n",
    "        out = self.conv_out(out)\n",
    "        out = self.ln_out(out)  # Use LayerNorm instead of BatchNorm\n",
    "        \n",
    "        return x * out.expand_as(x)\n",
    "\n",
    "class ImprovedDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.2):\n",
    "        super(ImprovedDecoder, self).__init__()\n",
    "        self.aspp = ASPP(in_channels, 256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Enhanced conv blocks with residual connections\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                self._make_dense_block(256 + in_channels, 128),\n",
    "                SEBlock(128),\n",
    "                self.dropout\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                self._make_dense_block(128 + in_channels, 64),\n",
    "                SEBlock(64),\n",
    "                self.dropout\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                self._make_dense_block(64 + in_channels, 32),\n",
    "                SEBlock(32),\n",
    "                self.dropout\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        # Additional processing path\n",
    "        self.extra_processing = nn.ModuleList([\n",
    "            Conv2DBlock(128, 128),\n",
    "            Conv2DBlock(64, 64),\n",
    "            Conv2DBlock(32, 32),\n",
    "        ])\n",
    "        \n",
    "        self.final_conv = nn.Sequential(\n",
    "            Conv2DBlock(32, 32),\n",
    "            nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        )\n",
    "        self.final_upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def _make_dense_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            Conv2DBlock(in_ch, out_ch),\n",
    "            Conv2DBlock(out_ch, out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = self.aspp(features[-1])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i, (feature, conv, extra) in enumerate(zip(\n",
    "            features[-2::-1], \n",
    "            self.conv_blocks, \n",
    "            self.extra_processing\n",
    "        )):\n",
    "            x = F.interpolate(x, size=feature.shape[2:], mode='bilinear', align_corners=False)\n",
    "            x = torch.cat([x, feature], dim=1)\n",
    "            x = conv(x)\n",
    "            x = x + extra(x)  # Residual connection with extra processing\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d3eff-8bb0-44b0-8f10-44d46efed676",
   "metadata": {},
   "source": [
    "## HE Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224def70-14c7-43a4-b847-0914e102c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HEExpert(BaseExpert):\n",
    "    def __init__(self, pretrained=True, num_cell_classes=6):\n",
    "        super().__init__()\n",
    "        # Initialize Swin Transformer backbone\n",
    "        if pretrained:\n",
    "            weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "        else:\n",
    "            weights = None\n",
    "        self.swin = swin_v2_b(weights=weights)\n",
    "        self.swin.head = nn.Identity()\n",
    "        self.num_cell_classes = num_cell_classes\n",
    "        \n",
    "        # Extra processing layers\n",
    "        self.extra_processing = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(256, 256),\n",
    "                SEBlock(256),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(512, 512),\n",
    "                SEBlock(512),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(1024, 1024),\n",
    "                SEBlock(1024),\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Feature Pyramid Network\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=[256, 512, 1024, 1024],\n",
    "            out_channels=256\n",
    "        )\n",
    "        \n",
    "        # Enhanced decoders\n",
    "        self.cell_seg = ImprovedDecoder(256, 2)      # Binary segmentation for cells\n",
    "        self.cell_type = ImprovedDecoder(256, 6)     # Cell type prediction\n",
    "        self.cell_hv = ImprovedDecoder(256, 2)       # HV maps for cells\n",
    "        \n",
    "        # Global context enhancement\n",
    "        self.global_context = nn.Sequential(\n",
    "            GlobalContextBlock(1024, 256),\n",
    "            SEBlock(1024)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        curr_x = x\n",
    "        for i, layer in enumerate(self.swin.features):\n",
    "            curr_x = layer(curr_x)\n",
    "            if i in [2, 4, 6, 7]:\n",
    "                feature = curr_x.permute(0, 3, 1, 2)\n",
    "                if len(features) < len(self.extra_processing):\n",
    "                    feature = self.extra_processing[len(features)](feature)\n",
    "                features.append(feature)\n",
    "        \n",
    "        fpn_features = self.fpn(features)\n",
    "        global_feature = self.global_context(features[-1])\n",
    "        \n",
    "        return {\n",
    "            'cell_mask': self.cell_seg(fpn_features),     # [B, 2, H, W] - [background, cell]\n",
    "            'cell_types': self.cell_type(fpn_features),   # [B, 6, H, W] - One channel per type\n",
    "            'cell_hv': self.cell_hv(fpn_features),        # [B, 2, H, W] - Horizontal/Vertical\n",
    "            'gate_weights': global_feature\n",
    "        }\n",
    "\n",
    "    def calculate_instance_map(self, predictions, magnification=40):\n",
    "        \"\"\"Convert network outputs to instance segmentation map\"\"\"\n",
    "        pred_map = torch.cat([\n",
    "            torch.argmax(predictions['cell_types'], dim=1, keepdim=True),    # Type prediction\n",
    "            torch.argmax(predictions['cell_mask'], dim=1, keepdim=True),     # Binary mask\n",
    "            predictions['cell_hv']                                           # HV maps\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Move to numpy and rearrange for post-processor\n",
    "        pred_map = pred_map.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        cell_post_processor = DetectionCellPostProcessor(\n",
    "            nr_types=self.num_cell_classes, \n",
    "            magnification=magnification,\n",
    "            gt=False\n",
    "        )\n",
    "        \n",
    "        instance_preds = []\n",
    "        type_preds = []\n",
    "        \n",
    "        for i in range(pred_map.shape[0]):\n",
    "            instance_pred, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map[i])\n",
    "            instance_preds.append(instance_pred)\n",
    "            type_preds.append(type_pred)\n",
    "            \n",
    "        return torch.tensor(np.stack(instance_preds)), type_preds\n",
    "    \n",
    "    @property\n",
    "    def input_channels(self):\n",
    "        return 3\n",
    "        \n",
    "    def freeze_encoder(self):\n",
    "        \"\"\"Freeze encoder to not train it\"\"\"\n",
    "        for param in self.swin.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_encoder(self):\n",
    "        \"\"\"Unfreeze encoder to train the whole model\"\"\"\n",
    "        for param in self.swin.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51347d5-31a4-4d83-aa05-34fc3700ae82",
   "metadata": {},
   "source": [
    "## MIF Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acbd03-6852-4493-b651-819097ee987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIFExpert(BaseExpert):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Initialize Swin Transformer backbone\n",
    "        if pretrained:\n",
    "            weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "        else:\n",
    "            weights = None\n",
    "        self.swin = swin_v2_b(weights=weights)\n",
    "        self.swin.head = nn.Identity()\n",
    "        \n",
    "        # Modify first conv layer for 2-channel MIF input \n",
    "        original_weight = self.swin.features[0][0].weight\n",
    "        self.swin.features[0][0] = nn.Conv2d(2, 128, kernel_size=(4, 4), stride=(4, 4))\n",
    "        with torch.no_grad():\n",
    "            self.swin.features[0][0].weight = nn.Parameter(\n",
    "                original_weight[:, :2, :, :].clone()\n",
    "            )\n",
    "            \n",
    "        # Extra processing layers\n",
    "        self.extra_processing = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(256, 256),\n",
    "                SEBlock(256),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(512, 512),\n",
    "                SEBlock(512),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                Conv2DBlock(1024, 1024),\n",
    "                SEBlock(1024),\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Feature Pyramid Network\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=[256, 512, 1024, 1024],\n",
    "            out_channels=256\n",
    "        )\n",
    "        \n",
    "        # Enhanced decoders - now outputting 2 channels each for binary segmentation\n",
    "        self.nuclei_seg = ImprovedDecoder(256, 2)     # [background, nuclei]\n",
    "        self.membrane_seg = ImprovedDecoder(256, 2)   # [background, membrane]\n",
    "        self.nuclei_hv = ImprovedDecoder(256, 2)      # HV maps for nuclei\n",
    "        self.membrane_hv = ImprovedDecoder(256, 2)    # HV maps for membrane\n",
    "        \n",
    "        # Global context enhancement\n",
    "        self.global_context = nn.Sequential(\n",
    "            GlobalContextBlock(1024, 256),\n",
    "            SEBlock(1024)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = []\n",
    "        curr_x = x\n",
    "        for i, layer in enumerate(self.swin.features):\n",
    "            curr_x = layer(curr_x)\n",
    "            if i in [2, 4, 6, 7]:\n",
    "                feature = curr_x.permute(0, 3, 1, 2)\n",
    "                if len(features) < len(self.extra_processing):\n",
    "                    feature = self.extra_processing[len(features)](feature)\n",
    "                features.append(feature)\n",
    "                \n",
    "        fpn_features = self.fpn(features)\n",
    "        global_feature = self.global_context(features[-1])\n",
    "        \n",
    "        return {\n",
    "            'nuclei_mask': self.nuclei_seg(fpn_features),      # [B, 2, H, W] - [background, nuclei]\n",
    "            'membrane_mask': self.membrane_seg(fpn_features),   # [B, 2, H, W] - [background, membrane]\n",
    "            'nuclei_hv': self.nuclei_hv(fpn_features),         # [B, 2, H, W] - horizontal/vertical\n",
    "            'membrane_hv': self.membrane_hv(fpn_features),      # [B, 2, H, W] - horizontal/vertical\n",
    "            'gate_weights': global_feature\n",
    "        }\n",
    "\n",
    "    def calculate_instance_map(self, predictions, tissue_type=\"nuclei\", magnification=40):\n",
    "        \"\"\"Convert network outputs to instance segmentation map\"\"\"\n",
    "        results = []\n",
    "        type_preds = []\n",
    "        \n",
    "        for i in range(predictions[f'{tissue_type}_mask'].shape[0]):\n",
    "            # Stack predictions for post-processing - following their approach\n",
    "            pred_map = np.concatenate([\n",
    "                np.zeros((256, 256, 1)),  # Type map (not used for MIF)\n",
    "                torch.argmax(predictions[f'{tissue_type}_mask'][i], dim=0).cpu().numpy()[..., None],\n",
    "                predictions[f'{tissue_type}_hv'][i].cpu().numpy().transpose(1, 2, 0)\n",
    "            ], axis=-1)\n",
    "            \n",
    "            # Initialize post-processor\n",
    "            post_processor = DetectionCellPostProcessor(\n",
    "                nr_types=1,  # Binary for MIF\n",
    "                magnification=magnification, \n",
    "                gt=False\n",
    "            )\n",
    "            \n",
    "            # Get instance predictions\n",
    "            instance_pred, type_pred = post_processor.post_process_cell_segmentation(pred_map)\n",
    "            results.append(instance_pred)\n",
    "            type_preds.append(type_pred)\n",
    "            \n",
    "        return torch.tensor(np.stack(results)), type_preds\n",
    "    \n",
    "    @property\n",
    "    def input_channels(self):\n",
    "        return 2\n",
    "        \n",
    "    def freeze_encoder(self):\n",
    "        \"\"\"Freeze encoder to not train it\"\"\"\n",
    "        for param in self.swin.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_encoder(self):\n",
    "        \"\"\"Unfreeze encoder to train the whole model\"\"\"\n",
    "        for param in self.swin.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92c3b4-2b7a-4a6c-821f-e2143b06e6f9",
   "metadata": {},
   "source": [
    "## Gating Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec5bfd-df27-4f45-9f46-af94cb80a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, base_temperature=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractors (equal depth and capacity for both modalities)\n",
    "        self.mif_features = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=3, padding=1),  # Increased initial channels\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.he_features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Stronger strength detector\n",
    "        self.strength_detector = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Light dropout for stability\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Enhanced decision network\n",
    "        self.decision_network = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "        self.base_temperature = base_temperature\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features based on input type\n",
    "        if x.shape[1] == 2:  # MIF input\n",
    "            features = self.mif_features(x)\n",
    "            base_bias = 0.5  # Keep MIF bias same as it's working well\n",
    "        else:  # H&E input\n",
    "            features = self.he_features(x)\n",
    "            base_bias = -0.6  # Slightly stronger bias for H&E\n",
    "            \n",
    "        # Get feature strength with more emphasis\n",
    "        strength = self.strength_detector(features)\n",
    "        strength = strength * 1.2  # Amplify strength detection\n",
    "        \n",
    "        # Get logits\n",
    "        logits = self.decision_network(features)\n",
    "        \n",
    "        # Apply bias with proper broadcasting and stronger effect for H&E\n",
    "        if x.shape[1] == 3:  # H&E input\n",
    "            bias = base_bias * strength * 1.1  # Extra scaling for H&E\n",
    "        else:\n",
    "            bias = base_bias * strength\n",
    "            \n",
    "        logits = logits + torch.cat([bias, -bias], dim=1)\n",
    "        \n",
    "        # Apply temperature with proper broadcasting\n",
    "        # Slightly higher temperature scaling for H&E\n",
    "        if x.shape[1] == 3:\n",
    "            adaptive_temp = self.base_temperature + (3.5 * strength)  # Higher temp for H&E\n",
    "        else:\n",
    "            adaptive_temp = self.base_temperature + (3.0 * strength)\n",
    "            \n",
    "        logits = logits * adaptive_temp\n",
    "        \n",
    "        return F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa1543-9eb9-4679-bea1-707c00b50a76",
   "metadata": {},
   "source": [
    "## Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a19a4-403e-4cdc-a6ab-99087c3381a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalExpertModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mif_expert = MIFExpert(pretrained=True)  # Specify pretrained parameter\n",
    "        self.he_expert = HEExpert()\n",
    "        self.gating_network = GatingNetwork()\n",
    "        \n",
    "    def forward(self, x, modality=None):\n",
    "        # Get gating weights\n",
    "        gate_weights = self.gating_network(x)\n",
    "\n",
    "        # Modality-specific training\n",
    "        if modality is not None:\n",
    "            if modality == 'mif':\n",
    "                return {**self.mif_expert(x), 'gate_weights': gate_weights}\n",
    "            elif modality == 'he':\n",
    "                return {**self.he_expert(x), 'gate_weights': gate_weights}\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown modality: {modality}\")\n",
    "\n",
    "        # Get outputs from both experts with appropriate input preprocessing\n",
    "        mif_output = self.mif_expert(x if x.shape[1] == 2 else x[:, :2])\n",
    "        he_output = self.he_expert(x if x.shape[1] == 3 else torch.cat([x, torch.zeros_like(x[:, :1])], dim=1))\n",
    "        \n",
    "        # Helper function to combine tensors with weights\n",
    "        def weighted_combine(tensor1, tensor2, weights):\n",
    "            return weights[:, 0:1].view(-1, 1, 1, 1) * tensor1 + \\\n",
    "                   weights[:, 1:2].view(-1, 1, 1, 1) * tensor2\n",
    "        \n",
    "        # Combine outputs based on gating weights\n",
    "        combined_output = {}\n",
    "        \n",
    "        # Combine segmentation masks\n",
    "        if 'nuclei_mask' in mif_output and 'cell_mask' in he_output:\n",
    "            combined_output['segmentation'] = weighted_combine(\n",
    "                mif_output['nuclei_mask'],\n",
    "                he_output['cell_mask'],\n",
    "                gate_weights\n",
    "            )\n",
    "        \n",
    "        # Combine HV maps\n",
    "        if 'nuclei_hv' in mif_output and 'cell_hv' in he_output:\n",
    "            combined_output['hv_maps'] = weighted_combine(\n",
    "                mif_output['nuclei_hv'],\n",
    "                he_output['cell_hv'],\n",
    "                gate_weights\n",
    "            )\n",
    "        \n",
    "        # Add cell type predictions from HE expert\n",
    "        if 'cell_types' in he_output:\n",
    "            combined_output['cell_types'] = he_output['cell_types']\n",
    "        \n",
    "        # Store gating weights for analysis\n",
    "        combined_output['gate_weights'] = gate_weights\n",
    "        \n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae17b7-f00f-45b9-b426-9989f5fe4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multimodal_model(loaders, print_gates=True, num_batches=2):\n",
    "    model = MultiModalExpertModel()\n",
    "    model.eval()\n",
    "    \n",
    "    def analyze_gating(outputs, input_type, expect_soft=False):\n",
    "        gates = outputs['gate_weights']\n",
    "        avg_weights = gates.mean(dim=0)\n",
    "        print(f\"\\n{input_type} Gating Analysis:\")\n",
    "        print(f\"Average weights: MIF Expert: {avg_weights[0]:.3f}, H&E Expert: {avg_weights[1]:.3f}\")\n",
    "        \n",
    "        # Analyze gating behavior\n",
    "        weight_diff = abs(avg_weights[0] - avg_weights[1])\n",
    "        if expect_soft:\n",
    "            is_correct = weight_diff < 0.5\n",
    "            print(f\"Soft Gating Status: {' BALANCED' if is_correct else ' TOO EXTREME'}\")\n",
    "        else:\n",
    "            if input_type.startswith(\"MIF\"):\n",
    "                is_correct = avg_weights[0] > avg_weights[1]\n",
    "            else:\n",
    "                is_correct = avg_weights[1] > avg_weights[0]\n",
    "            print(f\"Gating Status: {' CORRECT' if is_correct else ' INCORRECT'} preference\")\n",
    "        \n",
    "        print(f\"Weight difference: {weight_diff:.3f}\")\n",
    "        print(\"\\nPer-sample gating weights:\")\n",
    "        for i, gate in enumerate(gates):\n",
    "            print(f\"Sample {i}: MIF: {gate[0]:.3f}, H&E: {gate[1]:.3f}\")\n",
    "        \n",
    "        return weight_diff, gates\n",
    "    \n",
    "    # Test with real data\n",
    "    print(\"\\n=== Testing MIF Data ===\")\n",
    "    mif_metrics = []\n",
    "    for i, batch in enumerate(loaders['mif']['test']):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        inputs = batch['image']\n",
    "        print(f\"\\nBatch {i+1} - Input shape: {inputs.shape}\")\n",
    "        outputs = model(inputs)\n",
    "        weight_diff, gates = analyze_gating(outputs, \"MIF\")\n",
    "        mif_metrics.append((weight_diff, gates))\n",
    "        \n",
    "        print(\"\\nOutput shapes:\")\n",
    "        for k, v in outputs.items():\n",
    "            if k != 'gate_weights':\n",
    "                print(f\"{k}: {v.shape}\")\n",
    "    \n",
    "    print(\"\\n=== Testing H&E Data ===\")\n",
    "    he_metrics = []\n",
    "    for i, batch in enumerate(loaders['he']['test']):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        inputs = batch['image']\n",
    "        print(f\"\\nBatch {i+1} - Input shape: {inputs.shape}\")\n",
    "        outputs = model(inputs)\n",
    "        weight_diff, gates = analyze_gating(outputs, \"H&E\")\n",
    "        he_metrics.append((weight_diff, gates))\n",
    "        \n",
    "        print(\"\\nOutput shapes:\")\n",
    "        for k, v in outputs.items():\n",
    "            if k != 'gate_weights':\n",
    "                print(f\"{k}: {v.shape}\")\n",
    "    \n",
    "    # Test combined data\n",
    "    print(\"\\n=== Testing Combined Data ===\")\n",
    "    for i, batch in enumerate(loaders['combined']['val']):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        inputs = batch['image']\n",
    "        # Infer modality from input channels\n",
    "        modality = 'MIF' if inputs.shape[1] == 2 else 'H&E'\n",
    "        print(f\"\\nBatch {i+1} - Input shape: {inputs.shape}, Inferred Modality: {modality}\")\n",
    "        outputs = model(inputs)\n",
    "        analyze_gating(outputs, f\"Combined ({modality})\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calc_gating_metrics(metrics):\n",
    "        all_gates = torch.cat([m[1] for m in metrics])\n",
    "        avg_weight_diff = sum(m[0] for m in metrics) / len(metrics)\n",
    "        entropy = -(all_gates * torch.log(all_gates + 1e-10)).sum(dim=1).mean()\n",
    "        return avg_weight_diff, entropy\n",
    "    \n",
    "    print(\"\\n=== Overall Gating Metrics ===\")\n",
    "    mif_diff, mif_entropy = calc_gating_metrics(mif_metrics)\n",
    "    he_diff, he_entropy = calc_gating_metrics(he_metrics)\n",
    "    \n",
    "    print(f\"MIF Data - Avg Weight Difference: {mif_diff:.3f}, Entropy: {mif_entropy:.3f}\")\n",
    "    print(f\"H&E Data - Avg Weight Difference: {he_diff:.3f}, Entropy: {he_entropy:.3f}\")\n",
    "    \n",
    "    print(\"\\n=== Overall Gating Evaluation ===\")\n",
    "    avg_entropy = (mif_entropy + he_entropy) / 2\n",
    "    avg_diff = (mif_diff + he_diff) / 2\n",
    "    \n",
    "    print(f\"Average Entropy: {avg_entropy:.3f}\")\n",
    "    print(f\"Average Weight Difference: {avg_diff:.3f}\")\n",
    "    \n",
    "    print(\"\\nGating Behavior Assessment:\")\n",
    "    if avg_entropy > 0.5 and avg_diff < 0.3:\n",
    "        print(\" Good soft gating behavior\")\n",
    "    else:\n",
    "        print(\" Suboptimal soft gating:\")\n",
    "        if avg_entropy <= 0.5:\n",
    "            print(\"- Consider reducing temperature\")\n",
    "        if avg_diff >= 0.3:\n",
    "            print(\"- Consider adding entropy regularization\")\n",
    "\n",
    "# Run the test\n",
    "test_multimodal_model(loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01a9fe-0f9f-460b-8b1b-aa096529ada1",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ad2ef-a0cb-4689-93e3-db4dbe4aa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MSGELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSGELoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        dy_pred, dx_pred = self.compute_gradient(pred)\n",
    "        dy_target, dx_target = self.compute_gradient(target)\n",
    "        loss = F.mse_loss(dy_pred, dy_target) + F.mse_loss(dx_pred, dx_target)\n",
    "        return loss\n",
    "\n",
    "    def compute_gradient(self, x):\n",
    "        dy = x[:, :, 1:, :] - x[:, :, :-1, :]\n",
    "        dx = x[:, :, :, 1:] - x[:, :, :, :-1]\n",
    "        return dy, dx\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, recall_boost=0.7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.recall_boost = recall_boost\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Convert to proper format\n",
    "        binary_targets = (targets > 0).squeeze(1).long()  # Remove channel dim and convert to long\n",
    "        \n",
    "        # Ensure inputs are in correct format (B, C, H, W)\n",
    "        if inputs.shape[1] != 2:  # If not 2 channels, adjust input\n",
    "            inputs = torch.cat([1-inputs, inputs], dim=1)  # Create background and foreground channels\n",
    "            \n",
    "        ce_loss = F.cross_entropy(inputs, binary_targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_weight = self.alpha * (1-pt)**self.gamma\n",
    "        \n",
    "        pred_labels = torch.argmax(inputs, dim=1)\n",
    "        false_negative_mask = (binary_targets != pred_labels) & (binary_targets == 1)\n",
    "        boost_weights = torch.ones_like(ce_loss, device=ce_loss.device)\n",
    "        boost_weights[false_negative_mask] += self.recall_boost\n",
    "        \n",
    "        loss = focal_weight * ce_loss * boost_weights\n",
    "        return loss.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5, recall_weight=1.5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.recall_weight = recall_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Convert to proper format\n",
    "        binary_targets = (targets > 0).squeeze(1).long()  # Remove channel dim\n",
    "        \n",
    "        # Ensure inputs are in correct format (B, C, H, W)\n",
    "        if inputs.shape[1] != 2:  # If not 2 channels, adjust input\n",
    "            inputs = torch.cat([1-inputs, inputs], dim=1)  # Create background and foreground channels\n",
    "            \n",
    "        inputs = F.softmax(inputs, dim=1)\n",
    "        \n",
    "        # Convert targets to one-hot\n",
    "        targets_one_hot = F.one_hot(binary_targets, num_classes=2).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Calculate intersection and other terms\n",
    "        intersection = (inputs * targets_one_hot).sum(dim=(2, 3))\n",
    "        false_negatives = (targets_one_hot * (1 - inputs)).sum(dim=(2, 3))\n",
    "        false_positives = ((1 - targets_one_hot) * inputs).sum(dim=(2, 3))\n",
    "        \n",
    "        numerator = 2 * intersection + self.smooth\n",
    "        denominator = (inputs.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3)) + \n",
    "                      self.recall_weight * false_negatives + false_positives + self.smooth)\n",
    "        \n",
    "        dice = numerator / denominator\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, gate_supervision_weight=0.05, gate_entropy_weight=0.1, gate_smoothing=0.2):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_loss = DiceLoss(recall_weight=1.5)\n",
    "        self.focal_loss = FocalLoss(alpha=0.25, gamma=2, recall_boost=0.7)\n",
    "        self.hv_loss = MSGELoss()\n",
    "        self.cell_type_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        \n",
    "        self.lambda_dice = 1.0\n",
    "        self.lambda_focal = 1.0\n",
    "        self.lambda_type = 1.0\n",
    "        self.lambda_hv = 2.0\n",
    "        self.gate_supervision_weight = gate_supervision_weight\n",
    "        self.gate_entropy_weight = gate_entropy_weight\n",
    "        self.gate_smoothing = gate_smoothing\n",
    "\n",
    "    def normalize_masks(self, mask):\n",
    "        \"\"\"Normalize mask values to [0,1] range\"\"\"\n",
    "        if mask is not None:\n",
    "            return torch.clamp(mask.float(), 0, 1)\n",
    "        return mask\n",
    "        \n",
    "    def compute_entropy_regularization(self, gate_logits):\n",
    "        # Compute entropy using logits\n",
    "        probs = F.softmax(gate_logits, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        return -self.gate_entropy_weight * entropy\n",
    "        \n",
    "    def __call__(self, outputs, targets, modality):\n",
    "        losses_dict = {}\n",
    "        total_loss = 0\n",
    "\n",
    "        \n",
    "        if modality == 'mif':\n",
    "            mif_hv_loss = 0\n",
    "            nuclei_loss = 0\n",
    "            membrane_loss = 0\n",
    "            \n",
    "            if 'nuclei_mask' in outputs and 'nuclei' in targets['masks']:\n",
    "                nuclei_target = self.normalize_masks(targets['masks']['nuclei'])\n",
    "                nuclei_dice = self.lambda_dice * self.dice_loss(outputs['nuclei_mask'], nuclei_target)\n",
    "                nuclei_focal = self.lambda_focal * self.focal_loss(outputs['nuclei_mask'], nuclei_target)\n",
    "                nuclei_loss = nuclei_dice + nuclei_focal\n",
    "                total_loss += nuclei_loss\n",
    "                losses_dict['nuclei_loss'] = float(nuclei_loss)\n",
    "            \n",
    "            if 'membrane_mask' in outputs and 'membrane' in targets['masks']:\n",
    "                membrane_target = self.normalize_masks(targets['masks']['membrane'])\n",
    "                membrane_dice = self.lambda_dice * self.dice_loss(outputs['membrane_mask'], membrane_target)\n",
    "                membrane_focal = self.lambda_focal * self.focal_loss(outputs['membrane_mask'], membrane_target)\n",
    "                membrane_loss = membrane_dice + membrane_focal\n",
    "                total_loss += membrane_loss\n",
    "                losses_dict['membrane_loss'] = float(membrane_loss)\n",
    "            \n",
    "            if 'hv_maps' in targets:\n",
    "                if 'nuclei' in targets['hv_maps']:\n",
    "                    nuclei_hv = self.lambda_hv * (\n",
    "                        self.hv_loss(outputs['nuclei_hv'], targets['hv_maps']['nuclei']) +\n",
    "                        F.mse_loss(outputs['nuclei_hv'], targets['hv_maps']['nuclei'])\n",
    "                    )\n",
    "                    mif_hv_loss += nuclei_hv\n",
    "                    total_loss += nuclei_hv\n",
    "                if 'membrane' in targets['hv_maps']:\n",
    "                    membrane_hv = self.lambda_hv * (\n",
    "                        self.hv_loss(outputs['membrane_hv'], targets['hv_maps']['membrane']) +\n",
    "                        F.mse_loss(outputs['membrane_hv'], targets['hv_maps']['membrane'])\n",
    "                    )\n",
    "                    mif_hv_loss += membrane_hv\n",
    "                    total_loss += membrane_hv\n",
    "                losses_dict['mif_hv_loss'] = float(mif_hv_loss)\n",
    "                \n",
    "        else:  # HE modality\n",
    "            he_hv_loss = 0\n",
    "            cell_loss = 0\n",
    "            \n",
    "            if 'cell_mask' in outputs and 'cell' in targets['masks']:\n",
    "                cell_target = self.normalize_masks(targets['masks']['cell'])\n",
    "                cell_dice = self.lambda_dice * self.dice_loss(outputs['cell_mask'], cell_target)\n",
    "                cell_focal = self.lambda_focal * self.focal_loss(outputs['cell_mask'], cell_target)\n",
    "                cell_loss = cell_dice + cell_focal\n",
    "                total_loss += cell_loss\n",
    "                losses_dict['cell_loss'] = float(cell_loss)\n",
    "            \n",
    "            if 'cell_hv' in outputs and 'hv_maps' in targets and 'cell' in targets['hv_maps']:\n",
    "                cell_hv = self.lambda_hv * (\n",
    "                    self.hv_loss(outputs['cell_hv'], targets['hv_maps']['cell']) +\n",
    "                    F.mse_loss(outputs['cell_hv'], targets['hv_maps']['cell'])\n",
    "                )\n",
    "                he_hv_loss = cell_hv\n",
    "                total_loss += cell_hv\n",
    "                losses_dict['he_hv_loss'] = float(he_hv_loss)\n",
    "            \n",
    "            if 'cell_types' in outputs and 'semantic' in targets and 'cell_types' in targets['semantic']:\n",
    "                cell_type_target = targets['semantic']['cell_types']\n",
    "                if len(cell_type_target.shape) == 2:  # Global labels [B, C]\n",
    "                    B, C, H, W = outputs['cell_types'].shape\n",
    "                    cell_type_target = cell_type_target.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H, W)\n",
    "                    cell_type_loss = self.lambda_type * F.binary_cross_entropy_with_logits(\n",
    "                        outputs['cell_types'],\n",
    "                        cell_type_target.float()\n",
    "                    )\n",
    "                else:  # Pixel-wise labels [B, H, W]\n",
    "                    cell_type_loss = self.lambda_type * self.cell_type_loss(\n",
    "                        outputs['cell_types'],\n",
    "                        cell_type_target.long()\n",
    "                    )\n",
    "                total_loss += cell_type_loss\n",
    "                losses_dict['cell_type_loss'] = float(cell_type_loss)\n",
    "    \n",
    "\n",
    "        if 'gate_weights' in outputs:\n",
    "            target_gates = torch.zeros_like(outputs['gate_weights'])\n",
    "            primary_weight = 1.0 - self.gate_smoothing\n",
    "            secondary_weight = self.gate_smoothing\n",
    "            \n",
    "            if modality == 'mif':\n",
    "                target_gates[:, 0] = primary_weight\n",
    "                target_gates[:, 1] = secondary_weight\n",
    "            else:\n",
    "                target_gates[:, 0] = secondary_weight\n",
    "                target_gates[:, 1] = primary_weight\n",
    "            \n",
    "            # Use binary_cross_entropy_with_logits instead\n",
    "            gate_loss = self.gate_supervision_weight * F.binary_cross_entropy_with_logits(\n",
    "                outputs['gate_weights'],\n",
    "                target_gates\n",
    "            )\n",
    "            \n",
    "            # Compute entropy using logits\n",
    "            entropy_loss = self.compute_entropy_regularization(outputs['gate_weights'])\n",
    "            \n",
    "            total_loss += gate_loss + entropy_loss\n",
    "            losses_dict['gate_loss'] = float(gate_loss)\n",
    "            losses_dict['entropy_loss'] = float(entropy_loss)\n",
    "        \n",
    "        losses_dict['total_loss'] = float(total_loss)\n",
    "        return total_loss, losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39783486-d706-4f42-85b4-1a1f6b9008d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gating_network():\n",
    "    print(\"\\n=== Testing Gating Network ===\")\n",
    "    model = MultiModalExpertModel()\n",
    "    model.eval()\n",
    "    \n",
    "    def analyze_gating_behavior(gates, input_type):\n",
    "        avg_weights = gates.mean(dim=0)\n",
    "        entropy = -(F.softmax(gates, dim=1) * F.log_softmax(gates, dim=1)).sum(dim=1).mean()\n",
    "        weight_diff = abs(avg_weights[0] - avg_weights[1])\n",
    "        \n",
    "        print(f\"\\n{input_type} Gating Analysis:\")\n",
    "        print(f\"Average weights: MIF Expert: {avg_weights[0]:.3f}, H&E Expert: {avg_weights[1]:.3f}\")\n",
    "        print(f\"Gating Entropy: {entropy:.3f}\")\n",
    "        \n",
    "        # Evaluate gating behavior\n",
    "        if input_type.startswith(\"MIF\"):\n",
    "            is_correct = avg_weights[0] > avg_weights[1]\n",
    "            ideal_diff = 0.3  # Reduced from previous strict threshold\n",
    "        else:\n",
    "            is_correct = avg_weights[1] > avg_weights[0]\n",
    "            ideal_diff = 0.3\n",
    "            \n",
    "        status = \" CORRECT\" if is_correct else \" INCORRECT\"\n",
    "        softness = \"GOOD\" if weight_diff < ideal_diff else \"TOO EXTREME\"\n",
    "        print(f\"Gating Status: {status} preference, Softness: {softness}\")\n",
    "        print(f\"Weight difference: {weight_diff:.3f}\")\n",
    "        \n",
    "        # Detailed sample analysis\n",
    "        print(\"\\nPer-sample gating weights and entropy:\")\n",
    "        for i, gate in enumerate(gates):\n",
    "            sample_entropy = -(F.softmax(gate) * F.log_softmax(gate)).sum()\n",
    "            print(f\"Sample {i}: MIF: {gate[0]:.3f}, H&E: {gate[1]:.3f}, Entropy: {sample_entropy:.3f}\")\n",
    "        \n",
    "        return entropy, weight_diff\n",
    "\n",
    "    # Test settings\n",
    "    batch_size = 4\n",
    "    criterion = CombinedLoss(\n",
    "        gate_supervision_weight=0.05,  # Reduced supervision\n",
    "        gate_entropy_weight=0.1,       # Added entropy weight\n",
    "        gate_smoothing=0.2             # Added smoothing\n",
    "    )\n",
    "    \n",
    "    # Test with MIF input\n",
    "    print(\"\\n=== Testing MIF Data ===\")\n",
    "    mif_input = torch.randn(batch_size, 2, 256, 256)\n",
    "    mif_target = {\n",
    "        'masks': {\n",
    "            'nuclei': torch.randint(0, 2, (batch_size, 1, 256, 256)),\n",
    "            'membrane': torch.randint(0, 2, (batch_size, 1, 256, 256))\n",
    "        },\n",
    "        'hv_maps': {\n",
    "            'nuclei': torch.randn(batch_size, 2, 256, 256),\n",
    "            'membrane': torch.randn(batch_size, 2, 256, 256)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"MIF Input shape: {mif_input.shape}\")\n",
    "    mif_outputs = model(mif_input, modality='mif')\n",
    "    mif_entropy, mif_diff = analyze_gating_behavior(mif_outputs['gate_weights'], \"MIF\")\n",
    "    \n",
    "    # Test MIF loss with new components\n",
    "    mif_loss, mif_loss_dict = criterion(mif_outputs, mif_target, modality='mif')\n",
    "    print(\"\\nMIF Loss Components:\")\n",
    "    for k, v in mif_loss_dict.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    # Test with H&E input\n",
    "    print(\"\\n=== Testing H&E Data ===\")\n",
    "    he_input = torch.randn(batch_size, 3, 256, 256)\n",
    "    he_target = {\n",
    "        'masks': {\n",
    "            'cell': torch.randint(0, 2, (batch_size, 1, 256, 256))\n",
    "        },\n",
    "        'hv_maps': {\n",
    "            'cell': torch.randn(batch_size, 2, 256, 256)\n",
    "        },\n",
    "        'semantic': {\n",
    "            'cell_types': torch.randint(0, 6, (batch_size, 256, 256))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"H&E Input shape: {he_input.shape}\")\n",
    "    he_outputs = model(he_input, modality='he')\n",
    "    he_entropy, he_diff = analyze_gating_behavior(he_outputs['gate_weights'], \"H&E\")\n",
    "    \n",
    "    # Test H&E loss with new components\n",
    "    he_loss, he_loss_dict = criterion(he_outputs, he_target, modality='he')\n",
    "    print(\"\\nH&E Loss Components:\")\n",
    "    for k, v in he_loss_dict.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    # Overall gating evaluation\n",
    "    print(\"\\n=== Overall Gating Evaluation ===\")\n",
    "    avg_entropy = (mif_entropy + he_entropy) / 2\n",
    "    avg_diff = (mif_diff + he_diff) / 2\n",
    "    print(f\"Average Entropy: {avg_entropy:.3f}\")\n",
    "    print(f\"Average Weight Difference: {avg_diff:.3f}\")\n",
    "    \n",
    "    # Evaluate gating behavior\n",
    "    print(\"\\nGating Behavior Assessment:\")\n",
    "    if avg_entropy > 0.5 and avg_diff < 0.3:\n",
    "        print(\" Good soft gating behavior\")\n",
    "    else:\n",
    "        print(\" Suboptimal gating behavior:\")\n",
    "        if avg_entropy <= 0.5:\n",
    "            print(\"- Entropy too low, consider increasing gate_entropy_weight\")\n",
    "        if avg_diff >= 0.3:\n",
    "            print(\"- Gating too extreme, consider:\")\n",
    "            print(\"  * Increasing gate_smoothing\")\n",
    "            print(\"  * Reducing gate_supervision_weight\")\n",
    "            print(\"  * Adjusting base temperature in GatingNetwork\")\n",
    "    \n",
    "    # Test edge cases\n",
    "    print(\"\\n=== Testing Edge Cases ===\")\n",
    "    \n",
    "    # Small batch\n",
    "    small_batch = torch.randn(1, 2, 256, 256)\n",
    "    small_output = model(small_batch, modality='mif')\n",
    "    print(\" Small batch processing successful\")\n",
    "    \n",
    "    # Different spatial dimensions\n",
    "    diff_size = torch.randn(batch_size, 2, 128, 128)\n",
    "    diff_output = model(diff_size, modality='mif')\n",
    "    print(\" Different input size processing successful\")\n",
    "    \n",
    "    print(\"\\n=== Testing Complete ===\")\n",
    "\n",
    "# Run the test\n",
    "test_gating_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343e5e3-2121-48d2-bb6d-ccc1ff447c11",
   "metadata": {},
   "source": [
    "## METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f83f5d-1b64-481d-8ea6-75f464f6b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.measure import label\n",
    "from torchmetrics.functional import dice\n",
    "from torchmetrics.functional.classification import binary_jaccard_index\n",
    "\n",
    "def calculate_metrics(predictions, targets, phase='train'):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for both training and validation phases.\n",
    "    \n",
    "    Args:\n",
    "        predictions: dict containing model outputs\n",
    "        targets: dict containing ground truth\n",
    "        phase: 'train' or 'val'\n",
    "    Returns:\n",
    "        dict containing calculated metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'binary_dice_scores': [],\n",
    "        'binary_jaccard_scores': []\n",
    "    }\n",
    "    \n",
    "    # Handle binary segmentation metrics\n",
    "    if 'cell_mask' in predictions or 'nuclei_binary_map' in predictions:\n",
    "        # Get the appropriate prediction key\n",
    "        pred_key = 'cell_mask' if 'cell_mask' in predictions else 'nuclei_binary_map'\n",
    "        target_key = 'cell' if 'cell' in targets['masks'] else 'nuclei_binary_map'\n",
    "        \n",
    "        # Get predictions and resize if necessary\n",
    "        pred_binary = predictions[pred_key]\n",
    "        if pred_binary.shape[-2:] != targets['masks'][target_key].shape[-2:]:\n",
    "            pred_binary = F.interpolate(\n",
    "                pred_binary,\n",
    "                size=targets['masks'][target_key].shape[-2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        # Convert to binary predictions\n",
    "        pred_binary = torch.argmax(pred_binary, dim=1)\n",
    "        target_binary = targets['masks'][target_key].squeeze(1)\n",
    "        target_binary = (target_binary > 0).long()\n",
    "\n",
    "        # Calculate metrics for each sample in batch\n",
    "        for i in range(pred_binary.shape[0]):\n",
    "            # Convert to boolean tensors for accurate calculation\n",
    "            pred_mask = pred_binary[i].bool()\n",
    "            target_mask = target_binary[i].bool()\n",
    "\n",
    "            # Calculate intersection and union\n",
    "            intersection = (pred_mask & target_mask).sum().float()\n",
    "            union = (pred_mask | target_mask).sum().float()\n",
    "            pred_sum = pred_mask.sum().float()\n",
    "            target_sum = target_mask.sum().float()\n",
    "\n",
    "            # Calculate Dice score: 2|AB|/(|A|+|B|)\n",
    "            dice_score = (2.0 * intersection + 1e-6) / (pred_sum + target_sum + 1e-6)\n",
    "            \n",
    "            # Calculate Jaccard score: |AB|/|AB|\n",
    "            jaccard_score = (intersection + 1e-6) / (union + 1e-6)\n",
    "\n",
    "            metrics['binary_dice_scores'].append(float(dice_score.cpu()))\n",
    "            metrics['binary_jaccard_scores'].append(float(jaccard_score.cpu()))\n",
    "\n",
    "        # Add validation-specific metrics\n",
    "        if phase == 'val':\n",
    "            metrics['pq_scores'] = []\n",
    "            metrics['cell_type_pq_scores'] = []\n",
    "            \n",
    "            # Convert tensors to numpy for PQ calculation\n",
    "            pred_binary_np = pred_binary.cpu().numpy()\n",
    "            target_binary_np = target_binary.cpu().numpy()\n",
    "            \n",
    "            for i in range(pred_binary.shape[0]):\n",
    "                # Generate instance maps\n",
    "                pred_instance = label(pred_binary_np[i])\n",
    "                target_instance = label(target_binary_np[i])\n",
    "                \n",
    "                # Remap labels for consistency\n",
    "                pred_instance = remap_label(pred_instance)\n",
    "                target_instance = remap_label(target_instance)\n",
    "                \n",
    "                # Calculate PQ\n",
    "                [_, _, pq], _ = get_fast_pq(true=target_instance, pred=pred_instance)\n",
    "                metrics['pq_scores'].append(pq)\n",
    "                \n",
    "                # Calculate type-specific metrics if available\n",
    "                if 'cell_types' in predictions:\n",
    "                    pred_types = torch.argmax(predictions['cell_types'], dim=1).cpu().numpy()[i]\n",
    "                    target_types = targets['semantic']['cell_types'].cpu().numpy()[i]\n",
    "                    \n",
    "                    type_pq_scores = []\n",
    "                    for type_idx in range(1, predictions['cell_types'].shape[1]):\n",
    "                        pred_type = (pred_types == type_idx).astype(np.uint8)\n",
    "                        target_type = (target_types == type_idx).astype(np.uint8)\n",
    "                        \n",
    "                        if np.max(target_type) == 0:\n",
    "                            type_pq_scores.append(np.nan)\n",
    "                            continue\n",
    "                            \n",
    "                        pred_type = remap_label(label(pred_type))\n",
    "                        target_type = remap_label(label(target_type))\n",
    "                        [_, _, type_pq], _ = get_fast_pq(true=target_type, pred=pred_type, match_iou=0.5)\n",
    "                        type_pq_scores.append(type_pq)\n",
    "                    \n",
    "                    metrics['cell_type_pq_scores'].append(type_pq_scores)\n",
    "\n",
    "    # Calculate mean scores\n",
    "    metrics['mean_dice'] = np.mean(metrics['binary_dice_scores'])\n",
    "    metrics['mean_jaccard'] = np.mean(metrics['binary_jaccard_scores'])\n",
    "    \n",
    "    if phase == 'val' and 'pq_scores' in metrics:\n",
    "        metrics['mean_pq'] = np.nanmean(metrics['pq_scores'])\n",
    "        if 'cell_type_pq_scores' in metrics:\n",
    "            metrics['mean_type_pq'] = np.nanmean([np.nanmean(scores) for scores in metrics['cell_type_pq_scores']])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5861df9-6628-4821-bb1d-0eff529c806e",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cace7-cf5b-4d06-a356-0bac0a19c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from skimage.measure import label\n",
    "from torchmetrics.functional import dice\n",
    "from torchmetrics.functional.classification import binary_jaccard_index\n",
    "from cell_segmentation.utils.metrics import get_fast_pq, remap_label\n",
    "\n",
    "class BaseTrainer:\n",
    "    def __init__(self, model, optimizer, loss_fn, device, expert_type, checkpoint_dir='checkpoints'):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.expert_type = expert_type\n",
    "        self.checkpoint_dir = os.path.join(checkpoint_dir, expert_type)\n",
    "        self.best_loss = float('inf')\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, metrics, is_best=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': metrics['loss'],\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Saved periodic checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_model_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"Saved best model to {best_model_path}\")\n",
    "        \n",
    "        # Save final model\n",
    "        if epoch == self.total_epochs - 1:\n",
    "            final_model_path = os.path.join(self.checkpoint_dir, 'final_model.pt')\n",
    "            torch.save(checkpoint, final_model_path)\n",
    "            print(f\"Saved final model to {final_model_path}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "class ExpertTrainer(BaseTrainer):\n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        train_metrics = defaultdict(list)\n",
    "        losses = defaultdict(float)\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f'Training {self.expert_type.upper()} Expert')\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Filter data based on expert type\n",
    "            if self.expert_type == 'mif' and batch['image'].shape[1] != 2:\n",
    "                continue\n",
    "            if self.expert_type == 'he' and batch['image'].shape[1] != 3:\n",
    "                continue\n",
    "\n",
    "            images = batch['image'].to(self.device)\n",
    "            targets = {\n",
    "                'masks': {k: v.to(self.device) if v is not None else None \n",
    "                         for k, v in batch['targets']['masks'].items()},\n",
    "                'semantic': {k: v.to(self.device) if v is not None else None \n",
    "                            for k, v in batch['targets']['semantic'].items()},\n",
    "                'hv_maps': {k: v.to(self.device) if v is not None else None \n",
    "                           for k, v in batch['targets'].get('hv_maps', {}).items()}\n",
    "            }\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss, loss_dict = self.loss_fn(outputs, targets, self.expert_type)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Calculate training metrics\n",
    "            if self.expert_type == 'he':\n",
    "                batch_metrics = calculate_metrics(\n",
    "                    predictions=outputs,\n",
    "                    targets=targets,\n",
    "                    phase='train'\n",
    "                )\n",
    "                \n",
    "                for key, value in batch_metrics.items():\n",
    "                    if isinstance(value, list):\n",
    "                        train_metrics[key].extend(value)\n",
    "                    else:\n",
    "                        train_metrics[key].append(value)\n",
    "\n",
    "            # Track losses\n",
    "            for k, v in loss_dict.items():\n",
    "                losses[k] += float(v)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            if self.expert_type == 'he':\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{epoch_loss/num_batches:.4f}',\n",
    "                    'dice': f'{np.mean(train_metrics[\"binary_dice_scores\"][-batch[\"image\"].shape[0]:]):.4f}',\n",
    "                    'jaccard': f'{np.mean(train_metrics[\"binary_jaccard_scores\"][-batch[\"image\"].shape[0]:]):.4f}'\n",
    "                })\n",
    "            else:\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{epoch_loss/num_batches:.4f}',\n",
    "                    'nuclei_loss': f'{losses.get(\"nuclei_loss\", 0)/num_batches:.4f}',\n",
    "                    'membrane_loss': f'{losses.get(\"membrane_loss\", 0)/num_batches:.4f}',\n",
    "                    'hv_loss': f'{losses.get(\"mif_hv_loss\", 0)/num_batches:.4f}'\n",
    "                })\n",
    "\n",
    "        metrics = {\n",
    "            'loss': epoch_loss / num_batches,\n",
    "            **{k: v/num_batches for k, v in losses.items()}\n",
    "        }\n",
    "        \n",
    "        if self.expert_type == 'he':\n",
    "            metrics.update({\n",
    "                'dice': np.mean(train_metrics['binary_dice_scores']),\n",
    "                'jaccard': np.mean(train_metrics['binary_jaccard_scores']),\n",
    "                'mean_dice': np.mean(train_metrics.get('mean_dice', [0])),\n",
    "                'mean_jaccard': np.mean(train_metrics.get('mean_jaccard', [0]))\n",
    "            })\n",
    "            \n",
    "            print(\"\\nTraining Metrics:\")\n",
    "            print(f\"Overall Loss: {metrics['loss']:.4f}\")\n",
    "            print(f\"Dice Score: {metrics['dice']:.4f}\")\n",
    "            print(f\"Jaccard Index: {metrics['jaccard']:.4f}\")\n",
    "\n",
    "        self.current_epoch += 1\n",
    "        return metrics\n",
    "\n",
    "class GatingTrainer(BaseTrainer):\n",
    "    def __init__(self, model, optimizer, loss_fn, device, checkpoint_dir='checkpoints'):\n",
    "        super().__init__(model, optimizer, loss_fn, device, 'gating', checkpoint_dir)\n",
    "    \n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        losses = defaultdict(float)\n",
    "        \n",
    "        if hasattr(self.loss_fn, 'update_epoch'):\n",
    "            self.loss_fn.update_epoch(self.current_epoch)\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f'Training GATING Expert')\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(self.device)\n",
    "            targets = {\n",
    "                'masks': {k: v.to(self.device) if v is not None else None \n",
    "                         for k, v in batch['targets']['masks'].items()},\n",
    "                'semantic': {k: v.to(self.device) if v is not None else None \n",
    "                            for k, v in batch['targets']['semantic'].items()},\n",
    "                'hv_maps': {k: v.to(self.device) if v is not None else None \n",
    "                           for k, v in batch['targets'].get('hv_maps', {}).items()}\n",
    "            }\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                modality = 'mif' if images.shape[1] == 2 else 'he'\n",
    "                loss, loss_dict = self.loss_fn(outputs, targets, modality)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            for k, v in loss_dict.items():\n",
    "                losses[k] += float(v)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{epoch_loss/num_batches:.4f}',\n",
    "                **{k: f'{v/num_batches:.4f}' for k, v in losses.items()}\n",
    "            })\n",
    "\n",
    "        metrics = {\n",
    "            'loss': epoch_loss / num_batches,\n",
    "            **{k: v/num_batches for k, v in losses.items()}\n",
    "        }\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        return metrics\n",
    "def validate(model, val_loader, loss_fn, device, expert_type):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    num_batches = 0\n",
    "    val_metrics = defaultdict(list)\n",
    "    \n",
    "    class_names = {\n",
    "        1: 'Neoplastic',\n",
    "        2: 'Inflammatory', \n",
    "        3: 'Connective',\n",
    "        4: 'Dead',\n",
    "        5: 'Epithelial'\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            # Filter data based on expert type\n",
    "            if expert_type == 'mif' and batch['image'].shape[1] != 2:\n",
    "                continue\n",
    "            if expert_type == 'he' and batch['image'].shape[1] != 3:\n",
    "                continue\n",
    "            if expert_type == 'gating':\n",
    "                modality = 'mif' if batch['image'].shape[1] == 2 else 'he'\n",
    "                \n",
    "            images = batch['image'].to(device)\n",
    "            targets = {\n",
    "                'masks': {k: v.to(device) if v is not None else None \n",
    "                         for k, v in batch['targets']['masks'].items()},\n",
    "                'semantic': {k: v.to(device) if v is not None else None \n",
    "                            for k, v in batch['targets']['semantic'].items()},\n",
    "                'hv_maps': {k: v.to(device) if v is not None else None \n",
    "                           for k, v in batch['targets'].get('hv_maps', {}).items()}\n",
    "            }\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Handle different expert types\n",
    "            if expert_type == 'gating':\n",
    "                loss, loss_dict = loss_fn(outputs, targets, modality)\n",
    "                \n",
    "                # Calculate gating metrics\n",
    "                gates = outputs['gate_weights']\n",
    "                entropy = -(F.softmax(gates, dim=1) * F.log_softmax(gates, dim=1)).sum(dim=1).mean()\n",
    "                avg_weights = F.softmax(gates, dim=1).mean(dim=0)\n",
    "                weight_diff = abs(avg_weights[0] - avg_weights[1])\n",
    "                \n",
    "                val_metrics['gate_entropy'].append(float(entropy))\n",
    "                val_metrics['weight_diff'].append(float(weight_diff))\n",
    "                val_metrics['mif_weight'].append(float(avg_weights[0]))\n",
    "                val_metrics['he_weight'].append(float(avg_weights[1]))\n",
    "            else:\n",
    "                loss, loss_dict = loss_fn(outputs, targets, expert_type)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Track loss components\n",
    "            for k, v in loss_dict.items():\n",
    "                val_metrics[f'loss_{k}'].append(float(v))\n",
    "            \n",
    "            # Calculate additional metrics based on expert type\n",
    "            if expert_type == 'he':\n",
    "                batch_metrics = calculate_metrics(\n",
    "                    predictions=outputs,\n",
    "                    targets=targets,\n",
    "                    phase='val'\n",
    "                )\n",
    "                \n",
    "                for key, value in batch_metrics.items():\n",
    "                    if isinstance(value, list):\n",
    "                        val_metrics[key].extend(value)\n",
    "                    else:\n",
    "                        val_metrics[key].append(value)\n",
    "                        \n",
    "            num_batches += 1\n",
    "\n",
    "    # Calculate final metrics\n",
    "    metrics = {\n",
    "        'val_loss': val_loss / num_batches\n",
    "    }\n",
    "\n",
    "    # Add expert-specific metrics\n",
    "    if expert_type == 'gating':\n",
    "        metrics.update({\n",
    "            'val_gate_entropy': np.mean(val_metrics['gate_entropy']),\n",
    "            'val_weight_diff': np.mean(val_metrics['weight_diff']),\n",
    "            'val_mif_weight': np.mean(val_metrics['mif_weight']),\n",
    "            'val_he_weight': np.mean(val_metrics['he_weight'])\n",
    "        })\n",
    "        \n",
    "        print(\"\\nGating Network Validation Metrics:\")\n",
    "        print(f\"Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(f\"Gate Entropy: {metrics['val_gate_entropy']:.4f}\")\n",
    "        print(f\"Weight Difference: {metrics['val_weight_diff']:.4f}\")\n",
    "        print(f\"Average Weights - MIF: {metrics['val_mif_weight']:.4f}, H&E: {metrics['val_he_weight']:.4f}\")\n",
    "        \n",
    "    elif expert_type == 'he':\n",
    "        metrics.update({\n",
    "            'val_dice': np.nanmean(val_metrics.get('binary_dice_scores', [0])),\n",
    "            'val_jaccard': np.nanmean(val_metrics.get('binary_jaccard_scores', [0])),\n",
    "            'val_pq': np.nanmean(val_metrics.get('pq_scores', [0])),\n",
    "        })\n",
    "        \n",
    "        # Add cell type PQ scores\n",
    "        if 'cell_type_pq_scores' in val_metrics:\n",
    "            type_pq_scores = np.array(val_metrics['cell_type_pq_scores'])\n",
    "            for class_idx, class_name in class_names.items():\n",
    "                class_scores = type_pq_scores[:, class_idx-1]\n",
    "                metrics[f'val_{class_name}_pq'] = np.nanmean(class_scores)\n",
    "        \n",
    "        # Add loss components\n",
    "        for k in loss_dict.keys():\n",
    "            if f'loss_{k}' in val_metrics:\n",
    "                metrics[f'val_{k}'] = np.mean(val_metrics[f'loss_{k}'])\n",
    "        \n",
    "        print(\"\\nH&E Expert Validation Metrics:\")\n",
    "        print(f\"Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(f\"Dice Score: {metrics['val_dice']:.4f}\")\n",
    "        print(f\"Jaccard Index: {metrics['val_jaccard']:.4f}\")\n",
    "        print(f\"PQ Score: {metrics['val_pq']:.4f}\")\n",
    "        \n",
    "        if 'cell_type_pq_scores' in val_metrics:\n",
    "            print(\"\\nPer-class PQ Scores:\")\n",
    "            for class_name in class_names.values():\n",
    "                if f'val_{class_name}_pq' in metrics:\n",
    "                    print(f\"{class_name}: {metrics[f'val_{class_name}_pq']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_model(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, device, expert_type, checkpoint_dir='checkpoints'):\n",
    "    # Choose appropriate trainer based on expert_type\n",
    "    if expert_type == 'gating':\n",
    "        trainer = GatingTrainer(model, optimizer, loss_fn, device, checkpoint_dir)\n",
    "    else:\n",
    "        trainer = ExpertTrainer(model, optimizer, loss_fn, device, expert_type, checkpoint_dir)\n",
    "    \n",
    "    trainer.total_epochs = num_epochs\n",
    "    training_history = defaultdict(list)\n",
    "    \n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    \n",
    "    # Split dataset for validation if needed\n",
    "    if val_loader is None and expert_type != 'gating':\n",
    "        dataset_size = len(train_loader.dataset)\n",
    "        val_size = int(0.1 * dataset_size)\n",
    "        train_size = dataset_size - val_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            train_loader.dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=train_loader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=train_loader.num_workers,\n",
    "            pin_memory=train_loader.pin_memory,\n",
    "            collate_fn=train_loader.collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=train_loader.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=train_loader.num_workers,\n",
    "            pin_memory=train_loader.pin_memory,\n",
    "            collate_fn=train_loader.collate_fn\n",
    "        )\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(os.path.join(trainer.checkpoint_dir, 'best_model.pt')):\n",
    "        start_epoch, best_loss = trainer.load_checkpoint(\n",
    "            os.path.join(trainer.checkpoint_dir, 'best_model.pt')\n",
    "        )\n",
    "        trainer.best_loss = best_loss\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_metrics = trainer.train_epoch(train_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        for key, value in train_metrics.items():\n",
    "            training_history[key].append(value)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}:\")\n",
    "        for k, v in train_metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        if expert_type != 'gating' and ((epoch + 1) % 10 == 0 or epoch == num_epochs - 1):\n",
    "            val_metrics = validate(model, val_loader, loss_fn, device, expert_type)\n",
    "            print(\"\\nValidation metrics:\")\n",
    "            for k, v in val_metrics.items():\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "                training_history[k].append(v)\n",
    "            \n",
    "            is_best = val_metrics['val_loss'] < trainer.best_loss\n",
    "            if is_best:\n",
    "                trainer.best_loss = val_metrics['val_loss']\n",
    "        else:\n",
    "            is_best = train_metrics['loss'] < trainer.best_loss\n",
    "            if is_best:\n",
    "                trainer.best_loss = train_metrics['loss']\n",
    "        \n",
    "        trainer.save_checkpoint(\n",
    "            epoch,\n",
    "            {**train_metrics, 'epoch': epoch, 'expert_type': expert_type},\n",
    "            is_best\n",
    "        )\n",
    "        \n",
    "        # Early stopping check\n",
    "        if expert_type != 'gating' and 'val_metrics' in locals():\n",
    "            if val_metrics['val_loss'] > trainer.best_loss * 1.5:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    # Save training history\n",
    "    json_safe_history = {}\n",
    "    for k, v in training_history.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            json_safe_history[k] = [float(x) if hasattr(x, 'item') else x for x in v]\n",
    "        else:\n",
    "            json_safe_history[k] = float(v) if hasattr(v, 'item') else v\n",
    "\n",
    "    final_info = {\n",
    "        'expert_type': expert_type,\n",
    "        'final_loss': float(trainer.best_loss),\n",
    "        'training_history': json_safe_history,\n",
    "        'model_path': os.path.join(trainer.checkpoint_dir, 'best_model.pt')\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(trainer.checkpoint_dir, 'training_info.json'), 'w') as f:\n",
    "        json.dump(final_info, f, indent=4)\n",
    "    \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba624cb-a402-4c28-ada3-657712e86fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "\n",
    "def inspect_model_outputs(model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Test MIF input\n",
    "    mif_input = torch.randn(1, 2, 256, 256).to(device)\n",
    "    print(\"\\nTesting MIF input:\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(mif_input, 'mif')\n",
    "        print(\"Gate weights for MIF:\", outputs['gate_weights'].cpu().numpy())\n",
    "        for k, v in outputs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                print(f\"{k}: {list(v.shape)}\")\n",
    "    \n",
    "    # Test HE input\n",
    "    he_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "    print(\"\\nTesting H&E input:\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(he_input, 'he')\n",
    "        print(\"Gate weights for HE:\", outputs['gate_weights'].cpu().numpy())\n",
    "        for k, v in outputs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                print(f\"{k}: {list(v.shape)}\")\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiModalExpertModel().to(device)\n",
    "\n",
    "inspect_model_outputs(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a9c7d-482d-4b33-bd0b-d457a6aefc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DummyDataLoader:\n",
    "    def __init__(self, batch_size=4, num_batches=10, modality='he'):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.modality = modality\n",
    "        self.pin_memory = True\n",
    "        self.num_workers = 0\n",
    "        self.collate_fn = None\n",
    "        self.dataset = [create_dummy_data(batch_size, modality) for _ in range(num_batches)]\n",
    "        self.current_batch = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.current_batch < self.num_batches:\n",
    "            batch = self.dataset[self.current_batch]\n",
    "            self.current_batch += 1\n",
    "            return batch\n",
    "        raise StopIteration\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "class CombinedDummyLoader:\n",
    "    def __init__(self, batch_size=4, num_batches=10):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.pin_memory = True\n",
    "        self.num_workers = 0\n",
    "        self.collate_fn = None\n",
    "        # Create alternating MIF and H&E batches\n",
    "        self.dataset = []\n",
    "        for i in range(num_batches):\n",
    "            modality = 'mif' if i % 2 == 0 else 'he'\n",
    "            self.dataset.append(create_dummy_data(batch_size, modality))\n",
    "        self.current_batch = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.current_batch < self.num_batches:\n",
    "            batch = self.dataset[self.current_batch]\n",
    "            self.current_batch += 1\n",
    "            return batch\n",
    "        raise StopIteration\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "def create_combined_dummy_loader(batch_size=4, num_batches=10):\n",
    "    \"\"\"Creates a dataloader that alternates between MIF and H&E data\"\"\"\n",
    "    return CombinedDummyLoader(batch_size, num_batches)\n",
    "\n",
    "def test_training_loop(model, device, num_epochs=2):\n",
    "    print(\"\\n=== Testing Training Loop ===\")\n",
    "    \n",
    "    # Create dummy dataloaders with combined data\n",
    "    train_loader = create_combined_dummy_loader(batch_size=4, num_batches=10)\n",
    "    val_loader = create_combined_dummy_loader(batch_size=4, num_batches=5)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = CombinedLoss(\n",
    "        gate_supervision_weight=0.2,\n",
    "        gate_entropy_weight=0.15,\n",
    "        gate_smoothing=0.2\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            device=device,\n",
    "            expert_type='gating',\n",
    "            checkpoint_dir='checkpoints_dummy'\n",
    "        )\n",
    "        print(\"\\nTraining loop completed successfully!\")\n",
    "        print(\"Training history:\", history)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in training loop: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiModalExpertModel().to(device)\n",
    "    \n",
    "    # Run tests\n",
    "    test_model_with_dummy_data(model, device)\n",
    "    torch.cuda.empty_cache()  # Clear memory between tests\n",
    "    test_training_loop(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e030acd-b87b-4258-a050-a7d8e5eefbd6",
   "metadata": {},
   "source": [
    "## full dataset train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8df01-1bd3-4a11-b1b8-822d88dc3118",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d11270-b980-4752-b3b8-91afb662584e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Setup directories and parameters\n",
    "checkpoint_dir = 'checkpoints_separate_new2'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    # Create subdirectories for each expert\n",
    "    os.makedirs(os.path.join(checkpoint_dir, 'mif'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(checkpoint_dir, 'he'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(checkpoint_dir, 'gating'), exist_ok=True)\n",
    "\n",
    "num_epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Train MIF Expert using existing loaders\n",
    "print(\"\\n=== Training MIF Expert ===\")\n",
    "mif_model = MIFExpert(pretrained=True).to(device)\n",
    "mif_optimizer = torch.optim.Adam(mif_model.parameters(), lr=1e-4)\n",
    "mif_scheduler = CosineAnnealingWarmRestarts(mif_optimizer, T_0=5, T_mult=2)\n",
    "mif_loss = CombinedLoss()\n",
    "\n",
    "mif_history = train_model(\n",
    "    model=mif_model,\n",
    "    optimizer=mif_optimizer,\n",
    "    loss_fn=mif_loss,\n",
    "    train_loader=loaders['mif']['train'],\n",
    "    val_loader=loaders['mif']['val'],\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    expert_type='mif',\n",
    "    checkpoint_dir=os.path.join(checkpoint_dir, 'mif')\n",
    ")\n",
    "\n",
    "# Train HE Expert with safe checkpoint loading\n",
    "print(\"\\n=== Training HE Expert ===\")\n",
    "he_model = HEExpert().to(device)\n",
    "he_optimizer = torch.optim.Adam(he_model.parameters(), lr=1e-4)\n",
    "he_scheduler = CosineAnnealingWarmRestarts(he_optimizer, T_0=5, T_mult=2)\n",
    "he_loss = CombinedLoss()\n",
    "\n",
    "# Modify the checkpoint loading logic\n",
    "he_checkpoint_path = os.path.join(checkpoint_dir, 'he', 'best_model.pt')\n",
    "try:\n",
    "    if os.path.exists(he_checkpoint_path):\n",
    "        checkpoint = torch.load(he_checkpoint_path)\n",
    "        he_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        he_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Successfully loaded HE Expert checkpoint\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load HE checkpoint: {e}\")\n",
    "    print(\"Training HE Expert from scratch\")\n",
    "\n",
    "he_history = train_model(\n",
    "    model=he_model,\n",
    "    optimizer=he_optimizer,\n",
    "    loss_fn=he_loss,\n",
    "    train_loader=loaders['he']['train'],\n",
    "    val_loader=loaders['he']['val'],\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    expert_type='he',\n",
    "    checkpoint_dir=os.path.join(checkpoint_dir, 'he')\n",
    ")\n",
    "\n",
    "# Combine experts into final model\n",
    "print(\"\\n=== Creating Combined Model ===\")\n",
    "combined_model = MultiModalExpertModel().to(device)\n",
    "\n",
    "# Load trained expert weights\n",
    "combined_model.mif_expert.load_state_dict(mif_model.state_dict())\n",
    "combined_model.he_expert.load_state_dict(he_model.state_dict())\n",
    "\n",
    "# Train gating network\n",
    "print(\"\\n=== Training Gating Network ===\")\n",
    "# Freeze expert weights\n",
    "for param in combined_model.mif_expert.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in combined_model.he_expert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Train only gating network\n",
    "gating_optimizer = torch.optim.Adam(combined_model.gating_network.parameters(), lr=1e-4)\n",
    "\n",
    "gating_history = train_model(\n",
    "    model=combined_model,\n",
    "    optimizer=gating_optimizer,\n",
    "    loss_fn=CombinedLoss(gate_supervision_weight=1.0),\n",
    "    train_loader=loaders['combined']['train'],\n",
    "    val_loader=loaders['combined']['val'],  # Now using the validation loader\n",
    "    num_epochs=5,\n",
    "    device=device,\n",
    "    expert_type='gating',\n",
    "    checkpoint_dir=checkpoint_dir\n",
    ")\n",
    "\n",
    "# Save final combined model\n",
    "torch.save({\n",
    "    'model_state_dict': combined_model.state_dict(),\n",
    "    'mif_history': mif_history,\n",
    "    'he_history': he_history,\n",
    "    'gating_history': gating_history\n",
    "}, os.path.join(checkpoint_dir, 'final_combined_model.pt'))\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b644a63a-f3cf-4508-a906-abec55ca4626",
   "metadata": {},
   "source": [
    "## Gating check H&E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d2e01-bc0c-4449-a414-0ced1aa2645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import find_boundaries\n",
    "import matplotlib.colors as mcolors\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "import random\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def visualize_gating_decisions(combined_model, sample, device='cuda'):\n",
    "    \"\"\"Visualize how gating network routes information with post-processing\"\"\"\n",
    "    combined_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the input image and ground truth from sample\n",
    "        image = sample['image'].unsqueeze(0).to(device) if len(sample['image'].shape) == 3 else sample['image'].to(device)\n",
    "        \n",
    "        # Extract ground truth data following the reference implementation\n",
    "        gt_mask = sample['targets']['masks']['cell'] if 'targets' in sample and 'masks' in sample['targets'] and 'cell' in sample['targets']['masks'] else None\n",
    "        gt_types = sample['targets']['semantic']['cell_types'] if 'targets' in sample and 'semantic' in sample['targets'] and 'cell_types' in sample['targets']['semantic'] else None\n",
    "        \n",
    "        print(\"\\nInput Processing Info:\")\n",
    "        print(f\"Original image shape: {image.shape}\")\n",
    "        print(f\"Image value range: [{image.min():.3f}, {image.max():.3f}]\")\n",
    "        print(f\"Image dtype: {image.dtype}\")\n",
    "        \n",
    "        # For MIF expert, use only first 2 channels if it's an HE image\n",
    "        mif_input = image[:, :2] if image.shape[1] == 3 else image\n",
    "        # For HE expert, pad with zeros if it's a MIF image\n",
    "        he_input = image if image.shape[1] == 3 else torch.cat([image, torch.zeros_like(image[:, :1])], dim=1)\n",
    "        \n",
    "        # Get predictions\n",
    "        mif_output = combined_model(mif_input, modality='mif')\n",
    "        he_output = combined_model(he_input, modality='he')\n",
    "        combined_output = combined_model(image)\n",
    "        \n",
    "        # Extract gating weights\n",
    "        gate_weights = combined_output['gate_weights'][0].cpu().numpy()\n",
    "        \n",
    "        # Initialize post-processor\n",
    "        cell_post_processor = DetectionCellPostProcessor(\n",
    "            nr_types=6,\n",
    "            magnification=40\n",
    "        )\n",
    "        \n",
    "        # Process predictions\n",
    "        if 'cell_hv' in he_output:\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(he_output['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(he_output['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                he_output['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "            ], axis=-1)\n",
    "        else:\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(he_output['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(he_output['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                np.zeros((he_output['cell_types'][0].shape[1], he_output['cell_types'][0].shape[2], 2))\n",
    "            ], axis=-1)\n",
    "            \n",
    "        # Post-process predictions\n",
    "        instance_map, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 7, figsize=(35, 5))\n",
    "        \n",
    "        # Define class names and colors\n",
    "        class_names = {\n",
    "            0: \"Background\", 1: \"Neoplastic\", 2: \"Inflammatory\",\n",
    "            3: \"Connective\", 4: \"Dead\", 5: \"Epithelial\"\n",
    "        }\n",
    "        type_colors = {\n",
    "            0: 'black', 1: '#00FF00', 2: 'blue',\n",
    "            3: 'yellow', 4: 'magenta', 5: 'cyan'\n",
    "        }\n",
    "        \n",
    "        # Original image\n",
    "        img_display = image[0].cpu().numpy()\n",
    "        img_display = np.moveaxis(img_display, 0, -1)\n",
    "        if img_display.shape[-1] == 2:\n",
    "            img_display = np.concatenate([img_display, np.zeros_like(img_display[:,:,0:1])], axis=-1)\n",
    "        img_display = np.clip((img_display - img_display.min()) / \n",
    "                            (img_display.max() - img_display.min() + 1e-8), 0, 1)\n",
    "        axes[0].imshow(img_display)\n",
    "        axes[0].set_title('Input Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Binary Ground Truth\n",
    "        if 'targets' in sample and 'masks' in sample['targets'] and 'cell' in sample['targets']['masks']:\n",
    "            binary_gt = sample['targets']['masks']['cell'].squeeze().cpu().numpy()\n",
    "            if len(binary_gt.shape) == 1:  # If it's 1D, reshape it\n",
    "                binary_gt = binary_gt.reshape(image.shape[-2:])\n",
    "            axes[1].imshow(binary_gt > 0, cmap='binary')\n",
    "            axes[1].set_title('Binary GT')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No Ground Truth Available', ha='center', va='center')\n",
    "            axes[1].set_title('Binary GT')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Binary Prediction\n",
    "        binary_pred = torch.softmax(he_output['cell_mask'][0], dim=0)[1].cpu().numpy()\n",
    "        axes[2].imshow(binary_pred > 0.5, cmap='binary')\n",
    "        axes[2].set_title('Binary Prediction')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Gating weights visualization\n",
    "        bars = axes[3].bar(['MIF Expert', 'HE Expert'], gate_weights)\n",
    "        axes[3].set_title('Gating Network Weights')\n",
    "        axes[3].set_ylim(0, 1)\n",
    "        for idx, rect in enumerate(bars):\n",
    "            height = rect.get_height()\n",
    "            axes[3].text(rect.get_x() + rect.get_width()/2., height,\n",
    "                         f'{gate_weights[idx]:.3f}',\n",
    "                         ha='center', va='bottom')\n",
    "        \n",
    "        # Instance Segmentation\n",
    "        axes[4].imshow(instance_map, cmap='nipy_spectral')\n",
    "        axes[4].set_title('Instance Segmentation')\n",
    "        axes[4].axis('off')\n",
    "        \n",
    "        # Ground Truth Cell Types\n",
    "        if gt_types is not None and gt_mask is not None:\n",
    "            gt_mask_np = gt_mask.squeeze().cpu().numpy()\n",
    "            if len(gt_mask_np.shape) == 1:\n",
    "                gt_mask_np = gt_mask_np.reshape(image.shape[-2:])\n",
    "            gt_types_np = gt_types.squeeze().cpu().numpy()\n",
    "            if len(gt_types_np.shape) == 1:\n",
    "                gt_types_np = gt_types_np.reshape(image.shape[-2:])\n",
    "                \n",
    "            gt_type_map = np.zeros_like(gt_mask_np)\n",
    "            for i in range(1, int(gt_mask_np.max()) + 1):\n",
    "                mask = gt_mask_np == i\n",
    "                if mask.any():\n",
    "                    masked_types = gt_types_np[mask]\n",
    "                    if len(masked_types) > 0:\n",
    "                        cell_type = np.argmax(np.bincount(masked_types.astype(int)))\n",
    "                        gt_type_map[mask] = cell_type\n",
    "            \n",
    "            gt_type_rgb = np.zeros((*gt_type_map.shape, 3))\n",
    "            for type_id in range(len(class_names)):\n",
    "                color = np.array(mcolors.to_rgb(type_colors[type_id]))\n",
    "                gt_type_rgb[gt_type_map == type_id] = color\n",
    "                \n",
    "            axes[5].imshow(gt_type_rgb)\n",
    "            axes[5].set_title('Ground Truth Cell Types')\n",
    "        else:\n",
    "            axes[5].text(0.5, 0.5, 'No Ground Truth Types Available', ha='center', va='center')\n",
    "            axes[5].set_title('Ground Truth Cell Types')\n",
    "        axes[5].axis('off')\n",
    "        \n",
    "        # Predicted Cell Types\n",
    "        pred_type_overlay = img_display.copy()\n",
    "        type_counts = {i: 0 for i in range(6)}\n",
    "        \n",
    "        for cell_id, cell_info in type_pred.items():\n",
    "            cell_mask = instance_map == cell_id\n",
    "            cell_type = cell_info['type']\n",
    "            \n",
    "            # Filter out cells based on size\n",
    "            cell_size = np.sum(cell_mask)\n",
    "            if not (10 <= cell_size <= 10000):\n",
    "                instance_map[cell_mask] = 0\n",
    "                continue\n",
    "                \n",
    "            pred_type_overlay[cell_mask] = np.array(mcolors.to_rgb(type_colors[cell_type]))\n",
    "            type_counts[cell_type] += 1\n",
    "            \n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        pred_type_overlay[boundaries] = [1, 1, 1]\n",
    "        axes[6].imshow(pred_type_overlay)\n",
    "        axes[6].set_title('Predicted Cell Types')\n",
    "        axes[6].axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, edgecolor='none', \n",
    "                         label=f\"{class_names[i]}\") for i, color in type_colors.items()]\n",
    "        fig.legend(handles=legend_elements, loc='lower center', \n",
    "                  ncol=len(class_names), bbox_to_anchor=(0.5, 0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nCell Detection Statistics:\")\n",
    "        total_pred_cells = sum(type_counts.values())\n",
    "        print(f\"Number of detected cells: {total_pred_cells}\")\n",
    "        \n",
    "        print(\"\\nCell type distribution:\")\n",
    "        for type_id, count in type_counts.items():\n",
    "            percentage = (count / total_pred_cells * 100) if total_pred_cells > 0 else 0\n",
    "            print(f\"{class_names[type_id]}: {count} cells ({percentage:.1f}%)\")\n",
    "        \n",
    "\n",
    "# Initialize and load the combined model\n",
    "combined_model = MultiModalExpertModel().to(device)\n",
    "\n",
    "# Load checkpoint from the correct path\n",
    "checkpoint_dir = 'checkpoints_separate_new2'\n",
    "combined_checkpoint_path = os.path.join(checkpoint_dir, 'final_combined_model.pt')\n",
    "\n",
    "print(f\"\\nAttempting to load checkpoint from: {os.path.abspath(combined_checkpoint_path)}\")\n",
    "\n",
    "if os.path.exists(combined_checkpoint_path):\n",
    "    checkpoint = torch.load(combined_checkpoint_path)\n",
    "    combined_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    combined_model.eval()\n",
    "    print(\"Combined model loaded successfully!\")\n",
    "    \n",
    "    # Get a random test sample\n",
    "    test_dataset = loaders['he']['test'].dataset\n",
    "    random_idx = random.randint(0, len(test_dataset) - 1)\n",
    "    test_sample = test_dataset[random_idx]\n",
    "    print(f\"\\nVisualizing gating decisions for random HE test sample (index: {random_idx})...\")\n",
    "    visualize_gating_decisions(combined_model, test_sample, device)\n",
    "    \n",
    "else:\n",
    "    print(f\"Error: Checkpoint not found at {combined_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b83a06-a237-4cca-bc74-55d7e5552cb1",
   "metadata": {},
   "source": [
    "## Gating vs HE exoerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc4ce7-10af-4b6b-b20d-cb14ce7c58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "\n",
    "def compare_gating_vs_direct(combined_model, he_model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compare predictions between gated model and direct HE expert\n",
    "    \"\"\"\n",
    "    # Get a random sample\n",
    "    random_idx = np.random.randint(0, len(test_loader.dataset))\n",
    "    print(f\"Analyzing random sample {random_idx} out of {len(test_loader.dataset)} samples\")\n",
    "    \n",
    "    # Get the random sample\n",
    "    random_sample = test_loader.dataset[random_idx]\n",
    "    batch = test_loader.collate_fn([random_sample])\n",
    "    \n",
    "    # Define class names and colors\n",
    "    class_names = {\n",
    "        0: \"Background\", 1: \"Neoplastic\", 2: \"Inflammatory\",\n",
    "        3: \"Connective\", 4: \"Dead\", 5: \"Epithelial\"\n",
    "    }\n",
    "    type_colors = {\n",
    "        0: 'black', 1: '#00FF00', 2: 'blue',\n",
    "        3: 'yellow', 4: 'magenta', 5: 'cyan'\n",
    "    }\n",
    "    \n",
    "    # Initialize post-processor\n",
    "    cell_post_processor = DetectionCellPostProcessor(nr_types=6, magnification=40)\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    combined_model.eval()\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = batch['image'].to(device)\n",
    "        \n",
    "        # Get predictions from both models\n",
    "        gated_output = combined_model(images)\n",
    "        direct_output = he_model(images)\n",
    "        \n",
    "        # Debug output keys\n",
    "        print(\"\\nGated model output keys:\")\n",
    "        for key in gated_output.keys():\n",
    "            print(f\"- {key}\")\n",
    "            if isinstance(gated_output[key], torch.Tensor):\n",
    "                print(f\"  Shape: {gated_output[key].shape}\")\n",
    "                \n",
    "        print(\"\\nDirect HE model output keys:\")\n",
    "        for key in direct_output.keys():\n",
    "            print(f\"- {key}\")\n",
    "            if isinstance(direct_output[key], torch.Tensor):\n",
    "                print(f\"  Shape: {direct_output[key].shape}\")\n",
    "        \n",
    "        # Map gated output keys to standard names\n",
    "        gated_segmentation = gated_output['segmentation']  # cell mask\n",
    "        gated_hv = gated_output['hv_maps']  # horizontal/vertical maps\n",
    "        gated_types = gated_output['cell_types']\n",
    "        \n",
    "        # Process gated predictions\n",
    "        gated_pred_map = np.concatenate([\n",
    "            torch.argmax(gated_types[0], dim=0).cpu().numpy()[..., None],\n",
    "            torch.argmax(gated_segmentation[0], dim=0).cpu().numpy()[..., None],\n",
    "            gated_hv[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Process direct predictions\n",
    "        direct_pred_map = np.concatenate([\n",
    "            torch.argmax(direct_output['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "            torch.argmax(direct_output['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            direct_output['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Post-process predictions\n",
    "        gated_instance_map, gated_type_pred = cell_post_processor.post_process_cell_segmentation(gated_pred_map)\n",
    "        direct_instance_map, direct_type_pred = cell_post_processor.post_process_cell_segmentation(direct_pred_map)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "        fig.suptitle('Comparison: Gated Model vs Direct HE Expert', fontsize=16)\n",
    "        \n",
    "        # Original image\n",
    "        img_display = images[0].cpu().numpy()\n",
    "        img_display = np.moveaxis(img_display, 0, -1)\n",
    "        img_display = np.clip((img_display - img_display.min()) / \n",
    "                            (img_display.max() - img_display.min() + 1e-8), 0, 1)\n",
    "        \n",
    "        # Row 1: Gated Model Results\n",
    "        axes[0,0].imshow(img_display)\n",
    "        axes[0,0].set_title('Original Image')\n",
    "        axes[0,0].axis('off')\n",
    "        \n",
    "        gated_binary = torch.softmax(gated_segmentation[0], dim=0)[1].cpu().numpy()\n",
    "        axes[0,1].imshow(gated_binary > 0.5, cmap='binary')\n",
    "        axes[0,1].set_title('Gated Binary Pred')\n",
    "        axes[0,1].axis('off')\n",
    "        \n",
    "        axes[0,2].imshow(gated_instance_map, cmap='nipy_spectral')\n",
    "        axes[0,2].set_title('Gated Instance Seg')\n",
    "        axes[0,2].axis('off')\n",
    "        \n",
    "        # Gated cell type overlay\n",
    "        gated_boundaries = find_boundaries(gated_instance_map, mode='thick')\n",
    "        gated_type_overlay = img_display.copy()\n",
    "        for cell_id, cell_info in gated_type_pred.items():\n",
    "            cell_mask = gated_instance_map == cell_id\n",
    "            cell_type = cell_info['type']\n",
    "            gated_type_overlay[cell_mask] = np.array(mcolors.to_rgb(type_colors[cell_type]))\n",
    "        gated_type_overlay[gated_boundaries] = [1, 1, 1]\n",
    "        axes[0,3].imshow(gated_type_overlay)\n",
    "        axes[0,3].set_title('Gated Cell Types')\n",
    "        axes[0,3].axis('off')\n",
    "        \n",
    "        # Row 2: Direct HE Expert Results\n",
    "        axes[1,0].imshow(img_display)\n",
    "        axes[1,0].set_title('Original Image')\n",
    "        axes[1,0].axis('off')\n",
    "        \n",
    "        direct_binary = torch.softmax(direct_output['cell_mask'][0], dim=0)[1].cpu().numpy()\n",
    "        axes[1,1].imshow(direct_binary > 0.5, cmap='binary')\n",
    "        axes[1,1].set_title('Direct Binary Pred')\n",
    "        axes[1,1].axis('off')\n",
    "        \n",
    "        axes[1,2].imshow(direct_instance_map, cmap='nipy_spectral')\n",
    "        axes[1,2].set_title('Direct Instance Seg')\n",
    "        axes[1,2].axis('off')\n",
    "        \n",
    "        # Direct cell type overlay\n",
    "        direct_boundaries = find_boundaries(direct_instance_map, mode='thick')\n",
    "        direct_type_overlay = img_display.copy()\n",
    "        for cell_id, cell_info in direct_type_pred.items():\n",
    "            cell_mask = direct_instance_map == cell_id\n",
    "            cell_type = cell_info['type']\n",
    "            direct_type_overlay[cell_mask] = np.array(mcolors.to_rgb(type_colors[cell_type]))\n",
    "        direct_type_overlay[direct_boundaries] = [1, 1, 1]\n",
    "        axes[1,3].imshow(direct_type_overlay)\n",
    "        axes[1,3].set_title('Direct Cell Types')\n",
    "        axes[1,3].axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, edgecolor='none', \n",
    "                         label=class_names[i]) for i, color in type_colors.items()]\n",
    "        fig.legend(handles=legend_elements, loc='center right', \n",
    "                  bbox_to_anchor=(0.98, 0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nGated Model Statistics:\")\n",
    "        print(f\"Number of detected cells: {len(gated_type_pred)}\")\n",
    "        gated_type_counts = {}\n",
    "        for cell_id, cell_info in gated_type_pred.items():\n",
    "            cell_type = cell_info['type']\n",
    "            gated_type_counts[cell_type] = gated_type_counts.get(cell_type, 0) + 1\n",
    "            \n",
    "        print(\"\\nGated model cell type distribution:\")\n",
    "        for type_id in class_names.keys():\n",
    "            count = gated_type_counts.get(type_id, 0)\n",
    "            percentage = (count / len(gated_type_pred) * 100) if len(gated_type_pred) > 0 else 0\n",
    "            print(f\"{class_names[type_id]}: {count} cells ({percentage:.1f}%)\")\n",
    "            \n",
    "        print(\"\\nDirect HE Expert Statistics:\")\n",
    "        print(f\"Number of detected cells: {len(direct_type_pred)}\")\n",
    "        direct_type_counts = {}\n",
    "        for cell_id, cell_info in direct_type_pred.items():\n",
    "            cell_type = cell_info['type']\n",
    "            direct_type_counts[cell_type] = direct_type_counts.get(cell_type, 0) + 1\n",
    "            \n",
    "        print(\"\\nDirect HE expert cell type distribution:\")\n",
    "        for type_id in class_names.keys():\n",
    "            count = direct_type_counts.get(type_id, 0)\n",
    "            percentage = (count / len(direct_type_pred) * 100) if len(direct_type_pred) > 0 else 0\n",
    "            print(f\"{class_names[type_id]}: {count} cells ({percentage:.1f}%)\")\n",
    "            \n",
    "        # Compare raw predictions before post-processing\n",
    "        print(\"\\nComparing raw predictions:\")\n",
    "        \n",
    "        # Compare segmentation masks\n",
    "        gated_seg_probs = torch.softmax(gated_segmentation[0], dim=0)[1]\n",
    "        direct_seg_probs = torch.softmax(direct_output['cell_mask'][0], dim=0)[1]\n",
    "        seg_diff = (gated_seg_probs - direct_seg_probs).abs().mean().item()\n",
    "        print(f\"Average segmentation difference: {seg_diff:.4f}\")\n",
    "        \n",
    "        # Compare cell type predictions\n",
    "        gated_type_probs = torch.softmax(gated_types[0], dim=0)\n",
    "        direct_type_probs = torch.softmax(direct_output['cell_types'][0], dim=0)\n",
    "        type_diff = (gated_type_probs - direct_type_probs).abs().mean().item()\n",
    "        print(f\"Average cell type difference: {type_diff:.4f}\")\n",
    "        \n",
    "        # Compare HV maps\n",
    "        hv_diff = (gated_hv[0] - direct_output['cell_hv'][0]).abs().mean().item()\n",
    "        print(f\"Average HV map difference: {hv_diff:.4f}\")\n",
    "        \n",
    "        # Get max differences for each component\n",
    "        seg_max_diff = (gated_seg_probs - direct_seg_probs).abs().max().item()\n",
    "        type_max_diff = (gated_type_probs - direct_type_probs).abs().max().item()\n",
    "        hv_max_diff = (gated_hv[0] - direct_output['cell_hv'][0]).abs().max().item()\n",
    "        \n",
    "        print(f\"\\nMaximum differences:\")\n",
    "        print(f\"Max segmentation difference: {seg_max_diff:.4f}\")\n",
    "        print(f\"Max cell type difference: {type_max_diff:.4f}\")\n",
    "        print(f\"Max HV map difference: {hv_max_diff:.4f}\")\n",
    "        \n",
    "        # Add plots to compare probability distributions\n",
    "        fig_probs, axes_probs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig_probs.suptitle('Probability Distribution Comparison')\n",
    "        \n",
    "        # Segmentation probabilities\n",
    "        axes_probs[0].hist(gated_seg_probs.cpu().numpy().flatten(), bins=50, alpha=0.5, label='Gated')\n",
    "        axes_probs[0].hist(direct_seg_probs.cpu().numpy().flatten(), bins=50, alpha=0.5, label='Direct')\n",
    "        axes_probs[0].set_title('Segmentation Probabilities')\n",
    "        axes_probs[0].legend()\n",
    "        \n",
    "        # Cell type probabilities\n",
    "        axes_probs[1].hist(gated_type_probs.cpu().numpy().flatten(), bins=50, alpha=0.5, label='Gated')\n",
    "        axes_probs[1].hist(direct_type_probs.cpu().numpy().flatten(), bins=50, alpha=0.5, label='Direct')\n",
    "        axes_probs[1].set_title('Cell Type Probabilities')\n",
    "        axes_probs[1].legend()\n",
    "        \n",
    "        # HV map values\n",
    "        axes_probs[2].hist(gated_hv[0].cpu().numpy().flatten(), bins=50, alpha=0.5, label='Gated')\n",
    "        axes_probs[2].hist(direct_output['cell_hv'][0].cpu().numpy().flatten(), bins=50, alpha=0.5, label='Direct')\n",
    "        axes_probs[2].set_title('HV Map Values')\n",
    "        axes_probs[2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage:\n",
    "compare_gating_vs_direct(combined_model, he_model, loaders['he']['test'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc882f5-1d33-45c0-9cca-e9ff1f1a11ac",
   "metadata": {},
   "source": [
    "## Gating Check mIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d4f61-cf4f-4a7c-9217-829773ed5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gating_decisions_mif(combined_model, sample, device='cuda'):\n",
    "    \"\"\"Visualize how gating network routes information with post-processing for MIF images\"\"\"\n",
    "    combined_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the input image\n",
    "        image = sample['image'].unsqueeze(0).to(device) if len(sample['image'].shape) == 3 else sample['image'].to(device)\n",
    "        \n",
    "        print(\"\\nInput Processing Info:\")\n",
    "        print(f\"Original image shape: {image.shape}\")\n",
    "        print(f\"Image value range: [{image.min():.3f}, {image.max():.3f}]\")\n",
    "        print(f\"Image dtype: {image.dtype}\")\n",
    "        \n",
    "        # For MIF expert, use the first 2 channels\n",
    "        mif_input = image[:, :2] if image.shape[1] > 2 else image\n",
    "        # For HE expert, pad with zeros if it's a MIF image\n",
    "        he_input = torch.cat([image, torch.zeros_like(image[:, :1])], dim=1) if image.shape[1] == 2 else image\n",
    "        \n",
    "        # Get predictions with each expert separately and combined\n",
    "        mif_output = combined_model(mif_input, modality='mif')\n",
    "        he_output = combined_model(he_input, modality='he')\n",
    "        combined_output = combined_model(image)\n",
    "        \n",
    "        # Extract gating weights\n",
    "        gate_weights = combined_output['gate_weights'][0].cpu().numpy()\n",
    "        \n",
    "        # Initialize post-processors for nuclei and membrane\n",
    "        nuclei_processor = DetectionCellPostProcessor(nr_types=1, magnification=40, gt=False)\n",
    "        membrane_processor = DetectionCellPostProcessor(nr_types=1, magnification=40, gt=False)\n",
    "        \n",
    "        # Process nuclei predictions\n",
    "        nuclei_seg = torch.sigmoid(mif_output['nuclei_mask'][0, 1]).cpu().numpy() > 0.1\n",
    "        nuclei_hv = mif_output['nuclei_hv'][0].cpu().numpy()\n",
    "        nuclei_map = np.concatenate([\n",
    "            np.zeros((256, 256, 1)),\n",
    "            nuclei_seg[..., None],\n",
    "            nuclei_hv.transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Process membrane predictions\n",
    "        membrane_seg = torch.sigmoid(mif_output['membrane_mask'][0, 1]).cpu().numpy() > 0.3\n",
    "        membrane_hv = mif_output['membrane_hv'][0].cpu().numpy()\n",
    "        membrane_map = np.concatenate([\n",
    "            np.zeros((256, 256, 1)),\n",
    "            membrane_seg[..., None],\n",
    "            membrane_hv.transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Post-process predictions\n",
    "        nuclei_instance_map, nuclei_info = nuclei_processor.post_process_cell_segmentation(nuclei_map)\n",
    "        membrane_instance_map, membrane_info = membrane_processor.post_process_cell_segmentation(membrane_map)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 7, figsize=(35, 5))\n",
    "        \n",
    "        # Original image display\n",
    "        img_display = image[0].cpu().numpy()\n",
    "        img_display = np.moveaxis(img_display, 0, -1)\n",
    "        rgb_display = np.stack([\n",
    "            img_display[..., 0],  # Nuclei channel\n",
    "            img_display[..., 1],  # Membrane channel\n",
    "            img_display[..., 0]   # Nuclei channel again for blue\n",
    "        ], axis=-1)\n",
    "        rgb_display = np.clip((rgb_display - rgb_display.min()) / \n",
    "                            (rgb_display.max() - rgb_display.min() + 1e-8), 0, 1)\n",
    "        \n",
    "        axes[0].imshow(rgb_display)\n",
    "        axes[0].set_title('Input Image\\n(Nuclei=Red+Blue, Membrane=Green)')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Ground Truth Nuclei\n",
    "        if 'targets' in sample and 'masks' in sample['targets'] and 'nuclei' in sample['targets']['masks']:\n",
    "            gt_nuclei = sample['targets']['masks']['nuclei'].squeeze().cpu().numpy()\n",
    "            if len(gt_nuclei.shape) == 1:\n",
    "                gt_nuclei = gt_nuclei.reshape(image.shape[-2:])\n",
    "            axes[1].imshow(gt_nuclei, cmap='nipy_spectral')\n",
    "            axes[1].set_title('Nuclei GT')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No Nuclei GT Available', ha='center', va='center')\n",
    "            axes[1].set_title('Nuclei GT')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Ground Truth Membrane\n",
    "        if 'targets' in sample and 'masks' in sample['targets'] and 'membrane' in sample['targets']['masks']:\n",
    "            gt_membrane = sample['targets']['masks']['membrane'].squeeze().cpu().numpy()\n",
    "            if len(gt_membrane.shape) == 1:\n",
    "                gt_membrane = gt_membrane.reshape(image.shape[-2:])\n",
    "            axes[2].imshow(gt_membrane, cmap='nipy_spectral')\n",
    "            axes[2].set_title('Membrane GT')\n",
    "        else:\n",
    "            axes[2].text(0.5, 0.5, 'No Membrane GT Available', ha='center', va='center')\n",
    "            axes[2].set_title('Membrane GT')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Gating weights visualization\n",
    "        bars = axes[3].bar(['MIF Expert', 'HE Expert'], gate_weights)\n",
    "        axes[3].set_title('Gating Network Weights')\n",
    "        axes[3].set_ylim(0, 1)\n",
    "        for idx, rect in enumerate(bars):\n",
    "            height = rect.get_height()\n",
    "            axes[3].text(rect.get_x() + rect.get_width()/2., height,\n",
    "                         f'{gate_weights[idx]:.3f}',\n",
    "                         ha='center', va='bottom')\n",
    "        \n",
    "        # Predicted Instance Maps\n",
    "        axes[4].imshow(nuclei_instance_map, cmap='nipy_spectral')\n",
    "        axes[4].set_title(f'Nuclei Instances\\n(n={len(nuclei_info)})')\n",
    "        axes[4].axis('off')\n",
    "        \n",
    "        axes[5].imshow(membrane_instance_map, cmap='nipy_spectral')\n",
    "        axes[5].set_title(f'Membrane Instances\\n(n={len(membrane_info)})')\n",
    "        axes[5].axis('off')\n",
    "        \n",
    "        # Combined Overlay\n",
    "        combined_overlay = rgb_display.copy()\n",
    "        nuclei_boundaries = find_boundaries(nuclei_instance_map, mode='thick')\n",
    "        membrane_boundaries = find_boundaries(membrane_instance_map, mode='thick')\n",
    "        combined_overlay[nuclei_boundaries] = [1, 0, 0]    # Red for nuclei\n",
    "        combined_overlay[membrane_boundaries] = [0, 1, 0]  # Green for membrane\n",
    "        axes[6].imshow(combined_overlay)\n",
    "        axes[6].set_title('Combined Boundaries\\nRed=Nuclei, Green=Membrane')\n",
    "        axes[6].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nPrediction Statistics:\")\n",
    "        print(f\"Number of detected nuclei: {len(nuclei_info)}\")\n",
    "        print(f\"Number of detected membranes: {len(membrane_info)}\")\n",
    "        print(f\"\\nGating Network Decision:\")\n",
    "        print(f\"MIF Expert Weight: {gate_weights[0]:.3f}\")\n",
    "        print(f\"HE Expert Weight: {gate_weights[1]:.3f}\")\n",
    "        \n",
    "        return nuclei_instance_map, membrane_instance_map\n",
    "\n",
    "# Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize and load the combined model\n",
    "    combined_model = MultiModalExpertModel().to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_dir = 'checkpoints_separate_new2'\n",
    "    combined_checkpoint_path = os.path.join(checkpoint_dir, 'final_combined_model.pt')\n",
    "    \n",
    "    print(f\"\\nAttempting to load checkpoint from: {os.path.abspath(combined_checkpoint_path)}\")\n",
    "    \n",
    "    if os.path.exists(combined_checkpoint_path):\n",
    "        checkpoint = torch.load(combined_checkpoint_path)\n",
    "        combined_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        combined_model.eval()\n",
    "        print(\"Combined model loaded successfully!\")\n",
    "        \n",
    "        # Get a random test sample\n",
    "        test_dataset = loaders['mif']['test'].dataset\n",
    "        random_idx = random.randint(0, len(test_dataset) - 1)\n",
    "        test_sample = test_dataset[random_idx]\n",
    "        print(f\"\\nVisualizing gating decisions for random MIF test sample (index: {random_idx})...\")\n",
    "        nuclei_map, membrane_map = visualize_gating_decisions_mif(combined_model, test_sample, device)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Checkpoint not found at {combined_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832ee1e-c985-4b39-95a7-e0da8e655692",
   "metadata": {},
   "source": [
    "## Load HE EXPERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195fc61-3698-49c9-aef0-e3c2832f6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.models.swin_transformer import Swin_V2_B_Weights\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the HE model\n",
    "he_model = HEExpert(pretrained=True, num_cell_classes=6).to(device)\n",
    "\n",
    "# Debug checkpoint paths\n",
    "checkpoint_dir = os.path.join('checkpoints_separate_new2', 'he')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Looking for checkpoint in: {os.path.abspath(checkpoint_dir)}\")\n",
    "\n",
    "# List available files in the checkpoint directory\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"Files in checkpoint directory:\")\n",
    "    for file in os.listdir(checkpoint_dir):\n",
    "        print(f\"- {file}\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory '{checkpoint_dir}' does not exist!\")\n",
    "\n",
    "# First checkpoint file in the directory\n",
    "if os.path.exists(checkpoint_dir) and os.listdir(checkpoint_dir):\n",
    "    checkpoint_file = os.listdir(checkpoint_dir)[0]  # Get the first checkpoint file\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "    print(f\"Found checkpoint at: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    he_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    he_model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No checkpoint files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fdf6e-d859-4b98-9ea7-60189e853848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from cell_segmentation.utils import post_proc_vitaminp\n",
    "importlib.reload(post_proc_vitaminp)\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "\n",
    "# Now create a new instance of the post-processor with the updated parameters\n",
    "cell_post_processor = DetectionCellPostProcessor(nr_types=6, magnification=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fbb77-5c42-444b-969a-f69c78c42869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import find_boundaries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "import os\n",
    "\n",
    "def visualize_predictions(he_model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Visualize predictions using already loaded model and dataloader\n",
    "    \"\"\"\n",
    "    # Get a random sample\n",
    "    random_idx = np.random.randint(0, len(test_loader.dataset))\n",
    "    print(f\"Analyzing random sample {random_idx} out of {len(test_loader.dataset)} samples\")\n",
    "\n",
    "    # Get the random sample\n",
    "    random_sample = test_loader.dataset[random_idx]\n",
    "    batch = test_loader.collate_fn([random_sample])\n",
    "\n",
    "    # Define class names and colors\n",
    "    class_names = {\n",
    "        0: \"Background\", 1: \"Neoplastic\", 2: \"Inflammatory\",\n",
    "        3: \"Connective\", 4: \"Dead\", 5: \"Epithelial\"\n",
    "    }\n",
    "    type_colors = {\n",
    "        0: 'black', 1: '#00FF00', 2: 'blue',\n",
    "        3: 'yellow', 4: 'magenta', 5: 'cyan'\n",
    "    }\n",
    "\n",
    "    # Get predictions\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = batch['image'].to(device)\n",
    "        outputs = he_model(images)\n",
    "        \n",
    "        print(\"\\nAvailable keys in model outputs:\")\n",
    "        for key in outputs.keys():\n",
    "            print(f\"- {key}\")\n",
    "            \n",
    "        # Initialize post-processor\n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=6, magnification=40)\n",
    "        \n",
    "        # Process predictions - check if 'cell_hv' exists\n",
    "        if 'cell_hv' in outputs:\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "            ], axis=-1)\n",
    "        else:\n",
    "            # Handle case without cell_hv\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                np.zeros((outputs['cell_types'][0].shape[1], outputs['cell_types'][0].shape[2], 2))\n",
    "            ], axis=-1)\n",
    "\n",
    "        instance_map, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 7, figsize=(35, 5))\n",
    "        \n",
    "        # Original image\n",
    "        img_display = images[0].cpu().numpy()\n",
    "        img_display = np.moveaxis(img_display, 0, -1)\n",
    "        img_display = np.clip((img_display - img_display.min()) / \n",
    "                            (img_display.max() - img_display.min() + 1e-8), 0, 1)\n",
    "        axes[0].imshow(img_display)\n",
    "        axes[0].set_title('Original H&E')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Binary Ground Truth\n",
    "        if 'masks' in batch['targets'] and 'cell' in batch['targets']['masks']:\n",
    "            binary_gt = batch['targets']['masks']['cell'][0].squeeze().cpu().numpy()\n",
    "            axes[1].imshow(binary_gt > 0, cmap='binary')\n",
    "            axes[1].set_title('Binary GT')\n",
    "        else:\n",
    "            axes[1].set_title('Binary GT (Not Available)')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Binary Prediction\n",
    "        binary_pred = torch.softmax(outputs['cell_mask'][0], dim=0)[1].cpu().numpy()\n",
    "        axes[2].imshow(binary_pred > 0.5, cmap='binary')\n",
    "        axes[2].set_title('Binary Prediction')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Instance Segmentation\n",
    "        axes[3].imshow(instance_map, cmap='nipy_spectral')\n",
    "        axes[3].set_title('Instance Segmentation')\n",
    "        axes[3].axis('off')\n",
    "        \n",
    "        # Boundary Overlay\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        overlay = img_display.copy()\n",
    "        overlay[boundaries] = [1, 1, 1]  # White boundaries\n",
    "        axes[4].imshow(overlay)\n",
    "        axes[4].set_title('Boundary Overlay')\n",
    "        axes[4].axis('off')\n",
    "        \n",
    "        # Ground Truth Cell Type Map\n",
    "        # Ground Truth Cell Type Map\n",
    "        if 'semantic' in batch['targets'] and 'cell_types' in batch['targets']['semantic']:\n",
    "            cell_types = batch['targets']['semantic']['cell_types'][0].cpu().numpy()\n",
    "            cell_mask = batch['targets']['masks']['cell'][0].squeeze().cpu().numpy()\n",
    "\n",
    "            # Create visualization - Fixed version\n",
    "            gt_type_map = np.zeros_like(cell_mask)\n",
    "            for i in range(1, cell_mask.max() + 1):  # For each instance\n",
    "                mask = cell_mask == i\n",
    "                if mask.any():\n",
    "                    # Get the type for this masked region\n",
    "                    masked_types = cell_types[mask]\n",
    "                    if len(masked_types) > 0:\n",
    "                        cell_type = np.argmax(np.bincount(masked_types))\n",
    "                        gt_type_map[mask] = cell_type\n",
    "\n",
    "            gt_type_rgb = np.zeros((*gt_type_map.shape, 3))\n",
    "            for type_id in range(len(class_names)):\n",
    "                color = np.array(mcolors.to_rgb(type_colors[type_id]))\n",
    "                gt_type_rgb[gt_type_map == type_id] = color\n",
    "\n",
    "            axes[5].imshow(gt_type_rgb)\n",
    "            axes[5].set_title('Ground Truth Cell Types')\n",
    "        else:\n",
    "            axes[5].set_title('Ground Truth Cell Types (Not Available)')\n",
    "        axes[5].axis('off')\n",
    "        \n",
    "        # Predicted Cell Type Overlay\n",
    "        pred_type_overlay = img_display.copy()\n",
    "        for cell_id, cell_info in type_pred.items():\n",
    "            cell_mask = instance_map == cell_id\n",
    "            cell_type = cell_info['type']\n",
    "            pred_type_overlay[cell_mask] = np.array(mcolors.to_rgb(type_colors[cell_type]))\n",
    "        pred_type_overlay[boundaries] = [1, 1, 1]\n",
    "        axes[6].imshow(pred_type_overlay)\n",
    "        axes[6].set_title('Predicted Cell Types')\n",
    "        axes[6].axis('off')\n",
    "        \n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, edgecolor='none', \n",
    "                         label=class_names[i]) for i, color in type_colors.items()]\n",
    "        fig.legend(handles=legend_elements, loc='lower center', \n",
    "                  ncol=len(class_names), bbox_to_anchor=(0.5, 0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)\n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"\\nPrediction Statistics:\")\n",
    "        print(f\"Number of detected cells: {len(type_pred)}\")\n",
    "        type_counts = {}\n",
    "        for cell_id, cell_info in type_pred.items():\n",
    "            cell_type = cell_info['type']\n",
    "            type_counts[cell_type] = type_counts.get(cell_type, 0) + 1\n",
    "        \n",
    "        print(\"\\nCell type distribution:\")\n",
    "        for type_id, count in type_counts.items():\n",
    "            print(f\"{class_names[type_id]}: {count} cells\")\n",
    "\n",
    "            # Add these print statements after loading images and before model prediction:\n",
    "        print(\"\\nInput Processing Info:\")\n",
    "        print(f\"Original image shape before processing: {images.shape}\")\n",
    "        print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "        print(f\"Image dtype: {images.dtype}\")\n",
    "\n",
    "        # After model prediction, add:\n",
    "        print(\"\\nModel Output Info:\")\n",
    "        print(f\"cell_mask shape: {outputs['cell_mask'].shape}\")\n",
    "        print(f\"cell_mask logits range: [{outputs['cell_mask'].min():.3f}, {outputs['cell_mask'].max():.3f}]\")\n",
    "\n",
    "        if 'cell_hv' in outputs:\n",
    "           print(f\"cell_hv shape: {outputs['cell_hv'].shape}\")\n",
    "           print(f\"cell_hv range: [{outputs['cell_hv'].min():.3f}, {outputs['cell_hv'].max():.3f}]\")\n",
    "\n",
    "        print(\"\\nPost-processing Info:\")\n",
    "        print(f\"pred_map shape: {pred_map.shape}\")\n",
    "        print(f\"instance_map shape: {instance_map.shape}\")\n",
    "        print(f\"instance_map unique values: {len(np.unique(instance_map))} instances\")\n",
    "# Use with existing data:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "visualize_predictions(he_model, loaders['he']['test'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bf0c8-67d1-4ffe-9993-f78070ee5123",
   "metadata": {},
   "source": [
    "## Paul Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68b7e1-1448-4191-8c10-ab956e7cbd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single image from path\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert to RGB if necessary\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Resize if needed\n",
    "    if image.size != target_size:\n",
    "        image = image.resize(target_size)\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    image_array = np.array(image) / 255.0\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    image_tensor = torch.from_numpy(image_array).float()\n",
    "    image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "def visualize_single_image(he_model, image_path, device='cuda', save_predictions=True):\n",
    "    \"\"\"\n",
    "    Visualize predictions for a single image from path and optionally save predictions\n",
    "    Args:\n",
    "        he_model: The model to use for predictions\n",
    "        image_path: Path to the input image\n",
    "        device: Device to run the model on\n",
    "        save_predictions: Whether to save the prediction overlay\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image_tensor = load_and_preprocess_image(image_path)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    print(f\"Input tensor shape: {image_tensor.shape}\")\n",
    "    \n",
    "    # Define class names and colors\n",
    "    class_names = {\n",
    "        0: \"Background\", 1: \"Neoplastic\", 2: \"Inflammatory\",\n",
    "        3: \"Connective\", 4: \"Dead\", 5: \"Epithelial\"\n",
    "    }\n",
    "    type_colors = {\n",
    "        0: 'black', 1: '#00FF00', 2: 'blue',\n",
    "        3: 'yellow', 4: 'magenta', 5: 'cyan'\n",
    "    }\n",
    "\n",
    "    # Get predictions\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(image_tensor)\n",
    "        \n",
    "        print(\"\\nAvailable keys in model outputs:\")\n",
    "        for key in outputs.keys():\n",
    "            print(f\"- {key}\")\n",
    "            \n",
    "        # Initialize post-processor\n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=6, magnification=40)\n",
    "        \n",
    "        # Process predictions\n",
    "        if 'cell_hv' in outputs:\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "            ], axis=-1)\n",
    "        else:\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                np.zeros((outputs['cell_types'][0].shape[1], outputs['cell_types'][0].shape[2], 2))\n",
    "            ], axis=-1)\n",
    "\n",
    "        instance_map, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        \n",
    "        # Original image\n",
    "        img_display = image_tensor[0].cpu().numpy()\n",
    "        img_display = np.moveaxis(img_display, 0, -1)\n",
    "        axes[0].imshow(img_display)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Instance Segmentation\n",
    "        axes[1].imshow(instance_map, cmap='nipy_spectral')\n",
    "        axes[1].set_title('Instance Segmentation')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Boundary Overlay\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        overlay = img_display.copy()\n",
    "        overlay[boundaries] = [1, 1, 1]  # White boundaries\n",
    "        axes[2].imshow(overlay)\n",
    "        axes[2].set_title('Boundary Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Predicted Cell Type Overlay\n",
    "        pred_type_overlay = img_display.copy()\n",
    "        for cell_id, cell_info in type_pred.items():\n",
    "            cell_mask = instance_map == cell_id\n",
    "            cell_type = cell_info['type']\n",
    "            pred_type_overlay[cell_mask] = np.array(mcolors.to_rgb(type_colors[cell_type]))\n",
    "        pred_type_overlay[boundaries] = [1, 1, 1]\n",
    "        axes[3].imshow(pred_type_overlay)\n",
    "        axes[3].set_title('Predicted Cell Types')\n",
    "        axes[3].axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, edgecolor='none', \n",
    "                         label=class_names[i]) for i, color in type_colors.items()]\n",
    "        fig.legend(handles=legend_elements, loc='lower center', \n",
    "                  ncol=len(class_names), bbox_to_anchor=(0.5, 0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)\n",
    "        if save_predictions:\n",
    "            # Create output filename\n",
    "            base_name = os.path.basename(image_path)\n",
    "            name_without_ext = os.path.splitext(base_name)[0]\n",
    "            output_path = os.path.join(os.path.dirname(image_path), f\"prediction_{name_without_ext}.png\")\n",
    "            \n",
    "            # Save the final overlay (predicted cell types with boundaries)\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(pred_type_overlay)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Add legend\n",
    "            legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, edgecolor='none', \n",
    "                             label=class_names[i]) for i, color in type_colors.items()]\n",
    "            plt.legend(handles=legend_elements, loc='center left', \n",
    "                      bbox_to_anchor=(1, 0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"\\nSaved prediction overlay to: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"\\nPrediction Statistics:\")\n",
    "        print(f\"Number of detected cells: {len(type_pred)}\")\n",
    "        type_counts = {}\n",
    "        for cell_id, cell_info in type_pred.items():\n",
    "            cell_type = cell_info['type']\n",
    "            type_counts[cell_type] = type_counts.get(cell_type, 0) + 1\n",
    "        \n",
    "        print(\"\\nCell type distribution:\")\n",
    "        for type_id, count in type_counts.items():\n",
    "            print(f\"{class_names[type_id]}: {count} cells\")\n",
    "\n",
    "# Usage example:\n",
    "def process_folder(model, folder_path, device='cuda', save_predictions=True):\n",
    "    \"\"\"\n",
    "    Process all images in a folder\n",
    "    Args:\n",
    "        model: The model to use for predictions\n",
    "        folder_path: Path to the folder containing images\n",
    "        device: Device to run the model on\n",
    "        save_predictions: Whether to save prediction overlays\n",
    "    \"\"\"\n",
    "    supported_formats = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(filename.lower().endswith(fmt) for fmt in supported_formats):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            print(f\"\\nProcessing {filename}\")\n",
    "            visualize_single_image(model, image_path, device, save_predictions)\n",
    "\n",
    "# Use the functions:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "folder_path = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/patches/Xenium_Prime_Human_Lung_Cancer_FFPE_outs/Yasin_Model\"\n",
    "process_folder(he_model, folder_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81528f9d-0c6d-4685-828c-82c7700ba174",
   "metadata": {},
   "source": [
    "## NuInSeg validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b686e-d588-42f2-9a28-397a89e97428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from glob import glob\n",
    "import os\n",
    "import torch\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "def predict_binary_segmentation(he_model, base_path=\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/NuInsSeg\", device='cuda'):\n",
    "   model_size = (256, 256)\n",
    "   \n",
    "   organ_paths = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "   print(f\"\\nAvailable organs: {organ_paths}\")\n",
    "   \n",
    "   organ = random.choice(organ_paths)\n",
    "   organ_path = os.path.join(base_path, organ)\n",
    "   \n",
    "   img_paths = glob(os.path.join(organ_path, 'tissue images', '*.png'))\n",
    "   mask_paths = glob(os.path.join(organ_path, 'mask binary', '*.png'))\n",
    "   \n",
    "   print(f\"\\nNumber of images found: {len(img_paths)}\")\n",
    "   print(f\"Number of masks found: {len(mask_paths)}\")\n",
    "   \n",
    "   idx = random.randint(0, len(img_paths)-1)\n",
    "   img_path = img_paths[idx]\n",
    "   mask_path = mask_paths[idx]\n",
    "   \n",
    "   print(f\"\\nSelected image: {img_path}\")\n",
    "   print(f\"Selected mask: {mask_path}\")\n",
    "   \n",
    "   image = cv2.imread(img_path)\n",
    "   image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "   print(f\"Original image shape: {image.shape}\")\n",
    "   \n",
    "   image = cv2.resize(image, model_size)\n",
    "   print(f\"Resized image shape: {image.shape}\")\n",
    "   \n",
    "   # Updated normalization\n",
    "   image = image.astype(np.float32)\n",
    "   mean = [0.5, 0.5, 0.5]\n",
    "   std = [0.5, 0.5, 0.5]\n",
    "   image = (image / 255.0 - mean) / std\n",
    "   print(f\"Normalized image range: {image.min()} - {image.max()}\")\n",
    "   \n",
    "   image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "   print(f\"Image tensor shape: {image_tensor.shape}\")\n",
    "   \n",
    "   mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "   print(f\"Original mask shape: {mask.shape}\")\n",
    "   \n",
    "   mask = cv2.resize(mask, model_size)\n",
    "   mask = (mask > 128).astype(np.float32)\n",
    "   print(f\"Processed mask unique values: {np.unique(mask)}\")\n",
    "   \n",
    "   he_model.eval()\n",
    "   with torch.no_grad():\n",
    "       outputs = he_model(image_tensor)\n",
    "       print(\"\\nModel outputs:\", outputs.keys())\n",
    "       \n",
    "       logits = outputs['cell_mask'][0]\n",
    "       print(f\"Logits shape: {logits.shape}\")\n",
    "       print(f\"Logits range: {logits.min().item()} - {logits.max().item()}\")\n",
    "       \n",
    "       pred_map = np.concatenate([\n",
    "           np.zeros((model_size[0], model_size[1], 1)),\n",
    "           torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "           outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "       ], axis=-1)\n",
    "       \n",
    "       print(f\"Pred map shape: {pred_map.shape}\")\n",
    "       \n",
    "       cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "       instance_map, _ = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "       \n",
    "       raw_pred = logits[1].cpu().numpy()\n",
    "       print(f\"Raw prediction range: {raw_pred.min()} - {raw_pred.max()}\")\n",
    "       \n",
    "       binary_pred = torch.nn.functional.softmax(logits, dim=0)[1].cpu().numpy()\n",
    "       binary_pred = (binary_pred > 0.5).astype(np.float32)  # Added thresholding\n",
    "       \n",
    "       print(f\"\\nBinary prediction shape: {binary_pred.shape}\")\n",
    "       print(f\"Binary prediction unique values: {np.unique(binary_pred)}\")\n",
    "   \n",
    "   # Denormalize image for visualization\n",
    "   image_large = cv2.resize(image * std + mean, (512, 512))\n",
    "   image_large = np.clip(image_large, 0, 1)\n",
    "   mask_large = cv2.resize(mask, (512, 512))\n",
    "   binary_pred_large = cv2.resize(binary_pred, (512, 512))\n",
    "   instance_map_large = cv2.resize(instance_map.astype(np.float32), (512, 512))\n",
    "   boundaries = find_boundaries(instance_map_large.astype(np.int32), mode='thick')\n",
    "   \n",
    "   print(f\"\\nFinal shapes:\")\n",
    "   print(f\"Image large: {image_large.shape}\")\n",
    "   print(f\"Mask large: {mask_large.shape}\")\n",
    "   print(f\"Binary pred large: {binary_pred_large.shape}\")\n",
    "   print(f\"Instance map large: {instance_map_large.shape}\")\n",
    "   \n",
    "   fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "   \n",
    "   axes[0].imshow(image_large)\n",
    "   axes[0].set_title(f'Original Image\\n{organ}')\n",
    "   axes[0].axis('off')\n",
    "   \n",
    "   axes[1].imshow(mask_large, cmap='gray')\n",
    "   axes[1].set_title('Ground Truth')\n",
    "   axes[1].axis('off')\n",
    "   \n",
    "   axes[2].imshow(raw_pred, cmap='gray')\n",
    "   axes[2].set_title('Raw Logits')\n",
    "   axes[2].axis('off')\n",
    "   \n",
    "   axes[3].imshow(binary_pred, cmap='gray')\n",
    "   axes[3].set_title('Binary Prediction')\n",
    "   axes[3].axis('off')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   print(f\"\\nNumber of detected instances: {len(np.unique(instance_map)) - 1}\")\n",
    "\n",
    "# Run prediction \n",
    "predict_binary_segmentation(he_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e1193-4e90-43c1-98e4-58776f3e56a9",
   "metadata": {},
   "source": [
    "## Metrics NuInSeg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddd32a-455d-4a45-a9d9-ece352cb639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def process_single_image(image_path, mask_path, he_model, model_size=(256, 256), device='cuda'):\n",
    "    # Load and process image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, model_size)\n",
    "    \n",
    "    # Normalize\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "    image = (image.astype(np.float32) / 255.0 - mean) / std\n",
    "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # Load mask\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, model_size)\n",
    "    mask = (mask > 128).astype(np.float32)\n",
    "    \n",
    "    # Predict and post-process\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(image_tensor)\n",
    "        pred_map = np.concatenate([\n",
    "            np.zeros((model_size[0], model_size[1], 1)),\n",
    "            torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "        instance_map, _ = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create binary instances\n",
    "        binary_instances = np.zeros_like(instance_map, dtype=np.float32)\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        for i in np.unique(instance_map):\n",
    "            if i > 0:\n",
    "                instance_mask = instance_map == i\n",
    "                eroded_mask = instance_mask & ~find_boundaries(instance_mask, mode='thick')\n",
    "                binary_instances[eroded_mask] = 1\n",
    "        \n",
    "        return mask.flatten(), binary_instances.flatten()\n",
    "\n",
    "def calculate_dataset_metrics(he_model, base_path=\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/NuInsSeg\"):\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    organ_metrics = {}\n",
    "    \n",
    "    organ_paths = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    \n",
    "    for organ in organ_paths:\n",
    "        organ_path = os.path.join(base_path, organ)\n",
    "        img_paths = glob(os.path.join(organ_path, 'tissue images', '*.png'))\n",
    "        mask_paths = glob(os.path.join(organ_path, 'mask binary', '*.png'))\n",
    "        \n",
    "        organ_y_true = []\n",
    "        organ_y_pred = []\n",
    "        \n",
    "        for img_path, mask_path in zip(sorted(img_paths), sorted(mask_paths)):\n",
    "            y_true, y_pred = process_single_image(img_path, mask_path, he_model)\n",
    "            organ_y_true.extend(y_true)\n",
    "            organ_y_pred.extend(y_pred)\n",
    "            all_y_true.extend(y_true)\n",
    "            all_y_pred.extend(y_pred)\n",
    "        \n",
    "        # Calculate metrics per organ\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            organ_y_true, organ_y_pred, average='binary'\n",
    "        )\n",
    "        organ_metrics[organ] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
    "        all_y_true, all_y_pred, average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'overall': {\n",
    "            'precision': overall_precision,\n",
    "            'recall': overall_recall,\n",
    "            'f1': overall_f1\n",
    "        },\n",
    "        'per_organ': organ_metrics\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "metrics = calculate_dataset_metrics(he_model)\n",
    "print(\"\\nOverall Metrics:\")\n",
    "print(f\"Precision: {metrics['overall']['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['overall']['recall']:.3f}\")\n",
    "print(f\"F1 Score: {metrics['overall']['f1']:.3f}\")\n",
    "\n",
    "print(\"\\nPer-organ Metrics:\")\n",
    "for organ, scores in metrics['per_organ'].items():\n",
    "    print(f\"\\n{organ}:\")\n",
    "    print(f\"Precision: {scores['precision']:.3f}\")\n",
    "    print(f\"Recall: {scores['recall']:.3f}\")\n",
    "    print(f\"F1 Score: {scores['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f5625-e9f4-4097-8967-54f0674f2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def process_and_visualize_sample(he_model, base_path=\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/NuInsSeg\", model_size=(256, 256), device='cuda'):\n",
    "    # Get random organ and image\n",
    "    organ_paths = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    organ = random.choice(organ_paths)\n",
    "    organ_path = os.path.join(base_path, organ)\n",
    "    \n",
    "    img_paths = glob(os.path.join(organ_path, 'tissue images', '*.png'))\n",
    "    mask_paths = glob(os.path.join(organ_path, 'mask binary', '*.png'))\n",
    "    \n",
    "    idx = random.randint(0, len(img_paths)-1)\n",
    "    img_path = img_paths[idx]\n",
    "    mask_path = mask_paths[idx]\n",
    "    \n",
    "    print(f\"Processing organ: {organ}\")\n",
    "    print(f\"Image: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Process image\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    original_image = cv2.resize(image, model_size)\n",
    "    \n",
    "    # Normalize\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "    image = (original_image.astype(np.float32) / 255.0 - mean) / std\n",
    "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # Load mask\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, model_size)\n",
    "    mask = (mask > 128).astype(np.float32)\n",
    "    \n",
    "    # Model prediction\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(image_tensor)\n",
    "        pred_map = np.concatenate([\n",
    "            np.zeros((model_size[0], model_size[1], 1)),\n",
    "            torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Post-process\n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "        instance_map, _ = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create binary instances\n",
    "        binary_instances = np.zeros_like(instance_map, dtype=np.float32)\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        for i in np.unique(instance_map):\n",
    "            if i > 0:\n",
    "                instance_mask = instance_map == i\n",
    "                eroded_mask = instance_mask & ~find_boundaries(instance_mask, mode='thick')\n",
    "                binary_instances[eroded_mask] = 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        y_true = mask.flatten()\n",
    "        y_pred = binary_instances.flatten()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "        \n",
    "        print(\"\\nMetrics Details:\")\n",
    "        print(f\"True Positives: {np.sum((y_pred == 1) & (y_true == 1))}\")\n",
    "        print(f\"False Positives: {np.sum((y_pred == 1) & (y_true == 0))}\")\n",
    "        print(f\"False Negatives: {np.sum((y_pred == 0) & (y_true == 1))}\")\n",
    "        print(f\"Total Cells: {len(np.unique(instance_map)) - 1}\")\n",
    "        \n",
    "        print(f\"\\nMetrics:\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall: {recall:.3f}\")\n",
    "        print(f\"F1 Score: {f1:.3f}\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        \n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(mask, cmap='gray')\n",
    "        axes[1].set_title('Ground Truth')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(binary_instances, cmap='gray')\n",
    "        axes[2].set_title('Predicted Binary Instances')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        axes[3].imshow(instance_map, cmap='nipy_spectral')\n",
    "        axes[3].set_title(f'Instance Map\\n({len(np.unique(instance_map))-1} cells)')\n",
    "        axes[3].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "sample_metrics = process_and_visualize_sample(he_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f012bd9-9408-47fa-9e6b-0f38d36314c5",
   "metadata": {},
   "source": [
    "## MonuSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f6795-e7ec-4843-bac4-9482a369d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "\n",
    "def predict_monuseg(he_model, base_path=\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MonuSeg/kmms_test/kmms_test\", device='cuda'):\n",
    "  model_size = (256, 256)\n",
    "  \n",
    "  images_path = os.path.join(base_path, \"images\")\n",
    "  masks_path = os.path.join(base_path, \"masks\")\n",
    "  \n",
    "  mask_files = set(os.listdir(masks_path))\n",
    "  img_files = [f for f in os.listdir(images_path) if not f.startswith('.')]\n",
    "  \n",
    "  while True:\n",
    "      idx = random.randint(0, len(img_files)-1)\n",
    "      img_name = img_files[idx]\n",
    "      mask_name = next((m for m in [\n",
    "          img_name.strip(),\n",
    "          img_name.replace('.png', ' .png'),\n",
    "          img_name.split('.')[0] + ' .png'\n",
    "      ] if m in mask_files), None)\n",
    "      \n",
    "      if mask_name:\n",
    "          break\n",
    "  \n",
    "  print(f\"Processing image: {img_name} with mask: {mask_name}\")\n",
    "  \n",
    "  image = cv2.imread(os.path.join(images_path, img_name))\n",
    "  if image is None:\n",
    "      raise ValueError(f\"Failed to load image: {img_name}\")\n",
    "\n",
    "  # Convert BGR to RGB\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  print(\"\\nInput Processing Info:\")\n",
    "  print(f\"Original image shape before resize: {image.shape}\")\n",
    "  image = cv2.resize(image, model_size)\n",
    "  print(f\"Image shape after resize: {image.shape}\")\n",
    "  \n",
    "  # Normalization\n",
    "  image = image.astype(np.float32)\n",
    "  mean = [0.5, 0.5, 0.5]\n",
    "  std = [0.5, 0.5, 0.5]\n",
    "  image = (image / 255.0 - mean) / std\n",
    "  print(f\"Image value range after normalization: [{image.min():.3f}, {image.max():.3f}]\")\n",
    "  \n",
    "  image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "  print(f\"Final tensor shape: {image_tensor.shape}\")\n",
    "  print(f\"Tensor dtype: {image_tensor.dtype}\")\n",
    "  \n",
    "  mask = cv2.imread(os.path.join(masks_path, mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "  if mask is None:\n",
    "      raise ValueError(f\"Failed to load mask: {mask_name}\")\n",
    "      \n",
    "  mask = cv2.resize(mask, model_size)\n",
    "  mask = (mask > 128).astype(np.float32)\n",
    "\n",
    "  he_model.eval()\n",
    "  with torch.no_grad():\n",
    "      outputs = he_model(image_tensor)\n",
    "      logits = outputs['cell_mask'][0]\n",
    "      \n",
    "      print(\"\\nModel Output Info:\")\n",
    "      print(f\"cell_mask shape: {outputs['cell_mask'].shape}\")\n",
    "      print(f\"cell_mask logits range: [{outputs['cell_mask'].min():.3f}, {outputs['cell_mask'].max():.3f}]\")\n",
    "      \n",
    "      if 'cell_hv' in outputs:\n",
    "          print(f\"cell_hv shape: {outputs['cell_hv'].shape}\")\n",
    "          print(f\"cell_hv range: [{outputs['cell_hv'].min():.3f}, {outputs['cell_hv'].max():.3f}]\")\n",
    "      \n",
    "      pred_map = np.concatenate([\n",
    "          np.zeros((model_size[0], model_size[1], 1)),\n",
    "          torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "          outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "      ], axis=-1)\n",
    "      \n",
    "      print(\"\\nPost-processing Info:\")\n",
    "      print(f\"pred_map shape: {pred_map.shape}\")\n",
    "      \n",
    "      cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "      instance_map, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "      \n",
    "      print(f\"instance_map shape: {instance_map.shape}\")\n",
    "      print(f\"instance_map unique values: {len(np.unique(instance_map))} instances\")\n",
    "      \n",
    "      raw_pred = logits[1].cpu().numpy()\n",
    "      binary_pred = torch.nn.functional.softmax(logits, dim=0)[1].cpu().numpy()\n",
    "\n",
    "        # Add this after binary_pred calculation\n",
    "      binary_pred = (binary_pred > 0.5).astype(np.float32)\n",
    "        \n",
    "  fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "  \n",
    "  # Display original image\n",
    "  img_display = image.copy() * std + mean  # Denormalize\n",
    "  img_display = np.clip(img_display, 0, 1)  # Clip values\n",
    "  axes[0].imshow(img_display)  # No need for BGR2RGB conversion since already in RGB\n",
    "  axes[0].set_title('Original Image')\n",
    "  axes[0].axis('off')\n",
    "  \n",
    "  axes[1].imshow(mask, cmap='gray')\n",
    "  axes[1].set_title('Ground Truth')\n",
    "  axes[1].axis('off')\n",
    "  \n",
    "  axes[2].imshow(raw_pred, cmap='gray')\n",
    "  axes[2].set_title('Raw Logits')\n",
    "  axes[2].axis('off')\n",
    "  \n",
    "  axes[3].imshow(binary_pred, cmap='gray')\n",
    "  axes[3].set_title('Softmax Output')\n",
    "  axes[3].axis('off')\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  \n",
    "  print(f\"\\nNumber of detected instances: {len(np.unique(instance_map)) - 1}\")\n",
    "\n",
    "# Run prediction\n",
    "predict_monuseg(he_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf447ab2-c01a-45df-bf5d-fd93bfa44299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def predict_monuseg(he_model, base_path=\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MonuSeg/kmms_test/kmms_test\", device='cuda'):\n",
    "    model_size = (256, 256)\n",
    "    \n",
    "    # Path setup\n",
    "    images_path = os.path.join(base_path, \"images\")\n",
    "    masks_path = os.path.join(base_path, \"masks\")\n",
    "    \n",
    "    # File selection\n",
    "    mask_files = set(os.listdir(masks_path))\n",
    "    img_files = [f for f in os.listdir(images_path) if not f.startswith('.')]\n",
    "    \n",
    "    while True:\n",
    "        idx = random.randint(0, len(img_files)-1)\n",
    "        img_name = img_files[idx]\n",
    "        mask_name = next((m for m in [\n",
    "            img_name.strip(),\n",
    "            img_name.replace('.png', ' .png'),\n",
    "            img_name.split('.')[0] + ' .png'\n",
    "        ] if m in mask_files), None)\n",
    "        \n",
    "        if mask_name:\n",
    "            break\n",
    "    \n",
    "    print(f\"Processing image: {img_name} with mask: {mask_name}\")\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(os.path.join(images_path, img_name))\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_name}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    print(\"\\nInput Processing Info:\")\n",
    "    print(f\"Original image shape: {image.shape}\")\n",
    "    original_image = image.copy()\n",
    "    image = cv2.resize(image, model_size)\n",
    "    print(f\"Resized shape: {image.shape}\")\n",
    "    \n",
    "    # Normalization\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "    image = image.astype(np.float32)\n",
    "    image = (image / 255.0 - mean) / std\n",
    "    print(f\"Value range after normalization: [{image.min():.3f}, {image.max():.3f}]\")\n",
    "    \n",
    "    # Create tensor\n",
    "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "    print(f\"Tensor shape: {image_tensor.shape}\")\n",
    "    \n",
    "    # Load mask\n",
    "    mask = cv2.imread(os.path.join(masks_path, mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        raise ValueError(f\"Failed to load mask: {mask_name}\")\n",
    "    mask = cv2.resize(mask, model_size)\n",
    "    mask = (mask > 128).astype(np.float32)\n",
    "    \n",
    "    # Model prediction\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(image_tensor)\n",
    "        logits = outputs['cell_mask'][0]\n",
    "        \n",
    "        print(\"\\nModel Output Info:\")\n",
    "        print(f\"cell_mask shape: {outputs['cell_mask'].shape}\")\n",
    "        print(f\"cell_mask range: [{outputs['cell_mask'].min():.3f}, {outputs['cell_mask'].max():.3f}]\")\n",
    "        \n",
    "        # Get HV maps and create pred_map\n",
    "        if 'cell_hv' in outputs:\n",
    "            print(f\"cell_hv shape: {outputs['cell_hv'].shape}\")\n",
    "            print(f\"cell_hv range: [{outputs['cell_hv'].min():.3f}, {outputs['cell_hv'].max():.3f}]\")\n",
    "            hv_maps = outputs['cell_hv'][0].cpu().numpy()\n",
    "            h_map = hv_maps[0]\n",
    "            v_map = hv_maps[1]\n",
    "            \n",
    "        pred_map = np.concatenate([\n",
    "            np.zeros((model_size[0], model_size[1], 1)),\n",
    "            torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Post-process to get instance map\n",
    "        print(\"\\nPost-processing Info:\")\n",
    "        print(f\"pred_map shape: {pred_map.shape}\")\n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "        instance_map, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        print(f\"Detected instances: {len(np.unique(instance_map)) - 1}\")\n",
    "        \n",
    "        # Get boundaries first\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "\n",
    "        # Create binary instance visualization using the same instance map as boundaries\n",
    "        binary_instances = np.zeros_like(instance_map, dtype=np.float32)\n",
    "        for i in np.unique(instance_map):\n",
    "            if i > 0:  # Skip background\n",
    "                # Create a mask for this instance without the boundary\n",
    "                instance_mask = instance_map == i\n",
    "                eroded_mask = instance_mask & ~find_boundaries(instance_mask, mode='thick')\n",
    "                binary_instances[eroded_mask] = 1\n",
    "                \n",
    "        # Add thin black boundaries between cells\n",
    "        binary_instances[boundaries] = 0  # Make boundaries black\n",
    "        \n",
    "        # Get boundaries and predictions\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        binary_pred = torch.nn.functional.softmax(logits, dim=0)[1].cpu().numpy()\n",
    "        binary_pred = (binary_pred > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Create complete visualization\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        gs = plt.GridSpec(3, 3, figure=fig)\n",
    "        \n",
    "        # Original image\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        img_display = image.copy() * std + mean\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "        ax1.imshow(img_display)\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        ax2.imshow(mask, cmap='gray')\n",
    "        ax2.set_title('Ground Truth')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Raw binary prediction\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        ax3.imshow(binary_pred, cmap='gray')\n",
    "        ax3.set_title('Raw Binary Prediction')\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        # Separated binary instances\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        ax4.imshow(binary_instances, cmap='gray')\n",
    "        ax4.set_title('Separated Binary Instances')\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # Horizontal gradients\n",
    "        ax5 = fig.add_subplot(gs[1, 1])\n",
    "        ax5.imshow(h_map, cmap='RdBu')\n",
    "        ax5.set_title('Horizontal Gradients')\n",
    "        ax5.axis('off')\n",
    "        \n",
    "        # Vertical gradients\n",
    "        ax6 = fig.add_subplot(gs[1, 2])\n",
    "        ax6.imshow(v_map, cmap='RdBu')\n",
    "        ax6.set_title('Vertical Gradients')\n",
    "        ax6.axis('off')\n",
    "        \n",
    "        # Instance segmentation\n",
    "        ax7 = fig.add_subplot(gs[2, 0])\n",
    "        ax7.imshow(instance_map, cmap='nipy_spectral')\n",
    "        ax7.set_title('Instance Segmentation')\n",
    "        ax7.axis('off')\n",
    "        \n",
    "        # Boundary overlay\n",
    "        ax8 = fig.add_subplot(gs[2, 1])\n",
    "        overlay = img_display.copy()\n",
    "        overlay[boundaries] = [0, 0, 0]  # Black boundaries\n",
    "        ax8.imshow(overlay)\n",
    "        ax8.set_title('Boundary Overlay')\n",
    "        ax8.axis('off')\n",
    "        \n",
    "        # Combined visualization\n",
    "        ax9 = fig.add_subplot(gs[2, 2])\n",
    "        combined = img_display.copy()\n",
    "        instance_overlay = plt.cm.nipy_spectral(instance_map / instance_map.max())\n",
    "        combined = combined * 0.7 + instance_overlay[:, :, :3] * 0.3\n",
    "        combined[boundaries] = [0, 0, 0]  # Black boundaries\n",
    "        ax9.imshow(combined)\n",
    "        ax9.set_title('Combined Visualization')\n",
    "        ax9.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional binary visualization\n",
    "        fig2, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(mask, cmap='gray')\n",
    "        axes[0].set_title('Ground Truth')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(binary_pred, cmap='gray')\n",
    "        axes[1].set_title('Raw Binary Prediction')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(binary_instances, cmap='gray')\n",
    "        axes[2].set_title('Separated Binary Instances')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'instance_map': instance_map,\n",
    "            'type_pred': type_pred,\n",
    "            'boundaries': boundaries,\n",
    "            'binary_pred': binary_pred,\n",
    "            'binary_instances': binary_instances,\n",
    "            'hv_maps': {'h': h_map, 'v': v_map}\n",
    "        }\n",
    "\n",
    "# Usage:\n",
    "# results = predict_monuseg(he_model, base_path=\"path/to/data\")\n",
    "\n",
    "# Usage:\n",
    "results = predict_monuseg(he_model, base_path=\"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MonuSeg/kmms_test/kmms_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf81933-9632-4e32-a45e-c6eec4febcfd",
   "metadata": {},
   "source": [
    "## MonuSeg Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e308d2c-02ea-4c2c-a964-c0c2bdacfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def process_single_image(image_path, mask_path, he_model, model_size=(256, 256), device='cuda'):\n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, model_size)\n",
    "    \n",
    "    # Normalization\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "    image = (image.astype(np.float32) / 255.0 - mean) / std\n",
    "    \n",
    "    # Create tensor\n",
    "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # Load mask\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, model_size)\n",
    "    mask = (mask > 128).astype(np.float32)\n",
    "    \n",
    "    # Model prediction\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(image_tensor)\n",
    "        \n",
    "        # Get prediction maps\n",
    "        pred_map = np.concatenate([\n",
    "            np.zeros((model_size[0], model_size[1], 1)),\n",
    "            torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Post-process\n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "        instance_map, _ = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create binary instance map\n",
    "        binary_instances = np.zeros_like(instance_map, dtype=np.float32)\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        for i in np.unique(instance_map):\n",
    "            if i > 0:\n",
    "                instance_mask = instance_map == i\n",
    "                eroded_mask = instance_mask & ~find_boundaries(instance_mask, mode='thick')\n",
    "                binary_instances[eroded_mask] = 1\n",
    "        \n",
    "        return mask.flatten(), binary_instances.flatten()\n",
    "\n",
    "def calculate_metrics_for_dataset(he_model, base_path):\n",
    "    images_path = os.path.join(base_path, \"images\")\n",
    "    masks_path = os.path.join(base_path, \"masks\")\n",
    "    \n",
    "    mask_files = set(os.listdir(masks_path))\n",
    "    img_files = [f for f in os.listdir(images_path) if not f.startswith('.')]\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    for img_name in img_files:\n",
    "        mask_name = next((m for m in [\n",
    "            img_name.strip(),\n",
    "            img_name.replace('.png', ' .png'),\n",
    "            img_name.split('.')[0] + ' .png'\n",
    "        ] if m in mask_files), None)\n",
    "        \n",
    "        if mask_name:\n",
    "            y_true, y_pred = process_single_image(\n",
    "                os.path.join(images_path, img_name),\n",
    "                os.path.join(masks_path, mask_name),\n",
    "                he_model\n",
    "            )\n",
    "            all_y_true.extend(y_true)\n",
    "            all_y_pred.extend(y_pred)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_y_true, \n",
    "        all_y_pred, \n",
    "        average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Usage:\n",
    "metrics = calculate_metrics_for_dataset(he_model, \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MonuSeg/kmms_test/kmms_test\")\n",
    "print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['recall']:.3f}\") \n",
    "print(f\"F1 Score: {metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d7140-e0ad-4266-a6f0-80dca777c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def process_and_visualize_sample(he_model, base_path, model_size=(256, 256), device='cuda'):\n",
    "    # Setup paths\n",
    "    images_path = os.path.join(base_path, \"images\")\n",
    "    masks_path = os.path.join(base_path, \"masks\")\n",
    "    \n",
    "    # Get random image\n",
    "    img_files = [f for f in os.listdir(images_path) if not f.startswith('.')]\n",
    "    mask_files = set(os.listdir(masks_path))\n",
    "    \n",
    "    img_name = random.choice(img_files)\n",
    "    mask_name = next((m for m in [\n",
    "        img_name.strip(),\n",
    "        img_name.replace('.png', ' .png'),\n",
    "        img_name.split('.')[0] + ' .png'\n",
    "    ] if m in mask_files), None)\n",
    "    \n",
    "    print(f\"Processing: {img_name}\")\n",
    "    \n",
    "    # Load and process image\n",
    "    image = cv2.imread(os.path.join(images_path, img_name))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    original_image = cv2.resize(image, model_size)\n",
    "    \n",
    "    # Normalize\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "    image = (original_image.astype(np.float32) / 255.0 - mean) / std\n",
    "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # Load mask\n",
    "    mask = cv2.imread(os.path.join(masks_path, mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, model_size)\n",
    "    mask = (mask > 128).astype(np.float32)\n",
    "    \n",
    "    # Model prediction\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(image_tensor)\n",
    "        pred_map = np.concatenate([\n",
    "            np.zeros((model_size[0], model_size[1], 1)),\n",
    "            torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Post-process\n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "        instance_map, _ = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create binary instances\n",
    "        binary_instances = np.zeros_like(instance_map, dtype=np.float32)\n",
    "        boundaries = find_boundaries(instance_map, mode='thick')\n",
    "        for i in np.unique(instance_map):\n",
    "            if i > 0:\n",
    "                instance_mask = instance_map == i\n",
    "                eroded_mask = instance_mask & ~find_boundaries(instance_mask, mode='thick')\n",
    "                binary_instances[eroded_mask] = 1\n",
    "        \n",
    "        # Calculate metrics for this sample\n",
    "        y_true = mask.flatten()\n",
    "        y_pred = binary_instances.flatten()\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "        \n",
    "        print(\"\\nMetrics Calculation Details:\")\n",
    "        print(f\"Total pixels: {len(y_true)}\")\n",
    "        print(f\"True Positives: {np.sum((y_pred == 1) & (y_true == 1))}\")\n",
    "        print(f\"False Positives: {np.sum((y_pred == 1) & (y_true == 0))}\")\n",
    "        print(f\"False Negatives: {np.sum((y_pred == 0) & (y_true == 1))}\")\n",
    "        print(f\"\\nMetrics:\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall: {recall:.3f}\")\n",
    "        print(f\"F1 Score: {f1:.3f}\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(mask, cmap='gray')\n",
    "        axes[1].set_title('Ground Truth Mask')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(binary_instances, cmap='gray')\n",
    "        axes[2].set_title('Predicted Binary Instances')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "sample_metrics = process_and_visualize_sample(\n",
    "    he_model, \n",
    "    \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MonuSeg/kmms_test/kmms_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11acb58-34cf-43ab-9247-2ceebeaa057a",
   "metadata": {},
   "source": [
    "## MoNuSegTest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdc493-cd50-41f9-a081-b9e86beaf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_xml_mask(xml_path, img_shape):\n",
    "    \"\"\"Parse XML annotations to create binary mask.\"\"\"\n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for region in root.iter('Region'):\n",
    "        vertices = []\n",
    "        for vertex in region.iter('Vertex'):\n",
    "            x = float(vertex.get('X'))\n",
    "            y = float(vertex.get('Y'))\n",
    "            vertices.append([x, y])\n",
    "            \n",
    "        if vertices:\n",
    "            vertices = np.array(vertices, dtype=np.int32)\n",
    "            cv2.fillPoly(mask, [vertices], 1)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def load_dataset(data_dir):\n",
    "    \"\"\"Load images and masks from directory.\"\"\"\n",
    "    images = []\n",
    "    masks = []\n",
    "    filenames = []\n",
    "    \n",
    "    # Get all TIFF files (excluding hidden files)\n",
    "    tiff_files = [f for f in os.listdir(data_dir) \n",
    "                 if f.endswith('.tif') and not f.startswith('.')]\n",
    "    \n",
    "    for tiff_file in sorted(tiff_files):\n",
    "        img_path = os.path.join(data_dir, tiff_file)\n",
    "        xml_path = os.path.join(data_dir, tiff_file.replace('.tif', '.xml'))\n",
    "        \n",
    "        try:\n",
    "            img = tifffile.imread(img_path)\n",
    "            if os.path.exists(xml_path):\n",
    "                mask = parse_xml_mask(xml_path, img.shape)\n",
    "                masks.append(mask)\n",
    "                images.append(img)\n",
    "                filenames.append(tiff_file)\n",
    "                print(f\"Processed {tiff_file}: Image shape {img.shape}, Mask shape {mask.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {tiff_file}: {e}\")\n",
    "            \n",
    "    return images, masks, filenames\n",
    "\n",
    "# Load data\n",
    "data_dir = \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MoNuSegTestData/\"\n",
    "images, masks, filenames = load_dataset(data_dir)\n",
    "\n",
    "# Visualize first image and mask if data was loaded\n",
    "if images and masks:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[0])\n",
    "    plt.title(f'Original Image: {filenames[0]}')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(masks[0], cmap='gray')\n",
    "    plt.title('Segmentation Mask')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1eb718-3a96-4f9a-b8e8-ead69f001fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "from cell_segmentation.utils.metrics import get_fast_pq, cell_detection_scores\n",
    "import random\n",
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def parse_xml_mask(xml_path, shape):\n",
    "    \"\"\"\n",
    "    Parse XML annotations and create instance mask with each cell properly separated\n",
    "    \"\"\"\n",
    "    instance_mask = np.zeros(shape[:2], dtype=np.uint32)\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "        \n",
    "    # Process each region separately and assign unique instance IDs\n",
    "    instance_id = 1\n",
    "    for region in root.iter('Region'):\n",
    "        region_id = region.get('Id')\n",
    "        vertices = []\n",
    "        for vertex in region.iter('Vertex'):\n",
    "            x = float(vertex.get('X'))\n",
    "            y = float(vertex.get('Y'))\n",
    "            vertices.append([x, y])\n",
    "            \n",
    "        if vertices:\n",
    "            vertices = np.array(vertices, dtype=np.int32)\n",
    "            # Create temporary mask for this specific cell\n",
    "            cell_mask = np.zeros(shape[:2], dtype=np.uint8)\n",
    "            cv2.fillPoly(cell_mask, [vertices], 1)\n",
    "            \n",
    "            # Check if this cell overlaps with any existing cells\n",
    "            if np.any(instance_mask[cell_mask == 1] > 0):\n",
    "                overlap_ids = np.unique(instance_mask[cell_mask == 1])[1:]  # Skip 0\n",
    "            \n",
    "            # Assign unique instance ID to this cell\n",
    "            instance_mask[cell_mask == 1] = instance_id\n",
    "            \n",
    "            # Print statistics for verification\n",
    "            area = np.sum(cell_mask)\n",
    "            centroid = center_of_mass(cell_mask)\n",
    "\n",
    "            \n",
    "            instance_id += 1\n",
    "    \n",
    "    # Verification step\n",
    "    unique_instances = np.unique(instance_mask)\n",
    "    num_instances = len(unique_instances) - 1  # Subtract 1 for background\n",
    "    \n",
    "    return instance_mask\n",
    "\n",
    "def process_single_patch(image_path, xml_path, he_model, patch_size=256, device='cuda'):\n",
    "    \"\"\"\n",
    "    Process a single patch with improved ground truth handling\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not read image from {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get ground truth mask with separated instances\n",
    "    gt_mask = parse_xml_mask(xml_path, image.shape)\n",
    "    \n",
    "    # Random patch selection\n",
    "    h, w = image.shape[:2]\n",
    "    max_h = h - patch_size\n",
    "    max_w = w - patch_size\n",
    "    start_h = random.randint(0, max_h)\n",
    "    start_w = random.randint(0, max_w)\n",
    "    \n",
    "    # Extract patches\n",
    "    patch = image[start_h:start_h+patch_size, start_w:start_w+patch_size]\n",
    "    patch_mask = gt_mask[start_h:start_h+patch_size, start_w:start_w+patch_size]\n",
    "    \n",
    "    # Verify patch mask instances\n",
    "    patch_instances = np.unique(patch_mask)\n",
    "    print(f\"\\nPatch contains {len(patch_instances)-1} cells\")  # Subtract 1 for background\n",
    "    \n",
    "    # Process patch through model\n",
    "    patch_norm = (patch.astype(np.float32) / 255.0 - 0.5) / 0.5\n",
    "    patch_tensor = torch.from_numpy(patch_norm).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(patch_tensor)\n",
    "        pred_map = np.concatenate([\n",
    "            np.zeros((patch_size, patch_size, 1)),\n",
    "            torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        instance_pred, _ = DetectionCellPostProcessor(nr_types=2, magnification=40).post_process_cell_segmentation(pred_map)\n",
    "        boundaries = find_boundaries(instance_pred, mode='thick')\n",
    "        \n",
    "        metrics = calculate_metrics(instance_pred, patch_mask)\n",
    "        \n",
    "        return {\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1': metrics['f1'],\n",
    "            'bPQ': metrics['bPQ'],\n",
    "            'patch': patch,\n",
    "            'patch_mask': patch_mask,\n",
    "            'instance_pred': instance_pred,\n",
    "            'boundaries': boundaries,\n",
    "            'patch_coords': (start_h, start_w)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def get_instance_centroids(instance_map):\n",
    "    centroids = {}\n",
    "    for instance_id in np.unique(instance_map):\n",
    "        if instance_id == 0:  # Skip background\n",
    "            continue\n",
    "        mask = instance_map == instance_id\n",
    "        center = center_of_mass(mask)\n",
    "        centroids[instance_id] = center\n",
    "    return centroids\n",
    "\n",
    "def match_instances(pred_centroids, gt_centroids, distance_threshold=12):\n",
    "    if not pred_centroids or not gt_centroids:\n",
    "        return [], list(pred_centroids.keys()), list(gt_centroids.keys())\n",
    "    \n",
    "    pred_coords = np.array(list(pred_centroids.values()))\n",
    "    gt_coords = np.array(list(gt_centroids.values()))\n",
    "    pred_ids = list(pred_centroids.keys())\n",
    "    gt_ids = list(gt_centroids.keys())\n",
    "    \n",
    "    distances = cdist(pred_coords, gt_coords)\n",
    "    \n",
    "    matched_pairs = []\n",
    "    used_pred = set()\n",
    "    used_gt = set()\n",
    "    \n",
    "    while True:\n",
    "        if distances.size == 0 or distances.min() > distance_threshold:\n",
    "            break\n",
    "            \n",
    "        pred_idx, gt_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "        if pred_idx not in used_pred and gt_idx not in used_gt:\n",
    "            matched_pairs.append((pred_ids[pred_idx], gt_ids[gt_idx]))\n",
    "            used_pred.add(pred_idx)\n",
    "            used_gt.add(gt_idx)\n",
    "            \n",
    "        distances[pred_idx, :] = float('inf')\n",
    "        distances[:, gt_idx] = float('inf')\n",
    "    \n",
    "    unmatched_pred = [pid for i, pid in enumerate(pred_ids) if i not in used_pred]\n",
    "    unmatched_gt = [gid for i, gid in enumerate(gt_ids) if i not in used_gt]\n",
    "    \n",
    "    return matched_pairs, unmatched_pred, unmatched_gt\n",
    "\n",
    "def calculate_metrics(pred_inst, gt_inst):\n",
    "    # Calculate centroid-based metrics\n",
    "    pred_centroids = get_instance_centroids(pred_inst)\n",
    "    gt_centroids = get_instance_centroids(gt_inst)\n",
    "    matched_pairs, unmatched_pred, unmatched_gt = match_instances(pred_centroids, gt_centroids)\n",
    "    \n",
    "    tp = len(matched_pairs)\n",
    "    fp = len(unmatched_pred)\n",
    "    fn = len(unmatched_gt)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate bPQ\n",
    "    [bDQ, bSQ, bPQ], _ = get_fast_pq(gt_inst > 0, pred_inst > 0)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'bPQ': bPQ\n",
    "    }\n",
    "\n",
    "def process_single_patch(image_path, xml_path, he_model, patch_size=256, device='cuda'):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    gt_mask = parse_xml_mask(xml_path, image.shape)\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    max_h = h - patch_size\n",
    "    max_w = w - patch_size\n",
    "    start_h = random.randint(0, max_h)\n",
    "    start_w = random.randint(0, max_w)\n",
    "    \n",
    "    patch = image[start_h:start_h+patch_size, start_w:start_w+patch_size]\n",
    "    patch_mask = gt_mask[start_h:start_h+patch_size, start_w:start_w+patch_size]\n",
    "    \n",
    "    patch_norm = (patch.astype(np.float32) / 255.0 - 0.5) / 0.5\n",
    "    patch_tensor = torch.from_numpy(patch_norm).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = he_model(patch_tensor)\n",
    "        pred_map = np.concatenate([\n",
    "            np.zeros((patch_size, patch_size, 1)),\n",
    "            torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "            outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        instance_pred, _ = DetectionCellPostProcessor(nr_types=2, magnification=40).post_process_cell_segmentation(pred_map)\n",
    "        boundaries = find_boundaries(instance_pred, mode='thick')\n",
    "        \n",
    "        metrics = calculate_metrics(instance_pred, patch_mask)\n",
    "        \n",
    "        return {\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1': metrics['f1'],\n",
    "            'bPQ': metrics['bPQ'],\n",
    "            'patch': patch,\n",
    "            'patch_mask': patch_mask,\n",
    "            'instance_pred': instance_pred,\n",
    "            'boundaries': boundaries,\n",
    "            'patch_coords': (start_h, start_w)\n",
    "        }\n",
    "    \n",
    "    \n",
    "def generate_distinct_colors(n):\n",
    "    \"\"\"Generate n visually distinct colors using HSV color space\"\"\"\n",
    "    colors = []\n",
    "    for i in range(n):\n",
    "        hue = i / n\n",
    "        saturation = 0.7 + random.random() * 0.3  # Random between 0.7-1.0\n",
    "        value = 0.7 + random.random() * 0.3       # Random between 0.7-1.0\n",
    "        \n",
    "        # Convert HSV to RGB\n",
    "        h = hue * 6\n",
    "        c = value * saturation\n",
    "        x = c * (1 - abs(h % 2 - 1))\n",
    "        m = value - c\n",
    "        \n",
    "        if h < 1:\n",
    "            rgb = (c, x, 0)\n",
    "        elif h < 2:\n",
    "            rgb = (x, c, 0)\n",
    "        elif h < 3:\n",
    "            rgb = (0, c, x)\n",
    "        elif h < 4:\n",
    "            rgb = (0, x, c)\n",
    "        elif h < 5:\n",
    "            rgb = (x, 0, c)\n",
    "        else:\n",
    "            rgb = (c, 0, x)\n",
    "            \n",
    "        rgb = tuple(int((color + m) * 255) for color in rgb)\n",
    "        colors.append(rgb)\n",
    "    \n",
    "    random.shuffle(colors)  # Shuffle to avoid similar colors being adjacent\n",
    "    return colors\n",
    "\n",
    "def create_color_instance_mask(instance_mask):\n",
    "    \"\"\"Convert instance mask to RGB image with distinct colors for each instance\"\"\"\n",
    "    unique_instances = sorted(np.unique(instance_mask))\n",
    "    if 0 in unique_instances:  # Remove background\n",
    "        unique_instances = unique_instances[1:]\n",
    "    \n",
    "    # Generate colors for each instance\n",
    "    colors = generate_distinct_colors(len(unique_instances))\n",
    "    color_map = {instance_id: color for instance_id, color in zip(unique_instances, colors)}\n",
    "    \n",
    "    # Create RGB image\n",
    "    height, width = instance_mask.shape\n",
    "    colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for instance_id, color in color_map.items():\n",
    "        colored_mask[instance_mask == instance_id] = color\n",
    "        \n",
    "    return colored_mask\n",
    "\n",
    "def visualize_patch_comparison(image_path, xml_path, he_model, patch_size=256):\n",
    "    \"\"\"\n",
    "    Enhanced visualization with improved cell separation\n",
    "    \"\"\"\n",
    "    results = process_single_patch(image_path, xml_path, he_model, patch_size)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    # Original patch\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(results['patch'])\n",
    "    plt.title('Original Patch')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ground truth with instance IDs\n",
    "    plt.subplot(142)\n",
    "    gt_colored = create_color_instance_mask(results['patch_mask'])\n",
    "    plt.imshow(gt_colored)\n",
    "    # Add instance ID labels\n",
    "    for inst_id in np.unique(results['patch_mask']):\n",
    "        if inst_id == 0:  # Skip background\n",
    "            continue\n",
    "        mask = results['patch_mask'] == inst_id\n",
    "        cy, cx = center_of_mass(mask)\n",
    "        plt.text(cx, cy, str(inst_id), color='white', \n",
    "                horizontalalignment='center', verticalalignment='center',\n",
    "                bbox=dict(facecolor='black', alpha=0.7))\n",
    "    plt.title('Ground Truth (with IDs)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Predicted instances\n",
    "    plt.subplot(143)\n",
    "    pred_colored = create_color_instance_mask(results['instance_pred'])\n",
    "    plt.imshow(pred_colored)\n",
    "    plt.title('Predicted Instances')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Boundary overlay\n",
    "    plt.subplot(144)\n",
    "    overlay = results['patch'].copy()\n",
    "    overlay[results['boundaries']] = [255, 0, 0]  # Red boundaries\n",
    "    gt_boundaries = find_boundaries(results['patch_mask'])\n",
    "    overlay[gt_boundaries] = [0, 255, 0]  # Green boundaries for ground truth\n",
    "    plt.imshow(overlay)\n",
    "    plt.title('Boundaries (Red: Pred, Green: GT)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMetrics:\")\n",
    "    metrics = {k: results[k] for k in ['precision', 'recall', 'f1', 'bPQ']}\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "img_path = \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MoNuSegTestData/TCGA-44-2665-01B-06-BS6.tif\"\n",
    "xml_path = \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MoNuSegTestData/TCGA-44-2665-01B-06-BS6.xml\"\n",
    "\n",
    "\n",
    "results = visualize_patch_comparison(img_path, xml_path, he_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c3aed-298d-4a10-86e2-1d747502645a",
   "metadata": {},
   "source": [
    "## whole image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ec808-a191-407f-bc52-cdd4ee037e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_segmentation.utils.metrics import get_fast_pq, cell_detection_scores\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "class CellDetector:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "\n",
    "    def parse_xml_mask(self, xml_path, shape):\n",
    "        \"\"\"\n",
    "        Parse XML annotations and create instance mask with each cell properly separated\n",
    "        \"\"\"\n",
    "        instance_mask = np.zeros(shape[:2], dtype=np.uint32)\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "            \n",
    "        instance_id = 1\n",
    "        for region in root.iter('Region'):\n",
    "            vertices = []\n",
    "            for vertex in region.iter('Vertex'):\n",
    "                x = float(vertex.get('X'))\n",
    "                y = float(vertex.get('Y'))\n",
    "                vertices.append([x, y])\n",
    "                \n",
    "            if vertices:\n",
    "                vertices = np.array(vertices, dtype=np.int32)\n",
    "                cell_mask = np.zeros(shape[:2], dtype=np.uint8)\n",
    "                cv2.fillPoly(cell_mask, [vertices], 1)\n",
    "                \n",
    "                # Check for overlaps\n",
    "                if np.any(instance_mask[cell_mask == 1] > 0):\n",
    "                    overlap_ids = np.unique(instance_mask[cell_mask == 1])[1:]\n",
    "                \n",
    "                instance_mask[cell_mask == 1] = instance_id\n",
    "                instance_id += 1\n",
    "        \n",
    "        return instance_mask\n",
    "\n",
    "    def get_instance_centroids(self, instance_map):\n",
    "        \"\"\"\n",
    "        Calculate centroids for each instance in the map\n",
    "        \"\"\"\n",
    "        centroids = {}\n",
    "        for instance_id in np.unique(instance_map):\n",
    "            if instance_id == 0:  # Skip background\n",
    "                continue\n",
    "            mask = instance_map == instance_id\n",
    "            center = center_of_mass(mask)\n",
    "            centroids[instance_id] = center\n",
    "        return centroids\n",
    "\n",
    "    def match_instances(self, pred_centroids, gt_centroids, distance_threshold=12):\n",
    "        \"\"\"\n",
    "        Match predicted instances with ground truth instances\n",
    "        \"\"\"\n",
    "        if not pred_centroids or not gt_centroids:\n",
    "            return [], list(pred_centroids.keys()), list(gt_centroids.keys())\n",
    "        \n",
    "        pred_coords = np.array(list(pred_centroids.values()))\n",
    "        gt_coords = np.array(list(gt_centroids.values()))\n",
    "        pred_ids = list(pred_centroids.keys())\n",
    "        gt_ids = list(gt_centroids.keys())\n",
    "        \n",
    "        distances = cdist(pred_coords, gt_coords)\n",
    "        \n",
    "        matched_pairs = []\n",
    "        used_pred = set()\n",
    "        used_gt = set()\n",
    "        \n",
    "        while True:\n",
    "            if distances.size == 0 or distances.min() > distance_threshold:\n",
    "                break\n",
    "                \n",
    "            pred_idx, gt_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "            if pred_idx not in used_pred and gt_idx not in used_gt:\n",
    "                matched_pairs.append((pred_ids[pred_idx], gt_ids[gt_idx]))\n",
    "                used_pred.add(pred_idx)\n",
    "                used_gt.add(gt_idx)\n",
    "                \n",
    "            distances[pred_idx, :] = float('inf')\n",
    "            distances[:, gt_idx] = float('inf')\n",
    "        \n",
    "        unmatched_pred = [pid for i, pid in enumerate(pred_ids) if i not in used_pred]\n",
    "        unmatched_gt = [gid for i, gid in enumerate(gt_ids) if i not in used_gt]\n",
    "        \n",
    "        return matched_pairs, unmatched_pred, unmatched_gt\n",
    "\n",
    "    def calculate_metrics(self, pred_inst, gt_inst):\n",
    "        \"\"\"\n",
    "        Calculate detection and segmentation metrics including bPQ\n",
    "        \"\"\"\n",
    "        pred_centroids = self.get_instance_centroids(pred_inst)\n",
    "        gt_centroids = self.get_instance_centroids(gt_inst)\n",
    "        matched_pairs, unmatched_pred, unmatched_gt = self.match_instances(pred_centroids, gt_centroids)\n",
    "        \n",
    "        tp = len(matched_pairs)\n",
    "        fp = len(unmatched_pred)\n",
    "        fn = len(unmatched_gt)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Calculate bPQ\n",
    "        [bDQ, bSQ, bPQ], _ = get_fast_pq(gt_inst > 0, pred_inst > 0)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'bPQ': bPQ\n",
    "        }\n",
    "\n",
    "    def generate_distinct_colors(self, n):\n",
    "        \"\"\"\n",
    "        Generate visually distinct colors for instance visualization\n",
    "        \"\"\"\n",
    "        colors = []\n",
    "        for i in range(n):\n",
    "            hue = i / n\n",
    "            saturation = 0.7 + random.random() * 0.3\n",
    "            value = 0.7 + random.random() * 0.3\n",
    "            \n",
    "            h = hue * 6\n",
    "            c = value * saturation\n",
    "            x = c * (1 - abs(h % 2 - 1))\n",
    "            m = value - c\n",
    "            \n",
    "            if h < 1:\n",
    "                rgb = (c, x, 0)\n",
    "            elif h < 2:\n",
    "                rgb = (x, c, 0)\n",
    "            elif h < 3:\n",
    "                rgb = (0, c, x)\n",
    "            elif h < 4:\n",
    "                rgb = (0, x, c)\n",
    "            elif h < 5:\n",
    "                rgb = (x, 0, c)\n",
    "            else:\n",
    "                rgb = (c, 0, x)\n",
    "                \n",
    "            rgb = tuple(int((color + m) * 255) for color in rgb)\n",
    "            colors.append(rgb)\n",
    "        \n",
    "        random.shuffle(colors)\n",
    "        return colors\n",
    "\n",
    "    def create_color_instance_mask(self, instance_mask):\n",
    "        \"\"\"\n",
    "        Convert instance mask to RGB visualization\n",
    "        \"\"\"\n",
    "        unique_instances = sorted(np.unique(instance_mask))\n",
    "        if 0 in unique_instances:\n",
    "            unique_instances = unique_instances[1:]\n",
    "        \n",
    "        colors = self.generate_distinct_colors(len(unique_instances))\n",
    "        color_map = {instance_id: color for instance_id, color in zip(unique_instances, colors)}\n",
    "        \n",
    "        height, width = instance_mask.shape\n",
    "        colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        \n",
    "        for instance_id, color in color_map.items():\n",
    "            colored_mask[instance_mask == instance_id] = color\n",
    "            \n",
    "        return colored_mask\n",
    "\n",
    "    def process_full_image(self, image_path, xml_path, model, target_size=1024):\n",
    "        # Read and scale image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Calculate scaling factor\n",
    "        scale_factor = target_size / max(image.shape[:2])\n",
    "        new_height = int(image.shape[0] * scale_factor)\n",
    "        new_width = int(image.shape[1] * scale_factor)\n",
    "\n",
    "        # Add prints here \n",
    "        print(f\"Original image size: {image.shape[:2]}\")\n",
    "        print(f\"Scaled image size: {new_height}x{new_width}\")\n",
    "        print(f\"Scale factor: {scale_factor}\")\n",
    "        n_patches_h = (new_height + target_size - 1) // target_size\n",
    "        n_patches_w = (new_width + target_size - 1) // target_size\n",
    "        print(f\"Image will be split into {n_patches_h}x{n_patches_w} = {n_patches_h * n_patches_w} patches\")\n",
    "\n",
    "        # Resize image\n",
    "        image_resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Create ground truth mask\n",
    "        gt_mask = self.parse_xml_mask(xml_path, image.shape)\n",
    "        \n",
    "        # Custom resize for instance mask that preserves instance IDs\n",
    "        gt_mask_resized = np.zeros((new_height, new_width), dtype=np.uint32)\n",
    "        scale_y = new_height / gt_mask.shape[0]\n",
    "        scale_x = new_width / gt_mask.shape[1]\n",
    "        \n",
    "        # For each instance, resize its binary mask and then multiply back by instance ID\n",
    "        for instance_id in np.unique(gt_mask):\n",
    "            if instance_id == 0:  # Skip background\n",
    "                continue\n",
    "            # Create binary mask for this instance\n",
    "            binary_mask = (gt_mask == instance_id).astype(np.uint8)\n",
    "            # Resize binary mask\n",
    "            binary_mask_resized = cv2.resize(binary_mask, (new_width, new_height), \n",
    "                                           interpolation=cv2.INTER_NEAREST)\n",
    "            # Add back to result with correct instance ID\n",
    "            gt_mask_resized[binary_mask_resized > 0] = instance_id\n",
    "        \n",
    "        # Calculate patches\n",
    "        n_patches_h = (new_height + target_size - 1) // target_size\n",
    "        n_patches_w = (new_width + target_size - 1) // target_size\n",
    "        \n",
    "        # Pad image and mask\n",
    "        pad_h = n_patches_h * target_size - new_height\n",
    "        pad_w = n_patches_w * target_size - new_width\n",
    "        \n",
    "        image_padded = np.pad(image_resized, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "        gt_mask_padded = np.pad(gt_mask_resized, ((0, pad_h), (0, pad_w)), mode='constant')\n",
    "        \n",
    "        # Initialize prediction mask and metrics\n",
    "        pred_mask_full = np.zeros_like(gt_mask_padded)\n",
    "        metrics_list = []\n",
    "        \n",
    "        # Process patches\n",
    "        print(\"Processing patches...\")\n",
    "        for i in tqdm(range(n_patches_h)):\n",
    "            for j in range(n_patches_w):\n",
    "                start_h = i * target_size\n",
    "                start_w = j * target_size\n",
    "                end_h = start_h + target_size\n",
    "                end_w = start_w + target_size\n",
    "                \n",
    "                patch = image_padded[start_h:end_h, start_w:end_w]\n",
    "                patch_mask = gt_mask_padded[start_h:end_h, start_w:end_w]\n",
    "                \n",
    "                # Normalize patch\n",
    "                patch_norm = (patch.astype(np.float32) / 255.0 - 0.5) / 0.5\n",
    "                patch_tensor = torch.from_numpy(patch_norm).permute(2, 0, 1).unsqueeze(0).float().to(self.device)\n",
    "                \n",
    "                # Model inference\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(patch_tensor)\n",
    "                    pred_map = np.concatenate([\n",
    "                        np.zeros((target_size, target_size, 1)),\n",
    "                        torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                        outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "                    ], axis=-1)\n",
    "                    \n",
    "                    instance_pred, _ = DetectionCellPostProcessor(nr_types=2, magnification=40).post_process_cell_segmentation(pred_map)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    patch_metrics = self.calculate_metrics(instance_pred, patch_mask)\n",
    "                    metrics_list.append(patch_metrics)\n",
    "                    \n",
    "                    # Store predictions\n",
    "                    pred_mask_full[start_h:end_h, start_w:end_w] = instance_pred\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {\n",
    "            'precision': np.mean([m['precision'] for m in metrics_list]),\n",
    "            'recall': np.mean([m['recall'] for m in metrics_list]),\n",
    "            'f1': np.mean([m['f1'] for m in metrics_list]),\n",
    "            'bPQ': np.mean([m['bPQ'] for m in metrics_list])\n",
    "        }\n",
    "        \n",
    "        # Crop to original size\n",
    "        pred_mask_full = pred_mask_full[:new_height, :new_width]\n",
    "        gt_mask_padded = gt_mask_padded[:new_height, :new_width]\n",
    "        \n",
    "        return {\n",
    "            'pred_mask': pred_mask_full,\n",
    "            'gt_mask': gt_mask_padded,\n",
    "            'metrics': avg_metrics,\n",
    "            'all_patch_metrics': metrics_list\n",
    "        }\n",
    "\n",
    "    def visualize_results(self, results):\n",
    "        \"\"\"\n",
    "        Visualize full image results and metrics\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        \n",
    "        # Ground truth\n",
    "        plt.subplot(121)\n",
    "        gt_colored = self.create_color_instance_mask(results['gt_mask'])\n",
    "        plt.imshow(gt_colored)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        plt.subplot(122)\n",
    "        pred_colored = self.create_color_instance_mask(results['pred_mask'])\n",
    "        plt.imshow(pred_colored)\n",
    "        plt.title('Prediction')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"\\nOverall Metrics:\")\n",
    "        for k, v in results['metrics'].items():\n",
    "            print(f\"{k}: {v:.3f}\")\n",
    "        \n",
    "        # Plot metrics distribution\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        metrics = ['precision', 'recall', 'f1', 'bPQ']\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.subplot(1, 4, i+1)\n",
    "            values = [m[metric] for m in results['all_patch_metrics']]\n",
    "            plt.hist(values, bins=20)\n",
    "            plt.title(f'{metric} Distribution')\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "img_path = \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MoNuSegTestData/TCGA-ZF-A9R5-01A-01-TS1.tif\"\n",
    "xml_path = \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MoNuSegTestData/TCGA-ZF-A9R5-01A-01-TS1.xml\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = CellDetector(device='cuda')\n",
    "\n",
    "# Process image and visualize results using your existing he_model\n",
    "results = detector.process_full_image(img_path, xml_path, he_model)\n",
    "detector.visualize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba2897-9a43-4bd8-83a1-bfa9907618ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_dataset(data_dir, detector, model):\n",
    "    \"\"\"\n",
    "    Process all images in the dataset directory and calculate average metrics\n",
    "    \"\"\"\n",
    "    # Get all image files (excluding hidden files)\n",
    "    image_files = [f for f in os.listdir(data_dir) \n",
    "                   if f.endswith('.tif') \n",
    "                   and not f.startswith('._')\n",
    "                   and not f.startswith('.')]\n",
    "    \n",
    "    # Lists to store metrics for each image\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Process each image\n",
    "    print(f\"\\nProcessing {len(image_files)} images...\")\n",
    "    for img_file in tqdm(image_files):\n",
    "        img_path = os.path.join(data_dir, img_file)\n",
    "        xml_file = img_file.replace('.tif', '.xml')\n",
    "        xml_path = os.path.join(data_dir, xml_file)\n",
    "        \n",
    "        if not os.path.exists(xml_path):\n",
    "            print(f\"Warning: No XML file found for {img_file}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Process the image\n",
    "            results = detector.process_full_image(img_path, xml_path, model)\n",
    "            all_metrics.append(results['metrics'])\n",
    "            \n",
    "            # Print individual image results\n",
    "            print(f\"\\nMetrics for {img_file}:\")\n",
    "            for metric, value in results['metrics'].items():\n",
    "                print(f\"{metric}: {value:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate overall averages\n",
    "    if all_metrics:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"OVERALL AVERAGES ACROSS ALL IMAGES:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        metrics_keys = all_metrics[0].keys()\n",
    "        overall_metrics = {}\n",
    "        \n",
    "        for metric in metrics_keys:\n",
    "            values = [m[metric] for m in all_metrics]\n",
    "            mean_value = np.mean(values)\n",
    "            std_value = np.std(values)\n",
    "            \n",
    "            overall_metrics[metric] = {\n",
    "                'mean': mean_value,\n",
    "                'std': std_value\n",
    "            }\n",
    "            \n",
    "            print(f\"{metric}:\")\n",
    "            print(f\"  Mean: {mean_value:.3f}\")\n",
    "            print(f\"  Std:  {std_value:.3f}\")\n",
    "    \n",
    "    return overall_metrics\n",
    "\n",
    "# Path to your dataset\n",
    "data_dir = \"/rsrch5/home/plm/yshokrollahi/CellViT/configs/datasets/MoNuSegTestData\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = CellDetector(device='cuda')\n",
    "\n",
    "# Process all images and get overall metrics\n",
    "overall_metrics = process_dataset(data_dir, detector, he_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b17faf-b871-4ed4-82d2-1a339547b46a",
   "metadata": {},
   "source": [
    "## patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cdd9a-a4b9-429f-b2bb-e24dfc99ac88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab26898b-0484-4ae7-8280-351897f5c0be",
   "metadata": {},
   "source": [
    "## HE Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9017325-556f-4897-8ae1-797af4f73391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get predictions and visualize\n",
    "def visualize_and_calculate_metrics(he_model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Visualize predictions, ground truth, and calculate detailed metrics\n",
    "    \"\"\"\n",
    "    # Get a random sample\n",
    "    for batch in test_loader:\n",
    "        break\n",
    "\n",
    "    # Get predictions\n",
    "    he_model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = batch['image'].to(device)\n",
    "        outputs = he_model(images)\n",
    "        \n",
    "        # Initialize post-processor\n",
    "        cell_post_processor = DetectionCellPostProcessor(nr_types=6, magnification=40)\n",
    "        \n",
    "        # Process predictions\n",
    "        if 'cell_hv' in outputs:\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "            ], axis=-1)\n",
    "        else:\n",
    "            pred_map = np.concatenate([\n",
    "                torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                np.zeros((outputs['cell_types'][0].shape[1], outputs['cell_types'][0].shape[2], 2))\n",
    "            ], axis=-1)\n",
    "\n",
    "        instance_map, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(25, 6))\n",
    "        \n",
    "        # Original image\n",
    "        img_display = images[0].cpu().numpy()\n",
    "        img_display = np.moveaxis(img_display, 0, -1)\n",
    "        img_display = np.clip((img_display - img_display.min()) / \n",
    "                            (img_display.max() - img_display.min() + 1e-8), 0, 1)\n",
    "        axes[0].imshow(img_display)\n",
    "        axes[0].set_title('Original H&E')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Instance Segmentation\n",
    "        axes[1].imshow(instance_map, cmap='nipy_spectral')\n",
    "        axes[1].set_title('Instance Segmentation')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Define class colors and names\n",
    "        class_colors = {\n",
    "            0: 'black',\n",
    "            1: '#00FF00',  # Neoplastic\n",
    "            2: 'blue',     # Inflammatory\n",
    "            3: 'yellow',   # Connective\n",
    "            4: 'magenta',  # Dead\n",
    "            5: 'cyan'      # Epithelial\n",
    "        }\n",
    "        \n",
    "        class_names = {\n",
    "            0: \"Background\",\n",
    "            1: \"Neoplastic\", \n",
    "            2: \"Inflammatory\",\n",
    "            3: \"Connective\", \n",
    "            4: \"Dead\", \n",
    "            5: \"Epithelial\"\n",
    "        }\n",
    "        \n",
    "        # Ground Truth Cell Type Map\n",
    "        gt_type_overlay = img_display.copy()\n",
    "        if 'semantic' in batch['targets'] and 'cell_types' in batch['targets']['semantic']:\n",
    "            cell_types = batch['targets']['semantic']['cell_types'][0].cpu().numpy()\n",
    "            cell_mask = batch['targets']['masks']['cell'][0].squeeze().cpu().numpy()\n",
    "\n",
    "            gt_type_map = np.zeros_like(cell_mask)\n",
    "            for i in range(1, cell_mask.max() + 1):\n",
    "                mask = cell_mask == i\n",
    "                if mask.any():\n",
    "                    masked_types = cell_types[mask]\n",
    "                    if len(masked_types) > 0:\n",
    "                        cell_type = np.argmax(np.bincount(masked_types))\n",
    "                        gt_type_overlay[mask] = np.array(mcolors.to_rgb(class_colors[cell_type]))\n",
    "\n",
    "        axes[2].imshow(gt_type_overlay)\n",
    "        axes[2].set_title('Ground Truth Cell Types')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Predicted Cell Type Map\n",
    "        pred_type_overlay = img_display.copy()\n",
    "        for cell_id, cell_info in type_pred.items():\n",
    "            cell_mask = instance_map == cell_id\n",
    "            cell_type = cell_info['type']\n",
    "            pred_type_overlay[cell_mask] = np.array(mcolors.to_rgb(class_colors[cell_type]))\n",
    "        axes[3].imshow(pred_type_overlay)\n",
    "        axes[3].set_title('Predicted Cell Types')\n",
    "        axes[3].axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, edgecolor='none', \n",
    "                         label=class_names[i]) for i, color in class_colors.items()]\n",
    "        fig.legend(handles=legend_elements, loc='lower center', \n",
    "                  ncol=len(class_names), bbox_to_anchor=(0.5, 0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate detailed metrics\n",
    "        pred_cells = []\n",
    "        true_cells = []\n",
    "        \n",
    "        # Get predicted cells\n",
    "        for cell_id, cell_info in type_pred.items():\n",
    "            cell_mask = instance_map == cell_id\n",
    "            pred_cells.append({\n",
    "                'id': cell_id,\n",
    "                'type': cell_info['type'],\n",
    "                'mask': cell_mask\n",
    "            })\n",
    "            \n",
    "        # Get ground truth cells if available\n",
    "        if 'semantic' in batch['targets'] and 'cell_types' in batch['targets']['semantic']:\n",
    "            cell_types = batch['targets']['semantic']['cell_types'][0].cpu().numpy()\n",
    "            cell_mask = batch['targets']['masks']['cell'][0].squeeze().cpu().numpy()\n",
    "            \n",
    "            for i in range(1, cell_mask.max() + 1):\n",
    "                mask = cell_mask == i\n",
    "                if mask.any():\n",
    "                    masked_types = cell_types[mask]\n",
    "                    if len(masked_types) > 0:\n",
    "                        true_type = np.argmax(np.bincount(masked_types))\n",
    "                        true_cells.append({\n",
    "                            'id': i,\n",
    "                            'type': true_type,\n",
    "                            'mask': mask\n",
    "                        })\n",
    "\n",
    "        # Calculate detection metrics\n",
    "        TPd = 0  # True Positive detections\n",
    "        FPd = 0  # False Positive detections\n",
    "        FNd = 0  # False Negative detections\n",
    "        \n",
    "        # Calculate classification metrics per class\n",
    "        TPc = {i: 0 for i in range(1, 6)}  # True Positive classifications\n",
    "        FPc = {i: 0 for i in range(1, 6)}  # False Positive classifications\n",
    "        FNc = {i: 0 for i in range(1, 6)}  # False Negative classifications\n",
    "\n",
    "        # Match predictions to ground truth\n",
    "        matched_pred = set()\n",
    "        matched_true = set()\n",
    "        \n",
    "        for pred_cell in pred_cells:\n",
    "            found_match = False\n",
    "            for true_cell in true_cells:\n",
    "                if not true_cell['id'] in matched_true:\n",
    "                    # Calculate IoU\n",
    "                    intersection = np.logical_and(pred_cell['mask'], true_cell['mask']).sum()\n",
    "                    union = np.logical_or(pred_cell['mask'], true_cell['mask']).sum()\n",
    "                    iou = intersection / union if union > 0 else 0\n",
    "                    \n",
    "                    if iou > 0.5:  # IoU threshold\n",
    "                        TPd += 1\n",
    "                        found_match = True\n",
    "                        matched_pred.add(pred_cell['id'])\n",
    "                        matched_true.add(true_cell['id'])\n",
    "                        \n",
    "                        # Check classification\n",
    "                        if pred_cell['type'] == true_cell['type']:\n",
    "                            TPc[pred_cell['type']] += 1\n",
    "                        else:\n",
    "                            FPc[pred_cell['type']] += 1\n",
    "                            FNc[true_cell['type']] += 1\n",
    "                        break\n",
    "            \n",
    "            if not found_match:\n",
    "                FPd += 1\n",
    "                FPc[pred_cell['type']] += 1\n",
    "\n",
    "        # Count unmatched ground truth cells as FN\n",
    "        FNd = len(true_cells) - len(matched_true)\n",
    "        for true_cell in true_cells:\n",
    "            if true_cell['id'] not in matched_true:\n",
    "                FNc[true_cell['type']] += 1\n",
    "\n",
    "        # Print results\n",
    "        # Print results (same as before)\n",
    "        print(\"\\nDetection Metrics:\")\n",
    "        print(f\"TPd: {TPd}, FPd: {FPd}, FNd: {FNd}\")\n",
    "        print(f\"Detection Precision: {TPd/(TPd+FPd):.3f}\")\n",
    "        print(f\"Detection Recall: {TPd/(TPd+FNd):.3f}\")\n",
    "        \n",
    "        print(\"\\nClassification Metrics per Class:\")\n",
    "        class_names = {\n",
    "            1: \"Neoplastic\", \n",
    "            2: \"Inflammatory\",\n",
    "            3: \"Connective\", \n",
    "            4: \"Dead\", \n",
    "            5: \"Epithelial\"\n",
    "        }\n",
    "        for class_id in range(1, 6):\n",
    "            print(f\"\\n{class_names[class_id]}:\")\n",
    "            print(f\"TPc: {TPc[class_id]}, FPc: {FPc[class_id]}, FNc: {FNc[class_id]}\")\n",
    "            precision = TPc[class_id]/(TPc[class_id]+FPc[class_id]) if (TPc[class_id]+FPc[class_id]) > 0 else 0\n",
    "            recall = TPc[class_id]/(TPc[class_id]+FNc[class_id]) if (TPc[class_id]+FNc[class_id]) > 0 else 0\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall: {recall:.3f}\")\n",
    "            \n",
    "\n",
    "# Run the visualization and metrics calculation\n",
    "visualize_and_calculate_metrics(he_model, loaders['he']['test'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4fc64-2ea0-4ea0-a0e7-c4bb7faac4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "import os\n",
    "\n",
    "class_names = {\n",
    "    0: \"Background\",\n",
    "    1: \"Neoplastic\",\n",
    "    2: \"Inflammatory\",\n",
    "    3: \"Connective\", \n",
    "    4: \"Dead\",\n",
    "    5: \"Epithelial\"\n",
    "}\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    labels = [class_names[i] for i in range(1, 6)]\n",
    "    \n",
    "    sns.heatmap(confusion_matrix, \n",
    "                annot=True, \n",
    "                fmt='.0f',\n",
    "                cmap='Blues',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def get_instance_centroids_with_types(instance_map, type_map):\n",
    "    instances = {}\n",
    "    for instance_id in np.unique(instance_map):\n",
    "        if instance_id == 0:  # Skip background\n",
    "            continue\n",
    "        mask = instance_map == instance_id\n",
    "        center = center_of_mass(mask)\n",
    "        # Ensure type_map values are within valid range\n",
    "        valid_type_map = np.clip(type_map[mask], 0, 5)  # Clip to valid range\n",
    "        type_counts = np.bincount(valid_type_map.flatten(), minlength=6)\n",
    "        cell_type = type_counts[1:].argmax() + 1  # Get most common non-background type\n",
    "        instances[instance_id] = {\n",
    "            'centroid': center,\n",
    "            'type': min(cell_type, 5)  # Ensure type is within valid range\n",
    "        }\n",
    "    return instances\n",
    "\n",
    "def match_instances_with_types(pred_instances, gt_instances, distance_threshold=12):\n",
    "    if not pred_instances or not gt_instances:\n",
    "        return [], list(pred_instances.keys()), list(gt_instances.keys())\n",
    "    \n",
    "    pred_coords = np.array([info['centroid'] for info in pred_instances.values()])\n",
    "    gt_coords = np.array([info['centroid'] for info in gt_instances.values()])\n",
    "    pred_ids = list(pred_instances.keys())\n",
    "    gt_ids = list(gt_instances.keys())\n",
    "    \n",
    "    distances = cdist(pred_coords, gt_coords)\n",
    "    \n",
    "    matched_pairs = []\n",
    "    used_pred = set()\n",
    "    used_gt = set()\n",
    "    \n",
    "    while distances.size > 0 and distances.min() <= distance_threshold:\n",
    "        pred_idx, gt_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "        if pred_idx not in used_pred and gt_idx not in used_gt:\n",
    "            matched_pairs.append((pred_ids[pred_idx], gt_ids[gt_idx]))\n",
    "            used_pred.add(pred_idx)\n",
    "            used_gt.add(gt_idx)\n",
    "        \n",
    "        distances[pred_idx, :] = float('inf')\n",
    "        distances[:, gt_idx] = float('inf')\n",
    "    \n",
    "    unmatched_pred = [pid for i, pid in enumerate(pred_ids) if i not in used_pred]\n",
    "    unmatched_gt = [gid for i, gid in enumerate(gt_ids) if i not in used_gt]\n",
    "    \n",
    "    return matched_pairs, unmatched_pred, unmatched_gt\n",
    "\n",
    "def calculate_confusion_matrix(matched_pairs, pred_instances, gt_instances):\n",
    "    num_classes = 5  # Excluding background, classes 1-5\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    for pred_id, gt_id in matched_pairs:\n",
    "        # Ensure types are within valid range (1-5)\n",
    "        pred_type = min(max(pred_instances[pred_id]['type'] - 1, 0), 4)\n",
    "        gt_type = min(max(gt_instances[gt_id]['type'] - 1, 0), 4)\n",
    "        confusion_matrix[gt_type, pred_type] += 1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "def calculate_class_metrics(confusion_matrix, unmatched_pred, unmatched_gt):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    metrics = {}\n",
    "    \n",
    "    fpd = unmatched_pred\n",
    "    fnd = unmatched_gt\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        tpc = confusion_matrix[i, i]\n",
    "        tnc = np.sum(np.diag(confusion_matrix)) - tpc\n",
    "        \n",
    "        fpc = confusion_matrix[:, i].sum() - tpc\n",
    "        fnc = confusion_matrix[i, :].sum() - tpc\n",
    "        \n",
    "        precision = (tpc + tnc) / (tpc + tnc + 2*fpc + fpd) if (tpc + tnc + 2*fpc + fpd) > 0 else 0\n",
    "        recall = (tpc + tnc) / (tpc + tnc + 2*fnc + fnd) if (tpc + tnc + 2*fnc + fnd) > 0 else 0\n",
    "        f1 = 2*(tpc + tnc) / (2*(tpc + tnc) + 2*fpc + 2*fnc + fpd + fnd) if (2*(tpc + tnc) + 2*fpc + 2*fnc + fpd + fnd) > 0 else 0\n",
    "        \n",
    "        metrics[i + 1] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tp': tpc,\n",
    "            'tn': tnc,\n",
    "            'fp': fpc,\n",
    "            'fn': fnc,\n",
    "            'fpd': fpd,\n",
    "            'fnd': fnd\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(\"\\nClassification Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for class_id, class_metrics in metrics['class_metrics'].items():\n",
    "        print(f\"\\n{class_names[class_id]}:\")\n",
    "        print(f\"  Precision: {class_metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {class_metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {class_metrics['f1']:.4f}\")\n",
    "        print(f\"  TP: {class_metrics['tp']}\")\n",
    "        print(f\"  TN: {class_metrics['tn']}\")\n",
    "        print(f\"  FP: {class_metrics['fp']}\")\n",
    "        print(f\"  FN: {class_metrics['fn']}\")\n",
    "        print(f\"  FPd: {class_metrics['fpd']}\")\n",
    "        print(f\"  FNd: {class_metrics['fnd']}\")\n",
    "\n",
    "def evaluate_he_model(base_dir, test_loader, device='cuda'):\n",
    "    # Initialize HE expert model\n",
    "    he_model = HEExpert().to(device)\n",
    "    \n",
    "    # Load the HE expert checkpoint\n",
    "    checkpoint_path = os.path.join(base_dir, 'he', 'best_model.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        he_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded HE expert checkpoint from: {checkpoint_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
    "\n",
    "    he_model.eval()\n",
    "    cell_post_processor = DetectionCellPostProcessor(nr_types=6, magnification=40)\n",
    "    \n",
    "    confusion_matrix = np.zeros((5, 5))  # 5 classes (excluding background)\n",
    "    total_unmatched_pred = 0\n",
    "    total_unmatched_gt = 0\n",
    "    total_batches = len(test_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            progress = (batch_idx + 1) / total_batches * 100\n",
    "            print(f\"\\rEvaluation Progress: {progress:.1f}%\", end=\"\")\n",
    "            \n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['targets']\n",
    "            \n",
    "            for i in range(images.shape[0]):\n",
    "                # Get model predictions\n",
    "                outputs = he_model(images[i:i+1])\n",
    "                \n",
    "                # Process predictions using post-processor\n",
    "                pred_map = np.concatenate([\n",
    "                    torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()[..., None],\n",
    "                    torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                    outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "                ], axis=-1)\n",
    "\n",
    "                instance_pred, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "                \n",
    "                # Process ground truth\n",
    "                gt_instance = targets['masks']['cell'][i].squeeze().cpu().numpy()\n",
    "                cell_types = targets['semantic']['cell_types'][i].cpu().numpy()\n",
    "                gt_type = np.zeros_like(gt_instance)\n",
    "                active_type = np.argmax(cell_types)\n",
    "                gt_type[gt_instance > 0] = active_type\n",
    "                \n",
    "                # Convert predictions to centroids format\n",
    "                pred_instances = {}\n",
    "                for instance_id, info in type_pred.items():\n",
    "                    mask = instance_pred == instance_id\n",
    "                    centroid = center_of_mass(mask)\n",
    "                    # Ensure type is within valid range\n",
    "                    cell_type = min(max(info['type'], 1), 5)\n",
    "                    pred_instances[instance_id] = {\n",
    "                        'centroid': centroid,\n",
    "                        'type': cell_type\n",
    "                    }\n",
    "                \n",
    "                # Get ground truth instances\n",
    "                gt_instances = get_instance_centroids_with_types(gt_instance, gt_type)\n",
    "                \n",
    "                # Match instances and calculate metrics\n",
    "                matched_pairs, unmatched_pred, unmatched_gt = match_instances_with_types(\n",
    "                    pred_instances, gt_instances\n",
    "                )\n",
    "                \n",
    "                total_unmatched_pred += len(unmatched_pred)\n",
    "                total_unmatched_gt += len(unmatched_gt)\n",
    "                \n",
    "                batch_confusion = calculate_confusion_matrix(matched_pairs, pred_instances, gt_instances)\n",
    "                confusion_matrix += batch_confusion\n",
    "    \n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    class_metrics = calculate_class_metrics(confusion_matrix, total_unmatched_pred, total_unmatched_gt)\n",
    "    \n",
    "    results = {\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'class_metrics': class_metrics,\n",
    "        'unmatched_pred': total_unmatched_pred,\n",
    "        'unmatched_gt': total_unmatched_gt\n",
    "    }\n",
    "    \n",
    "    # Plot and print results\n",
    "    plot_confusion_matrix(confusion_matrix, 'HE Model Evaluation Confusion Matrix')\n",
    "    plt.show()\n",
    "    print_metrics(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = 'checkpoints_separate'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    try:\n",
    "        # Use the test loader\n",
    "        he_test_loader = loaders['he']['test']\n",
    "        results = evaluate_he_model(base_dir, he_test_loader, device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d1382-2f94-438d-9462-eba9e84b66e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cell_segmentation.utils.post_proc_vitaminp import DetectionCellPostProcessor\n",
    "import os\n",
    "\n",
    "class_names = {\n",
    "    0: \"Background\",\n",
    "    1: \"Cell\"  # Changed to binary classification\n",
    "}\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    labels = [class_names[i] for i in range(2)]  # Changed to binary\n",
    "    \n",
    "    sns.heatmap(confusion_matrix, \n",
    "                annot=True, \n",
    "                fmt='.0f',\n",
    "                cmap='Blues',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def get_instance_centroids_with_types(instance_map, type_map):\n",
    "    instances = {}\n",
    "    for instance_id in np.unique(instance_map):\n",
    "        if instance_id == 0:  # Skip background\n",
    "            continue\n",
    "        mask = instance_map == instance_id\n",
    "        center = center_of_mass(mask)\n",
    "        # Convert multi-class to binary (any cell type > 0 becomes 1)\n",
    "        cell_type = 1 if np.any(type_map[mask] > 0) else 0\n",
    "        instances[instance_id] = {\n",
    "            'centroid': center,\n",
    "            'type': cell_type\n",
    "        }\n",
    "    return instances\n",
    "\n",
    "def match_instances_with_types(pred_instances, gt_instances, distance_threshold=12):\n",
    "    if not pred_instances or not gt_instances:\n",
    "        return [], list(pred_instances.keys()), list(gt_instances.keys())\n",
    "    \n",
    "    pred_coords = np.array([info['centroid'] for info in pred_instances.values()])\n",
    "    gt_coords = np.array([info['centroid'] for info in gt_instances.values()])\n",
    "    pred_ids = list(pred_instances.keys())\n",
    "    gt_ids = list(gt_instances.keys())\n",
    "    \n",
    "    distances = cdist(pred_coords, gt_coords)\n",
    "    \n",
    "    matched_pairs = []\n",
    "    used_pred = set()\n",
    "    used_gt = set()\n",
    "    \n",
    "    while distances.size > 0 and distances.min() <= distance_threshold:\n",
    "        pred_idx, gt_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "        if pred_idx not in used_pred and gt_idx not in used_gt:\n",
    "            matched_pairs.append((pred_ids[pred_idx], gt_ids[gt_idx]))\n",
    "            used_pred.add(pred_idx)\n",
    "            used_gt.add(gt_idx)\n",
    "        \n",
    "        distances[pred_idx, :] = float('inf')\n",
    "        distances[:, gt_idx] = float('inf')\n",
    "    \n",
    "    unmatched_pred = [pid for i, pid in enumerate(pred_ids) if i not in used_pred]\n",
    "    unmatched_gt = [gid for i, gid in enumerate(gt_ids) if i not in used_gt]\n",
    "    \n",
    "    return matched_pairs, unmatched_pred, unmatched_gt\n",
    "\n",
    "def calculate_confusion_matrix(matched_pairs, pred_instances, gt_instances):\n",
    "    confusion_matrix = np.zeros((2, 2), dtype=int)  # Changed to 2x2 for binary\n",
    "    \n",
    "    for pred_id, gt_id in matched_pairs:\n",
    "        pred_type = pred_instances[pred_id]['type']\n",
    "        gt_type = gt_instances[gt_id]['type']\n",
    "        confusion_matrix[gt_type, pred_type] += 1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "def calculate_class_metrics(confusion_matrix, unmatched_pred, unmatched_gt):\n",
    "    metrics = {}\n",
    "    \n",
    "    fpd = unmatched_pred\n",
    "    fnd = unmatched_gt\n",
    "    \n",
    "    # Calculate metrics for the cell class (class 1)\n",
    "    tpc = confusion_matrix[1, 1]\n",
    "    tnc = confusion_matrix[0, 0]\n",
    "    fpc = confusion_matrix[0, 1]\n",
    "    fnc = confusion_matrix[1, 0]\n",
    "    \n",
    "    precision = tpc / (tpc + fpc + fpd) if (tpc + fpc + fpd) > 0 else 0\n",
    "    recall = tpc / (tpc + fnc + fnd) if (tpc + fnc + fnd) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics[1] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tpc,\n",
    "        'tn': tnc,\n",
    "        'fp': fpc,\n",
    "        'fn': fnc,\n",
    "        'fpd': fpd,\n",
    "        'fnd': fnd\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(\"\\nBinary Classification Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    cell_metrics = metrics['class_metrics'][1]\n",
    "    print(f\"\\nCell Detection Metrics:\")\n",
    "    print(f\"  Precision: {cell_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {cell_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {cell_metrics['f1']:.4f}\")\n",
    "    print(f\"  TP: {cell_metrics['tp']}\")\n",
    "    print(f\"  TN: {cell_metrics['tn']}\")\n",
    "    print(f\"  FP: {cell_metrics['fp']}\")\n",
    "    print(f\"  FN: {cell_metrics['fn']}\")\n",
    "    print(f\"  FPd: {cell_metrics['fpd']}\")\n",
    "    print(f\"  FNd: {cell_metrics['fnd']}\")\n",
    "\n",
    "def evaluate_he_model_binary(base_dir, test_loader, device='cuda'):\n",
    "    # Initialize HE model\n",
    "    he_model = HEExpert().to(device)\n",
    "    \n",
    "    # Load the HE expert checkpoint\n",
    "    checkpoint_path = os.path.join(base_dir, 'he', 'best_model.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        he_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded HE expert checkpoint from: {checkpoint_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
    "\n",
    "    he_model.eval()\n",
    "    # Modified to binary classification\n",
    "    cell_post_processor = DetectionCellPostProcessor(nr_types=2, magnification=40)\n",
    "    \n",
    "    confusion_matrix = np.zeros((2, 2))  # Changed to 2x2 for binary\n",
    "    total_unmatched_pred = 0\n",
    "    total_unmatched_gt = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['targets']\n",
    "            \n",
    "            for i in range(images.shape[0]):\n",
    "                outputs = he_model(images[i:i+1])\n",
    "                \n",
    "                # Convert multi-class predictions to binary\n",
    "                pred_types = torch.argmax(outputs['cell_types'][0], dim=0).cpu().numpy()\n",
    "                pred_types = (pred_types > 0).astype(int)  # Convert to binary (0: background, 1: cell)\n",
    "                \n",
    "                pred_map = np.concatenate([\n",
    "                    pred_types[..., None],\n",
    "                    torch.argmax(outputs['cell_mask'][0], dim=0).cpu().numpy()[..., None],\n",
    "                    outputs['cell_hv'][0].cpu().numpy().transpose(1, 2, 0)\n",
    "                ], axis=-1)\n",
    "\n",
    "                instance_pred, type_pred = cell_post_processor.post_process_cell_segmentation(pred_map)\n",
    "                \n",
    "                gt_instance = targets['masks']['cell'][i].squeeze().cpu().numpy()\n",
    "                cell_types = targets['semantic']['cell_types'][i].cpu().numpy()\n",
    "                gt_type = np.zeros_like(gt_instance)\n",
    "                gt_type[gt_instance > 0] = 1  # Convert to binary (0: background, 1: cell)\n",
    "                \n",
    "                pred_instances = get_instance_centroids_with_types(instance_pred, pred_types)\n",
    "                gt_instances = get_instance_centroids_with_types(gt_instance, gt_type)\n",
    "                \n",
    "                matched_pairs, unmatched_pred, unmatched_gt = match_instances_with_types(\n",
    "                    pred_instances, gt_instances\n",
    "                )\n",
    "                \n",
    "                total_unmatched_pred += len(unmatched_pred)\n",
    "                total_unmatched_gt += len(unmatched_gt)\n",
    "                \n",
    "                batch_confusion = calculate_confusion_matrix(matched_pairs, pred_instances, gt_instances)\n",
    "                confusion_matrix += batch_confusion\n",
    "    \n",
    "    class_metrics = calculate_class_metrics(confusion_matrix, total_unmatched_pred, total_unmatched_gt)\n",
    "    \n",
    "    results = {\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'class_metrics': class_metrics,\n",
    "        'unmatched_pred': total_unmatched_pred,\n",
    "        'unmatched_gt': total_unmatched_gt\n",
    "    }\n",
    "    \n",
    "    plot_confusion_matrix(confusion_matrix, 'Binary Cell Detection Confusion Matrix')\n",
    "    plt.show()\n",
    "    print_metrics(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = 'checkpoints_separate'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    try:\n",
    "        results = evaluate_he_model_binary(base_dir, he_test_loader, device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b6b82-8706-47cc-8bcf-62625f0d26b5",
   "metadata": {},
   "source": [
    "## mIF EXPERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29445dc-117b-490b-8dda-73e24068f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage.segmentation import find_boundaries\n",
    "from cell_segmentation.utils.post_proc_vitaminp_mif import DetectionCellPostProcessor\n",
    "import os\n",
    "\n",
    "def load_mif_expert_and_visualize(base_dir: str, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Load MIF expert model and visualize its predictions on TissueNet\n",
    "    \"\"\"\n",
    "    # Initialize MIF expert model\n",
    "    mif_model = MIFExpert(pretrained=True).to(device)\n",
    "    \n",
    "    # Load the MIF expert checkpoint\n",
    "    checkpoint_path = os.path.join(base_dir, 'mif', 'best_model.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        mif_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded MIF expert checkpoint from: {checkpoint_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
    "\n",
    "    # Create dataset manager and setup datasets\n",
    "    manager = ModularDatasetManager()\n",
    "    datasets = manager.setup_datasets()\n",
    "    test_loader = manager.get_dataloader('tissuenet', 'test', batch_size=1)\n",
    "\n",
    "    # Get a random sample\n",
    "    random_idx = np.random.randint(0, len(test_loader.dataset))\n",
    "    print(f\"Analyzing random sample {random_idx} out of {len(test_loader.dataset)} samples\")\n",
    "\n",
    "    def visualize_tissuenet(random_idx, test_loader, model, device):\n",
    "        sample = test_loader.dataset[random_idx % len(test_loader.dataset)]\n",
    "        batch = test_loader.collate_fn([sample])\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            images = batch['image'].to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            nuclei_processor = DetectionCellPostProcessor(nr_types=1, magnification=40, gt=False)\n",
    "            membrane_processor = DetectionCellPostProcessor(nr_types=1, magnification=40, gt=False)\n",
    "            \n",
    "            # Process nuclei using correct keys\n",
    "            nuclei_seg = torch.sigmoid(outputs['nuclei_mask'][0, 1]).cpu().numpy() > 0.1\n",
    "            nuclei_hv = outputs['nuclei_hv'][0].cpu().numpy()\n",
    "            nuclei_map = np.concatenate([\n",
    "                np.zeros((256, 256, 1)),\n",
    "                nuclei_seg[..., None],\n",
    "                nuclei_hv.transpose(1, 2, 0)\n",
    "            ], axis=-1)\n",
    "            \n",
    "            # Process membrane using correct keys\n",
    "            membrane_seg = torch.sigmoid(outputs['membrane_mask'][0, 1]).cpu().numpy() > 0.3\n",
    "            membrane_hv = outputs['membrane_hv'][0].cpu().numpy()\n",
    "            membrane_map = np.concatenate([\n",
    "                np.zeros((256, 256, 1)),\n",
    "                membrane_seg[..., None],\n",
    "                membrane_hv.transpose(1, 2, 0)\n",
    "            ], axis=-1)\n",
    "            \n",
    "            nuclei_instance_map, nuclei_info = nuclei_processor.post_process_cell_segmentation(nuclei_map)\n",
    "            membrane_instance_map, membrane_info = membrane_processor.post_process_cell_segmentation(membrane_map)\n",
    "            \n",
    "            # Visualization\n",
    "            fig, axes = plt.subplots(1, 7, figsize=(35, 5))\n",
    "            \n",
    "            # Original image\n",
    "            img_display = images[0].cpu().numpy()\n",
    "            img_display = np.moveaxis(img_display, 0, -1)\n",
    "            \n",
    "            # Create RGB display\n",
    "            rgb_display = np.stack([\n",
    "                img_display[..., 0],  # Nuclei channel\n",
    "                img_display[..., 1],  # Membrane channel\n",
    "                img_display[..., 0]   # Nuclei channel again for blue\n",
    "            ], axis=-1)\n",
    "            \n",
    "            # Normalize for display\n",
    "            rgb_display = np.clip((rgb_display - rgb_display.min()) / \n",
    "                                (rgb_display.max() - rgb_display.min() + 1e-8), 0, 1)\n",
    "            \n",
    "            axes[0].imshow(rgb_display)\n",
    "            axes[0].set_title('Original Image\\n(Nuclei=Red+Blue, Membrane=Green)')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Nuclei GT\n",
    "            if 'nuclei' in batch['targets']['masks']:\n",
    "                gt_nuclei = batch['targets']['masks']['nuclei'][0].squeeze().cpu().numpy()\n",
    "                axes[1].imshow(gt_nuclei, cmap='nipy_spectral')\n",
    "                axes[1].set_title('Nuclei GT')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # Membrane GT\n",
    "            if 'membrane' in batch['targets']['masks']:\n",
    "                gt_membrane = batch['targets']['masks']['membrane'][0].squeeze().cpu().numpy()\n",
    "                axes[2].imshow(gt_membrane, cmap='nipy_spectral')\n",
    "                axes[2].set_title('Membrane GT')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            # Instance maps\n",
    "            axes[3].imshow(nuclei_instance_map, cmap='nipy_spectral')\n",
    "            axes[3].set_title(f'Nuclei Instances\\n(n={len(nuclei_info)})')\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            axes[4].imshow(membrane_instance_map, cmap='nipy_spectral')\n",
    "            axes[4].set_title(f'Membrane Instances\\n(n={len(membrane_info)})')\n",
    "            axes[4].axis('off')\n",
    "            \n",
    "            # Overlays\n",
    "            nuclei_boundaries = find_boundaries(nuclei_instance_map, mode='thick')\n",
    "            nuclei_overlay = rgb_display.copy()\n",
    "            nuclei_overlay[nuclei_boundaries] = [1, 0, 0]  # Red boundaries\n",
    "            axes[5].imshow(nuclei_overlay)\n",
    "            axes[5].set_title('Nuclei Boundaries')\n",
    "            axes[5].axis('off')\n",
    "            \n",
    "            membrane_boundaries = find_boundaries(membrane_instance_map, mode='thick')\n",
    "            membrane_overlay = rgb_display.copy()\n",
    "            membrane_overlay[membrane_boundaries] = [0, 1, 0]  # Green boundaries\n",
    "            axes[6].imshow(membrane_overlay)\n",
    "            axes[6].set_title('Membrane Boundaries')\n",
    "            axes[6].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\nPrediction Statistics:\")\n",
    "            print(f\"Number of detected nuclei: {len(nuclei_info)}\")\n",
    "            print(f\"Number of detected membranes: {len(membrane_info)}\")\n",
    "            \n",
    "            return nuclei_instance_map, membrane_instance_map, mif_model, test_loader\n",
    "\n",
    "    return visualize_tissuenet(random_idx, test_loader, mif_model, device)\n",
    "\n",
    "# Run the visualization\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = 'checkpoints_separate'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    try:\n",
    "        nuclei_map, membrane_map, model, loader = load_mif_expert_and_visualize(base_dir, device)\n",
    "        print(\"Visualization complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92939cd1-aae2-4d15-adfd-ccd097542324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yshokrollahi",
   "language": "python",
   "name": "yshokrollahi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
