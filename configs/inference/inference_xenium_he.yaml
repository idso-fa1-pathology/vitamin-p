apiVersion: batch/v1
kind: Job
metadata:
  name: vitamin-p-xenium-he-inference
  namespace: yn-gpu-workload
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 600
  template:
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
        nvidia.com/gpu.product: "NVIDIA-H100-80GB-HBM3"
      securityContext:
        runAsUser: 297724
        runAsGroup: 1944303352
        fsGroup: 1944303352
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: '1000Gi'
        - name: program
          persistentVolumeClaim:
            claimName: yshokrollahi-gpu-rsrch9-home-plm
      containers:
        - name: inference-container
          image: hpcharbor.mdanderson.edu/yshokrollahi/vitamin-p:latest
          workingDir: /rsrch9/home/plm/idso_fa1_pathology/codes/yshokrollahi/vitamin-p-latest
          env:
            - name: HOME
              value: /tmp
            - name: PYTHONPATH
              value: /rsrch9/home/plm/idso_fa1_pathology/codes/yshokrollahi/vitamin-p-latest
          volumeMounts:
            - name: shm
              mountPath: "/dev/shm"
            - name: program
              mountPath: "/rsrch9/home/plm/idso_fa1_pathology"
          resources:
            limits:
              nvidia.com/gpu: "2"
              cpu: "30"
              memory: "350Gi"
          command:
            - /bin/bash
            - -c
            - |
              # ==========================================================
              # 1. Create the Python script dynamically inside the pod
              # ==========================================================
              cat << 'EOF' > run_xenium_he.py
              import torch
              import os
              import sys
              from pathlib import Path

              sys.path.insert(0, os.getcwd())

              from vitaminp import VitaminPFlex
              from vitaminp.inference import WSIPredictor

              # ============================================================================
              # CONFIGURATION
              # ============================================================================
              IMAGE_PATH = "/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics/Xenium_V1_Human_Lung_Cancer_FFPE_outs/Xenium_V1_Human_Lung_Cancer_FFPE_he_image_registered.ome.tif"
              CHECKPOINT_PATH = "checkpoints/vitamin_p_flex_large_fold21_best.pth"
              OUTPUT_DIR = "inference_dsp/xenium_v1_lung_results_he_registered"

              print("="*60)
              print("CONFIGURING INFERENCE FOR H&E")
              print("="*60)
              print(f"Image: {IMAGE_PATH}")
              print(f"Output: {OUTPUT_DIR}")

              os.makedirs(OUTPUT_DIR, exist_ok=True)

              # ============================================================================
              # SETUP MODEL & PREDICTOR
              # ============================================================================
              device = 'cuda' if torch.cuda.is_available() else 'cpu'
              print(f"\nUsing device: {device}")

              print("\nLoading model...")
              model = VitaminPFlex(model_size='large').to(device)

              print(f"Loading checkpoint: {CHECKPOINT_PATH}")
              checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
              if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                  state_dict = checkpoint['state_dict']
              elif isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                  state_dict = checkpoint['model_state_dict']
              else:
                  state_dict = checkpoint
              model.load_state_dict(state_dict)
              model.eval()
              print("✅ Model loaded")

              # Initialize Predictor
              # MOVED: batch_size is defined here in __init__, not in predict()
              predictor = WSIPredictor(
                  model=model,
                  device=device,
                  patch_size=512,
                  overlap=64,
                  target_mpp=0.263,
                  magnification=40,
                  tissue_dilation=1,
                  use_constrained_watershed=True,
                  cell_threshold=0.5,
                  batch_size=128  # <--- CORRECT LOCATION
              )

              print("\nPredictor settings:")
              print(f"   Patch size: 512")
              print(f"   Overlap: 64")
              print(f"   Target MPP: 0.263 μm/pixel")
              print(f"   Magnification: 40x")
              print(f"   Batch Size: 128")

              # ============================================================================
              # RUN INFERENCE
              # ============================================================================
              print(f"\n{'='*60}")
              print("RUNNING INFERENCE (H&E Nuclei & Cells)...")
              print(f"{'='*60}")

              # Removed batch_size from here
              results = predictor.predict(
                  wsi_path=IMAGE_PATH,
                  output_dir=OUTPUT_DIR,
                  branches=['he_nuclei', 'he_cell'],
                  filter_tissue=True,
                  tissue_threshold=0.20,
                  clean_overlaps=True,
                  save_geojson=True,
                  detection_threshold=0.5,
                  min_area_um=20.0,
                  mpp_override=0.263,
                  simplify_epsilon=None,
                  coord_precision=None,
                  save_parquet=True,
              )

              # ============================================================================
              # RESULTS
              # ============================================================================
              print(f"\n{'='*60}")
              print("RESULTS")
              print(f"{'='*60}")
              total_detections = 0
              for branch_name, branch_results in results.items():
                  n = branch_results['num_detections']
                  total_detections += n
                  print(f"   {branch_name}: {n} detections ({branch_results['processing_time']:.2f}s)")
              print(f"✅ Total: {total_detections} detections")
              print(f"   Output saved to: {OUTPUT_DIR}")
              EOF

              # ==========================================================
              # 2. Run the generated script
              # ==========================================================
              echo "Starting H&E inference..."
              python run_xenium_he.py
      restartPolicy: Never