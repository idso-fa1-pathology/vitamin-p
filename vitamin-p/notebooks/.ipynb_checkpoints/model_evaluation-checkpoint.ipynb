{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Enable mixed precision\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Set up multi-GPU strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated preprocess_pannuke_data function\n",
      "Preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_pannuke_fold(fold_path):\n",
    "    fold_number = os.path.basename(fold_path).split()[-1]  # Extract the fold number\n",
    "    images = np.load(os.path.join(fold_path, 'images', f'fold{fold_number}', 'images.npy'))\n",
    "    masks = np.load(os.path.join(fold_path, 'masks', f'fold{fold_number}', 'masks.npy'))\n",
    "    types = np.load(os.path.join(fold_path, 'images', f'fold{fold_number}', 'types.npy'))\n",
    "    \n",
    "    print(f\"Raw images shape: {images.shape}\")\n",
    "    print(f\"Raw masks shape: {masks.shape}\")\n",
    "    print(f\"Raw types shape: {types.shape}\")\n",
    "    \n",
    "    # Ensure images are float32 and in range [0, 1]\n",
    "    images = images.astype(np.float32) / 255.0\n",
    "    \n",
    "    print(f\"Processed images shape: {images.shape}\")\n",
    "    print(f\"Processed masks shape: {masks.shape}\")\n",
    "    print(f\"Images dtype: {images.dtype}\")\n",
    "    print(f\"Masks dtype: {masks.dtype}\")\n",
    "    print(f\"Images min and max: {np.min(images):.4f}, {np.max(images):.4f}\")\n",
    "    print(f\"Masks min and max: {np.min(masks):.4f}, {np.max(masks):.4f}\")\n",
    "    print(f\"Percentage of non-zero mask pixels: {(masks > 0).mean() * 100:.2f}%\")\n",
    "    \n",
    "    return images, masks, types\n",
    "\n",
    "def create_hv_maps(masks):\n",
    "    h, w = masks.shape[1:3]\n",
    "    y_coords, x_coords = np.ogrid[:h, :w]\n",
    "    hv_maps = np.zeros((masks.shape[0], h, w, 2), dtype=np.float32)\n",
    "    \n",
    "    for i in range(masks.shape[0]):\n",
    "        for c in range(masks.shape[-1]):\n",
    "            mask = masks[i, ..., c]\n",
    "            if mask.sum() > 0:\n",
    "                center_y, center_x = np.mean(np.where(mask), axis=1)\n",
    "                hv_maps[i, ..., 0] += (x_coords - center_x) * mask\n",
    "                hv_maps[i, ..., 1] += (y_coords - center_y) * mask\n",
    "    \n",
    "    # Normalize HV maps to [-1, 1]\n",
    "    hv_maps = np.clip(hv_maps / np.maximum(h, w), -1, 1)\n",
    "    return hv_maps\n",
    "\n",
    "def load_fold(fold_path):\n",
    "    fold_number = os.path.basename(fold_path).split()[-1]\n",
    "    images = np.load(os.path.join(fold_path, 'images', f'fold{fold_number}', 'images.npy'))\n",
    "    masks = np.load(os.path.join(fold_path, 'masks', f'fold{fold_number}', 'masks.npy'))\n",
    "    types = np.load(os.path.join(fold_path, 'images', f'fold{fold_number}', 'types.npy'))\n",
    "    \n",
    "    # Convert images to float32 and normalize to [0, 1]\n",
    "    images = images.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create binary masks for NP branch\n",
    "    binary_masks = (masks.sum(axis=-1) > 0).astype(np.float32)\n",
    "    \n",
    "    # Create horizontal and vertical distance maps for HV branch\n",
    "    hv_maps = create_hv_maps(masks)\n",
    "    \n",
    "    # Convert string labels to integer indices\n",
    "    unique_types = np.unique(types)\n",
    "    type_to_index = {t: i for i, t in enumerate(unique_types)}\n",
    "    types_indices = np.array([type_to_index[t] for t in types])\n",
    "    \n",
    "    # Convert types to one-hot encoded format\n",
    "    num_classes = len(unique_types)\n",
    "    types_one_hot = tf.keras.utils.to_categorical(types_indices, num_classes=num_classes)\n",
    "    \n",
    "    return images, binary_masks, hv_maps, masks, types_one_hot, unique_types, types_indices\n",
    "\n",
    "def preprocess_pannuke_data(data_dir, fold, batch_size, augment_fn=None):\n",
    "    all_images, all_binary_masks, all_hv_maps, all_masks, all_types, unique_types, all_type_indices = [], [], [], [], [], None, []\n",
    "    for fold_name in ['Fold 1', 'Fold 2', 'Fold 3']:\n",
    "        fold_path = os.path.join(data_dir, fold_name)\n",
    "        images, binary_masks, hv_maps, masks, types, fold_unique_types, type_indices = load_fold(fold_path)\n",
    "        all_images.append(images)\n",
    "        all_binary_masks.append(binary_masks)\n",
    "        all_hv_maps.append(hv_maps)\n",
    "        all_masks.append(masks)\n",
    "        all_types.append(types)\n",
    "        all_type_indices.extend(type_indices)\n",
    "        if unique_types is None:\n",
    "            unique_types = fold_unique_types\n",
    "\n",
    "    all_images = np.concatenate(all_images)\n",
    "    all_binary_masks = np.concatenate(all_binary_masks)\n",
    "    all_hv_maps = np.concatenate(all_hv_maps)\n",
    "    all_masks = np.concatenate(all_masks)\n",
    "    all_types = np.concatenate(all_types)\n",
    "\n",
    "    print(f\"All images shape: {all_images.shape}\")\n",
    "    print(f\"All binary masks shape: {all_binary_masks.shape}\")\n",
    "    print(f\"All HV maps shape: {all_hv_maps.shape}\")\n",
    "    print(f\"All masks shape: {all_masks.shape}\")\n",
    "    print(f\"All types shape: {all_types.shape}\")\n",
    "\n",
    "    # Compute class weights for each branch\n",
    "    class_weights = {\n",
    "        'np_branch': compute_class_weight('balanced', classes=np.unique(all_binary_masks), y=all_binary_masks.flatten()),\n",
    "        'nt_branch': compute_class_weight('balanced', classes=np.arange(all_masks.shape[-1]), y=np.argmax(all_masks, axis=-1).flatten()),\n",
    "        'tc_branch': compute_class_weight('balanced', classes=np.arange(all_types.shape[-1]), y=np.argmax(all_types, axis=-1))\n",
    "    }\n",
    "    \n",
    "    # Convert class weights to dictionaries\n",
    "    class_weight_dicts = {\n",
    "        'np_branch': dict(enumerate(class_weights['np_branch'])),\n",
    "        'nt_branch': dict(enumerate(class_weights['nt_branch'])),\n",
    "        'tc_branch': dict(enumerate(class_weights['tc_branch']))\n",
    "    }\n",
    "\n",
    "    # Split data\n",
    "    total_samples = len(all_images)\n",
    "    if fold == 1:\n",
    "        train_end = int(0.7 * total_samples)\n",
    "        val_end = int(0.85 * total_samples)\n",
    "        train = (all_images[:train_end], all_binary_masks[:train_end], all_hv_maps[:train_end], all_masks[:train_end], all_types[:train_end])\n",
    "        val = (all_images[train_end:val_end], all_binary_masks[train_end:val_end], all_hv_maps[train_end:val_end], all_masks[train_end:val_end], all_types[train_end:val_end])\n",
    "        test = (all_images[val_end:], all_binary_masks[val_end:], all_hv_maps[val_end:], all_masks[val_end:], all_types[val_end:])\n",
    "    elif fold == 2:\n",
    "        train_start = int(0.15 * total_samples)\n",
    "        train_end = int(0.85 * total_samples)\n",
    "        train = (all_images[train_start:train_end], all_binary_masks[train_start:train_end], all_hv_maps[train_start:train_end], all_masks[train_start:train_end], all_types[train_start:train_end])\n",
    "        val = (all_images[:train_start], all_binary_masks[:train_start], all_hv_maps[:train_start], all_masks[:train_start], all_types[:train_start])\n",
    "        test = (all_images[train_end:], all_binary_masks[train_end:], all_hv_maps[train_end:], all_masks[train_end:], all_types[train_end:])\n",
    "    elif fold == 3:\n",
    "        train_start = int(0.3 * total_samples)\n",
    "        val_start = int(0.85 * total_samples)\n",
    "        train = (all_images[train_start:], all_binary_masks[train_start:], all_hv_maps[train_start:], all_masks[train_start:], all_types[train_start:])\n",
    "        val = (all_images[val_start:], all_binary_masks[val_start:], all_hv_maps[val_start:], all_masks[val_start:], all_types[val_start:])\n",
    "        test = (all_images[:train_start], all_binary_masks[:train_start], all_hv_maps[:train_start], all_masks[:train_start], all_types[:train_start])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid fold number. Choose 1, 2, or 3.\")\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    print(\"Data loaded. Creating datasets...\")\n",
    "\n",
    "    def create_dataset(images, binary_masks, hv_maps, masks, types):\n",
    "        def prepare_data(image, labels):\n",
    "            if augment_fn is not None and tf.random.uniform(()) > 0.5:\n",
    "                image, labels['np_branch'], labels['hv_branch'], labels['nt_branch'], _ = augment_fn(\n",
    "                    image, labels['np_branch'], labels['hv_branch'], labels['nt_branch'], labels['tc_branch']\n",
    "                )\n",
    "            return image, labels\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            images,\n",
    "            {\n",
    "                'np_branch': binary_masks[..., np.newaxis],\n",
    "                'hv_branch': hv_maps,\n",
    "                'nt_branch': masks,\n",
    "                'tc_branch': types\n",
    "            }\n",
    "        ))\n",
    "        \n",
    "        if augment_fn is not None:\n",
    "            dataset = dataset.map(prepare_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        return dataset.cache().shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    train_dataset = create_dataset(*train)\n",
    "    val_dataset = create_dataset(*val)\n",
    "    test_dataset = create_dataset(*test)\n",
    "\n",
    "    print(\"Datasets created and optimized.\")\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        print(\"Sample data:\")\n",
    "        print(f\"Images shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"NP branch shape: {labels['np_branch'].shape}, dtype: {labels['np_branch'].dtype}\")\n",
    "        print(f\"HV branch shape: {labels['hv_branch'].shape}, dtype: {labels['hv_branch'].dtype}\")\n",
    "        print(f\"NT branch shape: {labels['nt_branch'].shape}, dtype: {labels['nt_branch'].dtype}\")\n",
    "        print(f\"TC branch shape: {labels['tc_branch'].shape}, dtype: {labels['tc_branch'].dtype}\")\n",
    "\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    return train_dataset, val_dataset, test_dataset, unique_types, class_weight_dict\n",
    "\n",
    "\n",
    "print(\"Updated preprocess_pannuke_data function\")\n",
    "\n",
    "print(\"Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Model Creation (ViT and Expert HE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT and Expert HE model creation functions defined\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_patches(images, patch_size):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patch_dims = patches.shape[-1]\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "    return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "def create_vit_model(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_transformer_layers,\n",
    "    num_heads,\n",
    "    mlp_head_units,\n",
    "    dropout_rate,\n",
    "    num_classes,\n",
    "):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    patches = layers.Lambda(lambda x: create_patches(x, patch_size))(inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    for _ in range(num_transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = layers.Dense(units=mlp_head_units[0], activation=\"gelu\")(x3)\n",
    "        x3 = layers.Dropout(dropout_rate)(x3)\n",
    "        x3 = layers.Dense(units=projection_dim)(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(dropout_rate)(representation)\n",
    "\n",
    "    features = layers.Dense(mlp_head_units[0], activation=\"gelu\")(representation)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(mlp_head_units[1], activation=\"gelu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def create_decoder_branch(inputs, num_filters, num_outputs, name):\n",
    "    x = inputs\n",
    "    for _ in range(3):  # Increase the number of layers\n",
    "        x = layers.Conv2D(num_filters, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, inputs])  # Add residual connection\n",
    "    outputs = layers.Conv2D(num_outputs, 1, activation='sigmoid', name=name)(x)\n",
    "    return outputs\n",
    "\n",
    "def create_he_expert(input_shape, num_classes):\n",
    "    vit_encoder = create_vit_model(\n",
    "        input_shape=input_shape,\n",
    "        patch_size=16,\n",
    "        num_patches=(input_shape[0] // 16) ** 2,\n",
    "        projection_dim=64,\n",
    "        num_transformer_layers=8,\n",
    "        num_heads=4,\n",
    "        mlp_head_units=[2048, 1024],\n",
    "        dropout_rate=0.1,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    encoder_output = vit_encoder(inputs)\n",
    "    \n",
    "    # Reshape encoder output to 2D\n",
    "    x = layers.Dense(input_shape[0] * input_shape[1], activation=\"relu\")(encoder_output)\n",
    "    x = layers.Reshape((input_shape[0], input_shape[1], -1))(x)\n",
    "    \n",
    "    # Decoder (adjust to maintain input dimensions)\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Three decoder branches\n",
    "    np_branch = layers.Conv2D(1, 1, activation='sigmoid', name=\"np_branch\")(x)\n",
    "    hv_branch = layers.Conv2D(2, 1, activation='tanh', name=\"hv_branch\")(x)\n",
    "    nt_branch = layers.Conv2D(6, 1, activation='softmax', name=\"nt_branch\")(x)\n",
    "    \n",
    "    # Tissue classification branch\n",
    "    tc_branch = layers.GlobalAveragePooling2D()(x)\n",
    "    tc_branch = layers.Dense(num_classes, activation='softmax', name=\"tc_branch\")(tc_branch)\n",
    "    \n",
    "    full_model = tf.keras.Model(inputs=inputs, outputs=[np_branch, hv_branch, nt_branch, tc_branch])\n",
    "    encoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_output)\n",
    "    \n",
    "    print(\"Model output shapes:\")\n",
    "    print(f\"NP branch: {np_branch.shape}\")\n",
    "    print(f\"HV branch: {hv_branch.shape}\")\n",
    "    print(f\"NT branch: {nt_branch.shape}\")\n",
    "    print(f\"TC branch: {tc_branch.shape}\")\n",
    "    \n",
    "    return full_model, encoder_model\n",
    "\n",
    "print(\"ViT and Expert HE model creation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Loss Functions and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions and callbacks defined\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def weighted_bce(class_weights):\n",
    "    class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\n",
    "    \n",
    "    def loss(y_true, y_pred):\n",
    "        # Cast inputs to float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        weights = tf.reduce_sum(class_weights_tensor * y_true, axis=-1)\n",
    "        return tf.reduce_mean(bce * weights)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_loss(alpha, gamma):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Cast inputs to float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        focal_loss = -alpha * y_true * tf.math.pow(1 - y_pred, gamma) * tf.math.log(y_pred)\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "    return loss\n",
    "\n",
    "class ShapePrintingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, train_data):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if batch == 0:\n",
    "            print(\"\\nChecking shapes on first batch:\")\n",
    "            x, y = next(iter(self.train_data))\n",
    "            y_pred = self.model(x, training=False)\n",
    "            for i, output_name in enumerate(['np_branch', 'hv_branch', 'nt_branch', 'tc_branch']):\n",
    "                print(f\"{output_name} - True: {y[output_name].shape}, Pred: {y_pred[i].shape}\")\n",
    "\n",
    "class WarmUpLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, warmup_batches, init_lr, verbose=0):\n",
    "        super(WarmUpLearningRateScheduler, self).__init__()\n",
    "        self.warmup_batches = warmup_batches\n",
    "        self.init_lr = init_lr\n",
    "        self.verbose = verbose\n",
    "        self.batch_count = 0\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_count = self.batch_count + 1\n",
    "        lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.batch_count <= self.warmup_batches:\n",
    "            lr = self.batch_count * self.init_lr / self.warmup_batches\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "            if self.verbose > 0:\n",
    "                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning '\n",
    "                      'rate to %s.' % (self.batch_count + 1, lr))\n",
    "\n",
    "class GradientNormLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gradient_norms = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.original_train_step = self.model.train_step\n",
    "\n",
    "        def log_gradient_norm(norm):\n",
    "            self.gradient_norms.append(float(norm.numpy()))\n",
    "            return norm\n",
    "\n",
    "        def log_learning_rate(lr):\n",
    "            self.learning_rates.append(float(lr.numpy()))\n",
    "            return lr\n",
    "\n",
    "        @tf.function\n",
    "        def train_step_with_gradient_logging(data):\n",
    "            x, y = data\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self.model(x, training=True)\n",
    "                loss = self.model.compiled_loss(y, y_pred, regularization_losses=self.model.losses)\n",
    "            \n",
    "            # Compute gradients\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            \n",
    "            # Compute gradient norm\n",
    "            global_norm = tf.linalg.global_norm(gradients)\n",
    "            \n",
    "            # Apply gradients\n",
    "            self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "            \n",
    "            # Log gradient norm and learning rate\n",
    "            tf.py_function(log_gradient_norm, [global_norm], Tout=tf.float32)\n",
    "            \n",
    "            if hasattr(self.model.optimizer, 'lr'):\n",
    "                current_lr = self.model.optimizer.lr\n",
    "                if callable(current_lr):\n",
    "                    current_lr = current_lr(self.model.optimizer.iterations)\n",
    "                tf.py_function(log_learning_rate, [current_lr], Tout=tf.float32)\n",
    "            elif hasattr(self.model.optimizer, '_decayed_lr'):\n",
    "                current_lr = self.model.optimizer._decayed_lr(tf.float32)\n",
    "                tf.py_function(log_learning_rate, [current_lr], Tout=tf.float32)\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            tf.summary.scalar('gradient_norm', global_norm, step=self.model.optimizer.iterations)\n",
    "            tf.summary.scalar('learning_rate', current_lr, step=self.model.optimizer.iterations)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.model.compiled_metrics.update_state(y, y_pred)\n",
    "            return {m.name: m.result() for m in self.model.metrics}\n",
    "\n",
    "        self.model.train_step = train_step_with_gradient_logging\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.train_step = self.original_train_step\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.gradient_norms:\n",
    "            avg_gradient_norm = sum(self.gradient_norms) / len(self.gradient_norms)\n",
    "            print(f\"\\nAverage Gradient Norm for Epoch {epoch + 1}: {avg_gradient_norm:.4f}\")\n",
    "        if self.learning_rates:\n",
    "            avg_learning_rate = sum(self.learning_rates) / len(self.learning_rates)\n",
    "            print(f\"Average Learning Rate for Epoch {epoch + 1}: {avg_learning_rate:.6f}\")\n",
    "        self.gradient_norms = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "print(\"Loss functions and callbacks defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics function defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Ensure y_true and y_pred have the same shape\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, average='binary')\n",
    "    recall = recall_score(y_true, y_pred_binary, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred_binary, average='binary')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "print(\"Metrics function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from tensorflow.keras import mixed_precision\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load configuration\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Data augmentation function\n",
    "def augment_data(image, np_mask, hv_map, nt_mask, tc_label):\n",
    "    # Random flip left-right\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        np_mask = tf.image.flip_left_right(np_mask)\n",
    "        hv_map = tf.image.flip_left_right(hv_map)\n",
    "        nt_mask = tf.image.flip_left_right(nt_mask)\n",
    "        hv_map = tf.stack([hv_map[..., 0] * -1, hv_map[..., 1]], axis=-1)\n",
    "\n",
    "    # Random flip up-down\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "        np_mask = tf.image.flip_up_down(np_mask)\n",
    "        hv_map = tf.image.flip_up_down(hv_map)\n",
    "        nt_mask = tf.image.flip_up_down(nt_mask)\n",
    "        hv_map = tf.stack([hv_map[..., 0], hv_map[..., 1] * -1], axis=-1)\n",
    "\n",
    "    # Random brightness\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    \n",
    "    # Random contrast\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    \n",
    "    # Ensure image values are still in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0, 1)\n",
    "\n",
    "    return image, np_mask, hv_map, nt_mask, tc_label\n",
    "\n",
    "# Metrics calculation\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, average='binary')\n",
    "    recall = recall_score(y_true, y_pred_binary, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred_binary, average='binary')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "def main(dry_run=True):\n",
    "    # Get the directory of the current notebook\n",
    "    notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    \n",
    "    # Construct paths to config files\n",
    "    config_dir = os.path.join(notebook_dir, '..', 'configs')\n",
    "    data_config_path = os.path.join(config_dir, 'data_config.yaml')\n",
    "    model_config_path = os.path.join(config_dir, 'model_config.yaml')\n",
    "    training_config_path = os.path.join(config_dir, 'training_config.yaml')\n",
    "\n",
    "    # Load configurations\n",
    "    data_config = load_config(data_config_path)\n",
    "    model_config = load_config(model_config_path)\n",
    "    training_config = load_config(training_config_path)\n",
    "    # Enable mixed precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    # Set up multi-GPU strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "    # Adjust batch size for multi-GPU setup\n",
    "    global_batch_size = 32  # Adjust as needed\n",
    "    model_config['batch_size'] = global_batch_size // strategy.num_replicas_in_sync\n",
    "\n",
    "    # Preprocess data\n",
    "    train_dataset, val_dataset, test_dataset, unique_types, class_weight_dict = preprocess_pannuke_data(\n",
    "        data_config['he_data_dir'],\n",
    "        data_config['fold'],\n",
    "        model_config['batch_size'],\n",
    "        augment_data\n",
    "    )\n",
    "    \n",
    "    # Optimize datasets\n",
    "    train_dataset = train_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"Unique tissue types: {unique_types}\")\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Create model\n",
    "        model, encoder = create_he_expert(model_config['input_shape'], len(unique_types))\n",
    "        model.summary()\n",
    "\n",
    "        # Define learning rate schedule\n",
    "        initial_learning_rate = model_config['learning_rate']\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            initial_learning_rate,\n",
    "            first_decay_steps=1000,\n",
    "            t_mul=2.0,\n",
    "            m_mul=0.9,\n",
    "            alpha=0.1\n",
    "        )\n",
    "\n",
    "        # Define optimizer with gradient clipping\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipvalue=0.5)\n",
    "\n",
    "        \n",
    "        \n",
    "                # Create weighted loss functions\n",
    "        np_weights = tf.constant(list(class_weight_dict['np_branch'].values()), dtype=tf.float32)\n",
    "        nt_weights = tf.constant(list(class_weight_dict['nt_branch'].values()), dtype=tf.float32)\n",
    "        tc_weights = tf.constant(list(class_weight_dict['tc_branch'].values()), dtype=tf.float32)\n",
    "\n",
    "        # Compile model with custom losses and class weights\n",
    "        losses = {\n",
    "            'np_branch': weighted_bce(np_weights),\n",
    "            'hv_branch': tf.keras.losses.MeanSquaredError(),\n",
    "            'nt_branch': weighted_focal_loss(alpha=0.25, gamma=2.0),\n",
    "            'tc_branch': tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "        }\n",
    "\n",
    "        \n",
    "        loss_weights = {\n",
    "            'np_branch': 1.0,\n",
    "            'hv_branch': 0.5,\n",
    "            'nt_branch': 0.1,\n",
    "            'tc_branch': 1.0\n",
    "        }\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=losses,\n",
    "            loss_weights=loss_weights,\n",
    "            metrics={\n",
    "                'np_branch': [tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=0.5, dtype=tf.float32)],\n",
    "                'hv_branch': [tf.keras.metrics.MeanAbsoluteError(dtype=tf.float32)],\n",
    "                'nt_branch': [tf.keras.metrics.CategoricalAccuracy(dtype=tf.float32)],\n",
    "                'tc_branch': [tf.keras.metrics.CategoricalAccuracy(dtype=tf.float32)]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Define callbacks\n",
    "    callbacks = [ShapePrintingCallback(train_dataset)]\n",
    "\n",
    "    if not dry_run:\n",
    "        # Add additional callbacks for full training\n",
    "        callbacks.extend([\n",
    "            tf.keras.callbacks.EarlyStopping(patience=training_config['early_stopping_patience']),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=training_config['model_checkpoint_path'],\n",
    "                save_best_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            WarmUpLearningRateScheduler(warmup_batches=1000, init_lr=1e-6, verbose=1),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "            GradientNormLogger()\n",
    "        ])\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        if dry_run:\n",
    "            print(\"Starting dry run...\")\n",
    "            history = model.fit(\n",
    "                train_dataset.take(5),  # Only take 5 batches\n",
    "                epochs=2,  # Run for 2 epochs\n",
    "                validation_data=val_dataset.take(2),  # Only take 2 batches for validation\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            print(\"Dry run completed successfully!\")\n",
    "        else:\n",
    "            print(\"Starting full training...\")\n",
    "            history = model.fit(\n",
    "                train_dataset,\n",
    "                epochs=training_config['epochs'],\n",
    "                validation_data=val_dataset,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            print(\"Full training completed!\")\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history)\n",
    "\n",
    "        # Evaluate model\n",
    "        print(\"Evaluating model on test set...\")\n",
    "        test_results = model.evaluate(test_dataset, verbose=1)\n",
    "        print(f\"Test Results: {test_results}\")\n",
    "\n",
    "        # Calculate and print metrics\n",
    "        print(\"Calculating detailed metrics...\")\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        for branch, y_true in zip(['np_branch', 'nt_branch', 'tc_branch'], test_dataset.map(lambda x, y: y)):\n",
    "            metrics = calculate_metrics(y_true[branch].numpy(), y_pred[branch])\n",
    "            print(f\"Metrics for {branch}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric}: {value}\")\n",
    "\n",
    "        # Save the model\n",
    "        print(\"Saving model...\")\n",
    "        model.save(training_config['final_model_path'])\n",
    "        \n",
    "        # Save the encoder separately\n",
    "        encoder.save(training_config['encoder_model_path'])\n",
    "\n",
    "        print(\"Model and encoder saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Run the Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n",
      "All images shape: (7901, 256, 256, 3)\n",
      "All binary masks shape: (7901, 256, 256)\n",
      "All HV maps shape: (7901, 256, 256, 2)\n",
      "All masks shape: (7901, 256, 256, 6)\n",
      "All types shape: (7901, 19)\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "main(dry_run=True)  # Set to False for full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yshokrollahi",
   "language": "python",
   "name": "yshokrollahi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
