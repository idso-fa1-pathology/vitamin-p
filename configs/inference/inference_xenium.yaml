apiVersion: batch/v1
kind: Job
metadata:
  name: vitamin-p-xenium-cell-inference-region1
  namespace: yn-gpu-workload
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 600
  template:
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
        nvidia.com/gpu.product: "NVIDIA-H100-80GB-HBM3"
      securityContext:
        runAsUser: 297724
        runAsGroup: 1944303352
        fsGroup: 1944303352
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: '1000Gi'
        - name: program
          persistentVolumeClaim:
            claimName: yshokrollahi-gpu-rsrch9-home-plm
      containers:
        - name: inference-container
          image: hpcharbor.mdanderson.edu/yshokrollahi/vitamin-p:latest
          workingDir: /rsrch9/home/plm/idso_fa1_pathology/codes/yshokrollahi/vitamin-p-latest
          env:
            - name: HOME
              value: /tmp
            - name: PYTHONPATH
              value: /rsrch9/home/plm/idso_fa1_pathology/codes/yshokrollahi/vitamin-p-latest
          volumeMounts:
            - name: shm
              mountPath: "/dev/shm"
            - name: program
              mountPath: "/rsrch9/home/plm/idso_fa1_pathology"
          resources:
            limits:
              nvidia.com/gpu: "2"
              cpu: "30"
              memory: "350Gi"
          command:
            - /bin/bash
            - -c
            - |
              # ==========================================================
              # 1. Create the Python script dynamically inside the pod
              # ==========================================================
              cat << 'EOF' > run_xenium_custom.py
              import torch
              import numpy as np
              import tifffile
              import os
              import sys
              from pathlib import Path

              sys.path.insert(0, os.getcwd())

              from vitaminp import VitaminPFlex
              from vitaminp.inference import WSIPredictor, ChannelConfig

              # ============================================================================
              # CONFIGURATION
              # ============================================================================
              DATA_DIR = "/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/Lung_Anthracosis/output-XETG00522__0057986__Region_1__20251203__234028"
              MORPHOLOGY_DIR = f"{DATA_DIR}/morphology_focus"
              CHECKPOINT_PATH = "checkpoints/vitamin_p_flex_large_fold21_best.pth"
              OUTPUT_DIR = "inference_dsp/xenium_results_twochannel_cells"

              print("="*60)
              print("READING ORIGINAL XENIUM CHANNELS")
              print("="*60)

              # Read DAPI channel
              dapi_path = f"{MORPHOLOGY_DIR}/ch0000_dapi.ome.tif"
              print(f"\nReading DAPI: {dapi_path}")
              with tifffile.TiffFile(dapi_path) as tif:
                  dapi = tif.pages[0].asarray()
                  print(f"   ✅ DAPI loaded: {dapi.shape}, dtype={dapi.dtype}")

              # Read CD45/E-cadherin channel
              cd45_path = f"{MORPHOLOGY_DIR}/ch0001_atp1a1_cd45_e-cadherin.ome.tif"
              print(f"Reading CD45/E-cadherin: {cd45_path}")
              with tifffile.TiffFile(cd45_path) as tif:
                  cd45_ecadherin = tif.pages[0].asarray()
                  print(f"   ✅ CD45/E-cadherin loaded: {cd45_ecadherin.shape}, dtype={cd45_ecadherin.dtype}")

              # Read 18S channel
              ch18s_path = f"{MORPHOLOGY_DIR}/ch0002_18s.ome.tif"
              print(f"Reading 18S: {ch18s_path}")
              with tifffile.TiffFile(ch18s_path) as tif:
                  ch18s = tif.pages[0].asarray()
                  print(f"   ✅ 18S loaded: {ch18s.shape}, dtype={ch18s.dtype}")

              # ============================================================================
              # Write as plain multi-page TIFF — one page per channel, (C, H, W).
              # Do NOT use ome=True or metadata= here. At 112k x 54k the OME XML
              # validator crashes with "shape does not match stored shape".
              # A plain multi-page TIFF has no such validation.
              # extract_tiles_streaming will read page[0].shape to get (H, W).
              # ============================================================================
              combined = np.stack([dapi, cd45_ecadherin, ch18s], axis=0)  # (C, H, W)
              print(f"\n✅ Combined shape: {combined.shape} (channels, height, width)")

              os.makedirs(OUTPUT_DIR, exist_ok=True)
              temp_image_path = os.path.join(OUTPUT_DIR, 'temp_combined_xenium.tiff')

              tifffile.imwrite(temp_image_path, combined)  # plain TIFF, no OME
              print(f"✅ Saved temporary file: {temp_image_path}")

              # ============================================================================
              # SETUP MODEL & PREDICTOR
              # ============================================================================
              device = 'cuda' if torch.cuda.is_available() else 'cpu'
              print(f"\nUsing device: {device}")

              print("\nLoading model...")
              model = VitaminPFlex(model_size='large').to(device)

              print(f"Loading checkpoint: {CHECKPOINT_PATH}")
              checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
              if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                  state_dict = checkpoint['state_dict']
              elif isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                  state_dict = checkpoint['model_state_dict']
              else:
                  state_dict = checkpoint
              model.load_state_dict(state_dict)
              model.eval()
              print("✅ Model loaded")

              config = ChannelConfig(
                  nuclear_channel=0,
                  membrane_channel=[1, 2],
                  membrane_combination='max',
                  channel_names={0: 'DAPI', 1: 'ATP1A1_CD45_ECad', 2: '18S'}
              )

              predictor = WSIPredictor(
                  model=model,
                  device=device,
                  patch_size=512,
                  overlap=64,
                  target_mpp=0.263,
                  magnification=40,
                  mif_channel_config=config,
                  tissue_dilation=1,
                  use_constrained_watershed=True,
                  cell_threshold=0.5,
              )

              print("\nPredictor settings:")
              print(f"   Patch size: 512")
              print(f"   Overlap: 64")
              print(f"   Target MPP: 0.263 μm/pixel")
              print(f"   Magnification: 40x")
              print(f"   Constrained watershed: enabled")

              # ============================================================================
              # RUN INFERENCE
              # ============================================================================
              print(f"\n{'='*60}")
              print("RUNNING INFERENCE...")
              print(f"{'='*60}")

              results = predictor.predict(
                  wsi_path=temp_image_path,
                  output_dir=OUTPUT_DIR,
                  branches='mif_cell',
                  filter_tissue=True,
                  tissue_threshold=0.01,
                  clean_overlaps=True,
                  save_geojson=True,
                  detection_threshold=0.5,
                  min_area_um=20.0,
                  mpp_override=0.263,
                  simplify_epsilon=None,
                  coord_precision=None,
                  save_parquet=True,
              )

              # ============================================================================
              # RESULTS — multi-branch returns nested dict keyed by branch name
              # ============================================================================
              print(f"\n{'='*60}")
              print("RESULTS")
              print(f"{'='*60}")
              total_detections = 0
              for branch_name, branch_results in results.items():
                  n = branch_results['num_detections']
                  total_detections += n
                  print(f"   {branch_name}: {n} detections ({branch_results['processing_time']:.2f}s)")
              print(f"✅ Total: {total_detections} detections")
              print(f"   Output saved to: {OUTPUT_DIR}")

              # Clean up temp file
              if os.path.exists(temp_image_path):
                  os.remove(temp_image_path)
                  print(f"✅ Cleaned up temporary file")
              EOF

              # ==========================================================
              # 2. Run the generated script
              # ==========================================================
              echo "Starting Xenium inference..."
              python run_xenium_custom.py
      restartPolicy: Never